[
  {
    "objectID": "assignments/01-blast.html",
    "href": "assignments/01-blast.html",
    "title": "NCBI Blast",
    "section": "",
    "text": "Assignment\n\n\n\nAnnotate a provided, unknown fasta file with Gene Ontology terms. Your code should be fully reproducible.\nsee also https://rpubs.com/sr320/1026094\nIn your code directory create a file.\n01-blast.Rmd"
  },
  {
    "objectID": "assignments/01-blast.html#joining-blast-table-with-annotation-table",
    "href": "assignments/01-blast.html#joining-blast-table-with-annotation-table",
    "title": "NCBI Blast",
    "section": "Joining blast table with annotation table",
    "text": "Joining blast table with annotation table\nAt this point we have a blast output table and annotation table both with a Uniprot accession number. Thus we can join the two tables and be able to get more functional information about thet genes.\n```{bash}\nhead -2 ../output/Ab_4-uniprot_blastx.tab\nwc -l ../output/Ab_4-uniprot_blastx.tab\n```\n```{bash}\ntr '|' '\\t' &lt; ../output/Ab_4-uniprot_blastx.tab | head -2\n```\n```{bash}\ntr '|' '\\t' &lt; ../output/Ab_4-uniprot_blastx.tab \\\n&gt; ../output/Ab_4-uniprot_blastx_sep.tab\n```\n```{bash}\nhead -2 ../data/uniprot_table_r2023_01.tab\nwc -l ../data/uniprot_table_r2023_01.tab\n```\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.0     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.4.1     ✔ tibble    3.1.8\n✔ lubridate 1.9.3     ✔ tidyr     1.3.0\n✔ purrr     1.0.1     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(\"kableExtra\")\n\n\nAttaching package: 'kableExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\n\n\nbltabl &lt;- read.csv(\"../output/Ab_4-uniprot_blastx_sep.tab\", sep = '\\t', header = FALSE)\n\n\ncd ../data\ncurl -O https://gannet.fish.washington.edu/seashell/snaps/uniprot_table_r2023_01.tab\n\n\nspgo &lt;- read.csv(\"../data/uniprot_table_r2023_01.tab\", sep = '\\t', header = TRUE)\n\n\nstr(spgo)\n\n\nannot_tab &lt;-\n  left_join(bltabl, spgo,  by = c(\"V3\" = \"Entry\")) %&gt;%\n  select(V1, V3, V13, Protein.names, Organism, Gene.Ontology..biological.process., Gene.Ontology.IDs) %&gt;% mutate(V1 = str_replace_all(V1, \n            pattern = \"solid0078_20110412_FRAG_BC_WHITE_WHITE_F3_QV_SE_trimmed\", replacement = \"Ab\"))\n\n\nkbl(\nhead(\n  left_join(bltabl, spgo,  by = c(\"V3\" = \"Entry\")) %&gt;%\n  select(V1, V3, V13, Protein.names, Organism, Gene.Ontology..biological.process., Gene.Ontology.IDs) %&gt;% mutate(V1 = str_replace_all(V1, \n            pattern = \"solid0078_20110412_FRAG_BC_WHITE_WHITE_F3_QV_SE_trimmed\", replacement = \"Ab\"))\n)\n) %&gt;%\n  kable_styling(bootstrap_options = c(\"striped\", \"hover\", \"condensed\", \"responsive\"))\n\n\nleft_join(bltabl, spgo,  by = c(\"V3\" = \"Entry\")) %&gt;%\n  select(V1, V3, V13, Protein.names, Organism, Gene.Ontology..biological.process., Gene.Ontology.IDs) %&gt;% mutate(V1 = str_replace_all(V1, \n            pattern = \"solid0078_20110412_FRAG_BC_WHITE_WHITE_F3_QV_SE_trimmed\", replacement = \"Ab\")) %&gt;%\n  write_delim(\"../output/blast_annot_go.tab\", delim = '\\t')"
  },
  {
    "objectID": "assignments/08-bedtools.html",
    "href": "assignments/08-bedtools.html",
    "title": "BEDtools",
    "section": "",
    "text": "Assignment\n\n\n\nRun some basic sub-commands in BEDtools."
  },
  {
    "objectID": "assignments/08-bedtools.html#convert-bam-to-bed",
    "href": "assignments/08-bedtools.html#convert-bam-to-bed",
    "title": "BEDtools",
    "section": "Convert bam to bed",
    "text": "Convert bam to bed\n/home/shared/bedtools2/bin/bedtools bamtobed \\ \n-i ../data/19F_R1_val_1_bismark_bt2_pe.deduplicated.sorted.bam \\ \n&gt; ../output/08-19F.bed\n\n\n\n\n\n\nWarning\n\n\n\nThis is big file. What should you not do with it?"
  },
  {
    "objectID": "assignments/08-bedtools.html#get-coverage-of-sequence-reads-on-gene-regions",
    "href": "assignments/08-bedtools.html#get-coverage-of-sequence-reads-on-gene-regions",
    "title": "BEDtools",
    "section": "Get coverage of sequence reads on gene regions",
    "text": "Get coverage of sequence reads on gene regions"
  },
  {
    "objectID": "assignments/08-bedtools.html#default-behavior",
    "href": "assignments/08-bedtools.html#default-behavior",
    "title": "BEDtools",
    "section": "Default behavior",
    "text": "Default behavior\nAfter each interval in A, bedtools coverage will report:\n\nThe number of features in B that overlapped (by at least one base pair) the A interval.\nThe number of bases in A that had non-zero coverage from features in B.\nThe length of the entry in A.\nThe fraction of bases in A that had non-zero coverage from features in B.\n\n/home/shared/bedtools2/bin/bedtools coverage \\\n-a ../data/C_virginica-3.0_Gnomon_genes.bed \\\n-b ../output/08-19F.bed \\\n&gt; ../output/08-gene-19F-coverage.out"
  },
  {
    "objectID": "assignments/00-bash.html",
    "href": "assignments/00-bash.html",
    "title": "bash",
    "section": "",
    "text": "Warning\n\n\n\nFor this self directed tutorial you will need to download data-shell.zip and navigate to using a terminal. This could be “Terminal” within Rstudio, or a stand alone application.\nThe part of the operating system responsible for managing files and directories is called the file system. It organizes our data into files, which hold information, and directories (also called “folders”), which hold files or other directories.\nSeveral commands are frequently used to create, inspect, rename, and delete files and directories. To start exploring them, let’s open a shell window:\nThe dollar sign is a prompt, which shows us that the shell is waiting for input; your shell may show something more elaborate.\nType the command whoami, then press the Enter key (sometimes marked Return) to send the command to the shell. The command’s output is the ID of the current user, i.e., it shows us who the shell thinks we are:\nMore specifically, when we type whoami the shell:\nNext, let’s find out where we are by running a command called pwd (which stands for “print working directory”). At any moment, our current working directory is our current default directory, i.e., the directory that the computer assumes we want to run commands in unless we explicitly specify something else. Here, the computer’s response is /home/jovyan\nTo understand what a “home directory” is, let’s have a look at how the file system as a whole is organized. At the top is the root directory that holds everything else. We refer to it using a slash character / on its own; this is the leading slash in /home/jovyan."
  },
  {
    "objectID": "assignments/00-bash.html#ls",
    "href": "assignments/00-bash.html#ls",
    "title": "bash",
    "section": "ls",
    "text": "ls\nLet’s see what’s in this directory by running ls, which stands for “listing”:\nls\ncreatures  molecules           pizza.cfg\ndata       north-pacific-gyre  solar.pdf\nDesktop    notes.txt           writing\n\nls prints the names of the files and directories in the current directory in alphabetical order, arranged neatly into columns. We can make its output more comprehensible by using the flag -F, which tells ls to add a trailing / to the names of directories:\nls -F\ncreatures/  molecules/           pizza.cfg\ndata/       north-pacific-gyre/  solar.pdf\nDesktop/    notes.txt            writing/\nHere, we can see that data-shell contains seven sub-directories. The names that don’t have trailing slashes, like notes.txt, pizza.cfg, and solar.pdf, are plain old files. And note that there is a space between ls and -F: without it, the shell thinks we’re trying to run a command called ls-F, which doesn’t exist."
  },
  {
    "objectID": "assignments/00-bash.html#relative-path",
    "href": "assignments/00-bash.html#relative-path",
    "title": "bash",
    "section": "relative path",
    "text": "relative path\nNow let’s take a look at what’s in data-shell directory by running ls -F data, i.e., the command ls with the arguments -F and data. The second argument — the one without a leading dash — tells ls that we want a listing of something other than our current working directory:\n ls -F data\namino-acids.txt  animal-counts/  animals.txt  elements/  morse.txt  pdb/  planets.txt  salmon.txt  sunspot.txt\nThe output shows us that there are four text files and two sub-sub-directories. Organizing things hierarchically in this way helps us keep track of our work: it’s possible to put hundreds of files in our home directory, just as it’s possible to pile hundreds of printed papers on our desk, but it’s a self-defeating strategy.\nNotice, by the way that we spelled the directory name data. It doesn’t have a trailing slash: that’s added to directory names by ls when we use the -F flag to help us tell things apart. And it doesn’t begin with a slash because it’s a relative path, i.e., it tells ls how to find something from where we are, rather than from the root of the file system."
  },
  {
    "objectID": "assignments/00-bash.html#absolute-path",
    "href": "assignments/00-bash.html#absolute-path",
    "title": "bash",
    "section": "absolute path",
    "text": "absolute path\nIf we run ls -F /data (with a leading slash) we get a different answer, because /data is an absolute path:\nls -F /data\nNote you will get an “No file” warning here. This is because we this directory does not exist.\nThe leading / tells the computer to follow the path from the root of the filesystem, so it always refers to exactly one directory, no matter where we are when we run the command.\nIf we wanted to use the **absolute path* to list out the contents of this directory we could used\nls -F /home/jovyan/data-shell/data/\nNote this would work no matter what our pwd is."
  },
  {
    "objectID": "assignments/00-bash.html#nelles-pipeline-organizing-files",
    "href": "assignments/00-bash.html#nelles-pipeline-organizing-files",
    "title": "bash",
    "section": "Nelle’s Pipeline: Organizing Files",
    "text": "Nelle’s Pipeline: Organizing Files\nKnowing just this much about files and directories, Nelle is ready to organize the files that the protein assay machine will create. First, she creates a directory called north-pacific-gyre (to remind herself where the data came from). Inside that, she creates a directory called 2012-07-03, which is the date she started processing the samples. She used to use names like conference-paper and revised-results, but she found them hard to understand after a couple of years. (The final straw was when she found herself creating a directory called revised-revised-results-3.)\n\nNelle names her directories “year-month-day”, with leading zeroes for months and days, because the shell displays file and directory names in alphabetical order. If she used month names, December would come before July; if she didn’t use leading zeroes, November (‘11’) would come before July (‘7’).\n\nEach of her physical samples is labelled according to her lab’s convention with a unique ten-character ID, such as “NENE01729A”. This is what she used in her collection log to record the location, time, depth, and other characteristics of the sample, so she decides to use it as part of each data file’s name. Since the assay machine’s output is plain text, she will call her files NENE01729A.txt, NENE01812A.txt, and so on. All 1520 files will go into the same directory.\nIf she is in her home directory, Nelle can see what files she has using the command:\nls north-pacific-gyre/2012-07-03/\nThis is a lot to type, but she can let the shell do most of the work. If she types:\nls nor\nand then presses tab, the shell automatically completes the directory name for her:\nls north-pacific-gyre/\nIf she presses tab again, Bash will add 2012-07-03/ to the command, since it’s the only possible completion. Pressing tab again does nothing, since there are 1520 possibilities; pressing tab twice brings up a list of all the files, and so on. This is called tab completion, and we will see it in many other tools as we go on."
  },
  {
    "objectID": "assignments/00-bash.html#key-points",
    "href": "assignments/00-bash.html#key-points",
    "title": "bash",
    "section": "Key Points",
    "text": "Key Points\n\nThe file system is responsible for managing information on the disk.\nInformation is stored in files, which are stored in directories (folders).\nDirectories can also store other directories, which forms a directory tree.\n/ on its own is the root directory of the whole filesystem.\nA relative path specifies a location starting from the current location.\nAn absolute path specifies a location from the root of the filesystem.\nDirectory names in a path are separated with / on Unix, but \\ on Windows.\n.. means “the directory above the current one”; . on its own means “the current directory”.\nMost files’ names are something.extension. The extension isn’t required, and doesn’t guarantee anything, but is normally used to indicate the type of data in the file.\nMost commands take options (flags) which begin with a -."
  },
  {
    "objectID": "assignments/00-bash.html#word-count",
    "href": "assignments/00-bash.html#word-count",
    "title": "bash",
    "section": "word count",
    "text": "word count\nLet’s go into that directory with cd and run the command wc *.pdb. wc is the “word count” command: it counts the number of lines, words, and characters in files. The * in *.pdb matches zero or more characters, so the shell turns *.pdb into a complete list of .pdb files:\ncd molecules\n$ wc *.pdb\n\n  20  156 1158 cubane.pdb\n  12   84  622 ethane.pdb\n   9   57  422 methane.pdb\n  30  246 1828 octane.pdb\n  21  165 1226 pentane.pdb\n  15  111  825 propane.pdb\n 107  819 6081 total\n\nWildcards\n* is a wildcard. It matches zero or more characters, so *.pdb matches ethane.pdb, propane.pdb, and so on. On the other hand, p*.pdb only matches pentane.pdb and propane.pdb, because the ‘p’ at the front only matches itself.\n? is also a wildcard, but it only matches a single character. This means that p?.pdb matches pi.pdb or p5.pdb, but not propane.pdb. We can use any number of wildcards at a time: for example, p*.p?* matches anything that starts with a ‘p’ and ends with ‘.’, ‘p’, and at least one more character (since the ‘?’ has to match one character, and the final * can match any number of characters). Thus, p*.p?* would match preferred.practice, and even p.pi (since the first * can match no characters at all), but not quality.practice (doesn’t start with ‘p’) or preferred.p (there isn’t at least one character after the ‘.p’).\nWhen the shell sees a wildcard, it expands the wildcard to create a list of matching filenames before running the command that was asked for. This means that commands like wc and ls never see the wildcard characters, just what those wildcards matched. This is another example of orthogonal design.\nIf we run wc -l instead of just wc, the output shows only the number of lines per file:\nwc -l *.pdb\n  20  cubane.pdb\n  12  ethane.pdb\n   9  methane.pdb\n  30  octane.pdb\n  21  pentane.pdb\n  15  propane.pdb\n 107  total\nWe can also use -w to get only the number of words, or -c to get only the number of characters."
  },
  {
    "objectID": "assignments/00-bash.html#redirect",
    "href": "assignments/00-bash.html#redirect",
    "title": "bash",
    "section": "redirect",
    "text": "redirect\nWhich of these files is shortest? It’s an easy question to answer when there are only six files, but what if there were 6000? Our first step toward a solution is to run the command:\nwc -l *.pdb &gt; lengths\nThe &gt; tells the shell to redirect the command’s output to a file instead of printing it to the screen. The shell will create the file if it doesn’t exist, or overwrite the contents of that file if it does. (This is why there is no screen output: everything that wc would have printed has gone into the file lengths instead.) ls lengths confirms that the file exists:\nls lengths\nlengths"
  },
  {
    "objectID": "assignments/00-bash.html#cat",
    "href": "assignments/00-bash.html#cat",
    "title": "bash",
    "section": "cat",
    "text": "cat\nWe can now send the content of lengths to the screen using cat lengths. cat stands for “concatenate”: it prints the contents of files one after another. There’s only one file in this case, so cat just shows us what it contains:\ncat lengths\n  20  cubane.pdb\n  12  ethane.pdb\n   9  methane.pdb\n  30  octane.pdb\n  21  pentane.pdb\n  15  propane.pdb\n 107  total"
  },
  {
    "objectID": "assignments/00-bash.html#sort",
    "href": "assignments/00-bash.html#sort",
    "title": "bash",
    "section": "sort",
    "text": "sort\nNow let’s use the sort command to sort its contents. We will also use the -n flag to specify that the sort is numerical instead of alphabetical. This does not change the file; instead, it sends the sorted result to the screen:\nsort -n lengths\n  9  methane.pdb\n 12  ethane.pdb\n 15  propane.pdb\n 20  cubane.pdb\n 21  pentane.pdb\n 30  octane.pdb\n107  total"
  },
  {
    "objectID": "assignments/00-bash.html#head",
    "href": "assignments/00-bash.html#head",
    "title": "bash",
    "section": "head",
    "text": "head\nWe can put the sorted list of lines in another temporary file called sorted-lengths by putting &gt; sorted-lengths after the command, just as we used &gt; lengths to put the output of wc into lengths. Once we’ve done that, we can run another command called head to get the first few lines in sorted-lengths:\nsort -n lengths &gt; sorted-lengths\nhead -1 sorted-lengths\n  9  methane.pdb\nUsing the parameter -1 with head tells it that we only want the first line of the file; -20 would get the first 20, and so on. Since sorted-lengths contains the lengths of our files ordered from least to greatest, the output of head must be the file with the fewest lines."
  },
  {
    "objectID": "assignments/00-bash.html#pipe",
    "href": "assignments/00-bash.html#pipe",
    "title": "bash",
    "section": "pipe",
    "text": "pipe\nIf you think this is confusing, you’re in good company: even once you understand what wc, sort, and head do, all those intermediate files make it hard to follow what’s going on. We can make it easier to understand by running sort and head together:\nsort -n lengths | head -1\n  9  methane.pdb\nThe vertical bar between the two commands is called a pipe. It tells the shell that we want to use the output of the command on the left as the input to the command on the right. The computer might create a temporary file if it needs to, or copy data from one program to the other in memory, or something else entirely; we don’t have to know or care.\nWe can use another pipe to send the output of wc directly to sort, which then sends its output to head:\nwc -l *.pdb | sort -n | head -1\n  9  methane.pdb\n\nHere’s what actually happens behind the scenes when we create a pipe. When a computer runs a program—any program—it creates a process in memory to hold the program’s software and its current state. Every process has an input channel called standard input. (By this point, you may be surprised that the name is so memorable, but don’t worry: most Unix programmers call it “stdin”. Every process also has a default output channel called standard output] (or “stdout”)\n\n\nThe shell is actually just another program. Under normal circumstances, whatever we type on the keyboard is sent to the shell on its standard input, and whatever it produces on standard output is displayed on our screen. When we tell the shell to run a program, it creates a new process and temporarily sends whatever we type on our keyboard to that process’s standard input, and whatever the process sends to standard output to the screen.\nHere’s what happens when we run wc -l *.pdb &gt; lengths. The shell starts by telling the computer to create a new process to run the wc program. Since we’ve provided some filenames as parameters, wc reads from them instead of from standard input. And since we’ve used &gt; to redirect output to a file, the shell connects the process’s standard output to that file.\nIf we run wc -l *.pdb | sort -n instead, the shell creates two processes (one for each process in the pipe) so that wc and sort run simultaneously. The standard output of wc is fed directly to the standard input of sort; since there’s no redirection with &gt;, sort’s output goes to the screen. And if we run wc -l *.pdb | sort -n | head -1, we get three processes with data flowing from the files, through wc to sort, and from sort through head to the screen.\nThis simple idea is why Unix has been so successful. Instead of creating enormous programs that try to do many different things, Unix programmers focus on creating lots of simple tools that each do one job well, and that work well with each other. This programming model is called pipes and filters. We’ve already seen pipes; a filter is a program like wc or sort that transforms a stream of input into a stream of output. Almost all of the standard Unix tools can work this way: unless told to do otherwise, they read from standard input, do something with what they’ve read, and write to standard output.\nThe key is that any program that reads lines of text from standard input and writes lines of text to standard output can be combined with every other program that behaves this way as well. You can and should write your programs this way so that you and other people can put those programs into pipes to multiply their power.\n\n\nRedirecting Input\nAs well as using &gt; to redirect a program’s output, we can use &lt; to redirect its input, i.e., to read from a file instead of from standard input. For example, instead of writing wc ammonia.pdb, we could write wc &lt; ammonia.pdb. In the first case, wc gets a command line parameter telling it what file to open. In the second, wc doesn’t have any command line parameters, so it reads from standard input, but we have told the shell to send the contents of ammonia.pdb to wc’s standard input."
  },
  {
    "objectID": "assignments/05-slidedeck.html",
    "href": "assignments/05-slidedeck.html",
    "title": "Project slidedeck",
    "section": "",
    "text": "Assignment\n\n\n\nTLDR: Create and publish a presentation in Quarto on you research project.\n\n\n\n\n\nStart by going to New File &gt; Quarto Presentation. (Leave render type as default)\nSave file in research repo, in the code directory, with the prefix 05.\nCreate Slides using ## (Heading 2) to title new slide.\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use same code and principles that you used for knitting a report in week three. Including the set-up chunk. Chunk options work the same way.\n\n\n\nPublish slides using Rpubs and provide link in readme.\n\n\n\n\nFor this assignment you will want to develop slides that\n\nClearly demonstrate your project goal\nMethods taken\nPreliminary Results\nOutline of next steps for next 4 weeks\n\n\n\n\n\nShow core code\nShow parts of initial data (could be first few lines etc)\nInclude a table\nInclude image\nInclude plot generated from code\nHighlight specific lines of code in code block"
  },
  {
    "objectID": "assignments/05-slidedeck.html#creating-slides",
    "href": "assignments/05-slidedeck.html#creating-slides",
    "title": "Project slidedeck",
    "section": "",
    "text": "Start by going to New File &gt; Quarto Presentation. (Leave render type as default)\nSave file in research repo, in the code directory, with the prefix 05.\nCreate Slides using ## (Heading 2) to title new slide.\n\n\n\n\n\n\n\nTip\n\n\n\nYou can use same code and principles that you used for knitting a report in week three. Including the set-up chunk. Chunk options work the same way.\n\n\n\nPublish slides using Rpubs and provide link in readme."
  },
  {
    "objectID": "assignments/05-slidedeck.html#rubric",
    "href": "assignments/05-slidedeck.html#rubric",
    "title": "Project slidedeck",
    "section": "",
    "text": "For this assignment you will want to develop slides that\n\nClearly demonstrate your project goal\nMethods taken\nPreliminary Results\nOutline of next steps for next 4 weeks"
  },
  {
    "objectID": "assignments/05-slidedeck.html#minimum-required-components",
    "href": "assignments/05-slidedeck.html#minimum-required-components",
    "title": "Project slidedeck",
    "section": "",
    "text": "Show core code\nShow parts of initial data (could be first few lines etc)\nInclude a table\nInclude image\nInclude plot generated from code\nHighlight specific lines of code in code block"
  },
  {
    "objectID": "assignments/10-compendium.html",
    "href": "assignments/10-compendium.html",
    "title": "New Release",
    "section": "",
    "text": "Assignment\n\n\n\nMake new release of repo such that Zenodo archive will be updated with better version of your repo."
  },
  {
    "objectID": "assignments/07-CG.html",
    "href": "assignments/07-CG.html",
    "title": "CG Motifs",
    "section": "",
    "text": "Assignment\n\n\n\n\nIdentify any prior assignments you would like regraded (this week only). 2) Visualize CG motifs in 10 of your sequences.\n\n\n\n\nAmnesty\nThis week you can turn in all missing assignments to be assessed. Indicate your decision to do this and which specific assignments here\n\n\nCG Motifs\nFor this you will take 10 sequences related to your project, ID CG motifs using the emboss package: fuzznuc, and visualize in IGV. You do not have to follow this workflow exactly, but it is provided here for guidance. This uses R package seqinr.\n\n```{r}\nlibrary(seqinr)\n\n# Replace 'input.fasta' with the name of your multi-sequence fasta file\ninput_file &lt;- \"input.fasta\"\nsequences &lt;- read.fasta(input_file)\n\n```\n\n\n```{r}\n# Set the seed for reproducibility (optional)\nset.seed(42)\n\nnumber_of_sequences_to_select &lt;- 10\n\nif (length(sequences) &lt; number_of_sequences_to_select) {\n  warning(\"There are fewer than 10 sequences in the fasta file. All sequences will be selected.\")\n  number_of_sequences_to_select &lt;- length(sequences)\n}\n\nselected_indices &lt;- sample(length(sequences), number_of_sequences_to_select)\nselected_sequences &lt;- sequences[selected_indices]\n\n```\n\n\n```{r}\n# Replace 'output.fasta' with your desired output file name\noutput_file &lt;- \"../output/output.fasta\"\nwrite.fasta(selected_sequences, names(selected_sequences), output_file, open = \"w\")\n```\n\n\n```{bash}\n#likely will not need; fix issue where gff and fa name did not match\n# sed -i 's/&gt;lcl|/&gt;/g' ../output/10_seqs.fa\n```\n\n\n```{bash}\n#needed downstream for IGV\n/home/shared/samtools-1.12/samtools faidx \\\n../output/10_seqs.fa\n```\n\n\n```{bash}\nfuzznuc -sequence ../output/10_seqs.fa -pattern CG -rformat gff -outfile ../output/CGoutput.gff\n```\nPush these files to GitHub. Grab raw urls to visualize in IGV. Fasta file is the “genome”. Take 2 screenshots and place in code file. At the top of your code page be sure to provide link to visual report (rpubs). Alternatively you can also output to markdown."
  },
  {
    "objectID": "assignments/03-knit.html",
    "href": "assignments/03-knit.html",
    "title": "Knitting Reports",
    "section": "",
    "text": "Screen Recording\n\n\n\n\n\n\n\n\n\n\n\nAssignment\n\n\n\nTLDR: Take what you did in week 1 & 2 and improve upon the code such that results have profound meaning, code is explained, and a pretty report is produced.\n\n\n\n\nNext Levelling\nFor the past two weeks you got the job done. Now lets not only learn how you go it done, but improve on the output and spend more time to create an html report. To create an attractive web-accessible report using RMarkdown, here are some steps to consider a strive for.\n\nWrite your RMarkdown content: An RMarkdown file consists of three main parts: YAML header, Markdown text, and R code chunks. Use Markdown for formatting text, and R code chunks to insert code and results.\n\n\n\nyaml\n\n---\ntitle: \"Your Report Title\"\nauthor: \"Your Name\"\ndate: \"`r format(Sys.Date(), '%B %d, %Y')`\"\noutput: \n  html_document:\n    theme: readable\n    toc: true\n    toc_float: true\n    number_sections: true\n    code_folding: show\n---\n\nThis example uses the “readable” theme and includes a table of contents, numbered sections, and code folding options.\nOther themes include: “default”, “cerulean”, “journal”, “flatly”, “darkly”, “readable”, “spacelab”, “united”, “cosmo”, “lumen”, “paper”, “sandstone”, “simplex”, “yeti”.\nCode highlights argument must be one of default, tango, pygments, kate, monochrome, espresso, zenburn, haddock, breezedark, textmate, arrow, or rstudio or a file with extension .theme.\n\nMarkdown text: Use Markdown syntax for text formatting (headings, lists, links, etc.).\n\n## Introduction\n\nThis is a sample report created using RMarkdown. You can add *italic*, **bold**, or [links](https://example.com).\n\n- Bullet point 1\n- Bullet point 2\n\nR code chunks: Insert R code and its output in your report using R code chunks.\n\n```{r setup, include=FALSE}\nknitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)\n```\n\n```{r example-plot}\nlibrary(ggplot2)\ndata(mtcars)\nggplot(mtcars, aes(x = wt, y = mpg)) + geom_point() + theme_minimal()\n```\n\nCustomize the appearance: Use CSS, HTML widgets, or additional R packages to further enhance the visual appeal of your report.\n\n\nCustom CSS: Create a separate CSS file and link it in the YAML header.\nHTML widgets: Use packages like leaflet for maps, DT for tables, or plotly for interactive plots.\nAdditional R packages: Use packages like kableExtra for formatting tables, flexdashboard for dashboard layouts, or formattable for conditional formatting.\n\n\nRender the report: In RStudio, click the “Knit” button to generate the HTML output.\nShare the report: Host the generated HTML file on a web server, or use services like GitHub Pages or RStudio Connect to share your report with others.\n\nBy combining RMarkdown with the right formatting, customizations, and packages, you can create visually appealing, web-accessible reports that effectively communicate your insights and analyses.\n\n\nPractical Aspects\nI would recommend copying prior Rmd files and renaming them with the prefix 03.1, 03.2, etc depending on how many separate pages make sense for you.\nThe specific things I will be looking for this week include\n1. Addressing prior comments\n2. Explaining what code is doing\n3. Making output more easily understandable\n4. Adding unique visual features\n5. Generation of html report.\n\n\n\n\n\n\nNote\n\n\n\nYou would want to only run (eval=TRUE) simple tasks in the knit, which is different than running chunk in Rmd.\n\n\n\n\n\n\n\n\nCaution\n\n\n\nBe careful about committing files, particularly those that are a result of cache. It is likely best to ignore those.\n\n\n\n\nInspiration\n3.1 Blast https://rpubs.com/sr320/1026094\n3.2 RNA-seq https://rpubs.com/sr320/1026190\nwhereas code is\n\nhttps://github.com/course-fish546-2023/steven-coursework/blob/main/assignments/code/3.1-blast.Rmd\nhttps://github.com/course-fish546-2023/steven-coursework/blob/main/assignments/code/3.2-dge.Rmd"
  },
  {
    "objectID": "assignments/02-DGE.html",
    "href": "assignments/02-DGE.html",
    "title": "Differential Gene Expression",
    "section": "",
    "text": "Note\n\n\n\nHere is a a fundamental overview of high-throughput differential gene expression analysis (RNA-seq)\nFor this assignment you will be taking RNA-seq reads off the sequencer, and determining what genes are expressed higher in treatment group A compared to treatments group B. Why would someone want to do this? This can tell you something about the physiological response to a “treatment”, which generally speaking could be anything from environment, disease, developmental stage, tissue, species…"
  },
  {
    "objectID": "assignments/02-DGE.html#downloading-reference",
    "href": "assignments/02-DGE.html#downloading-reference",
    "title": "Differential Gene Expression",
    "section": "Downloading reference",
    "text": "Downloading reference\nThis code grabs the Pacific oyster fasta file of genes and does so ignoring the fact that gannet does not have a security certificate to authenticate (--insecure). This is usually not recommended however we know the server.\n``` {{bash}}\ncd ../data\ncurl --insecure -O https://gannet.fish.washington.edu/seashell/bu-github/nb-2023/Cgigas/data/rna.fna\n```\n\n\n\n\n\n\nNote\n\n\n\nCreating index can take some time\n\n\nThis code is indexing the file rna.fna while also renaming it as cgigas_roslin_rna.index.\n``` {{bash}}\n/home/shared/kallisto/kallisto \\\nindex -i \\\n../data/cgigas_roslin_rna.index \\\n../data/rna.fna\n```"
  },
  {
    "objectID": "assignments/02-DGE.html#downloading-sequence-reads",
    "href": "assignments/02-DGE.html#downloading-sequence-reads",
    "title": "Differential Gene Expression",
    "section": "Downloading sequence reads",
    "text": "Downloading sequence reads\nSequence reads are on a public server at https://gannet.fish.washington.edu/seashell/bu-github/nb-2023/Cgigas/data/nopp/\n\n\n\nSample\nSampleID\n\n\nD-control\nD54\n\n\nD-control\nD55\n\n\nD-control\nD56\n\n\nD-control\nD57\n\n\nD-control\nD58\n\n\nD-control\nD59\n\n\nD-control\nM45\n\n\nD-control\nM46\n\n\nD-control\nM48\n\n\nD-control\nM49\n\n\nD-control\nM89\n\n\nD-control\nM90\n\n\nD-desiccation\nN48\n\n\nD-desiccation\nN49\n\n\nD-desiccation\nN50\n\n\nD-desiccation\nN51\n\n\nD-desiccation\nN52\n\n\nD-desiccation\nN53\n\n\nD-desiccation\nN54\n\n\nD-desiccation\nN55\n\n\nD-desiccation\nN56\n\n\nD-desiccation\nN57\n\n\nD-desiccation\nN58\n\n\nD-desiccation\nN59\n\n\n\nThis code uses recursive feature of wget (see this weeks’ reading) to get all 24 files. Additionally as with curl above we are ignoring the fact there is not security certificate with --no-check-certificate\ncd ../data \nwget --recursive --no-parent --no-directories \\\n--no-check-certificate \\\n--accept '*.fastq.gz' \\\nhttps://gannet.fish.washington.edu/seashell/bu-github/nb-2023/Cgigas/data/nopp/\nThe next chunk first creates a subdirectory\nThen performs the following steps:\n\nUses the find utility to search for all files in the ../data/ directory that match the pattern *fastq.gz.\n\nUses the basename command to extract the base filename of each file (i.e., the filename without the directory path), and removes the suffix _L001_R1_001.fastq.gz.\nRuns the kallisto quant command on each input file, with the following options:\n\n\n-i ../data/cgigas_roslin_rna.index: Use the kallisto index file located at ../data/cgigas_roslin_rna.index.\n-o ../output/kallisto_01/{}: Write the output files to a directory called ../output/kallisto_01/ with a subdirectory named after the base filename of the input file (the {} is a placeholder for the base filename).\n-t 40: Use 40 threads for the computation.\n--single -l 100 -s 10: Specify that the input file contains single-end reads (–single), with an average read length of 100 (-l 100) and a standard deviation of 10 (-s 10).\nThe input file to process is specified using the {} placeholder, which is replaced by the base filename from the previous step.\n\n``` {{bash}}\nmkdir ../output/kallisto_01\n\nfind ../data/*fastq.gz \\\n| xargs basename -s _L001_R1_001.fastq.gz | xargs -I{} /home/shared/kallisto/kallisto \\\nquant -i ../data/cgigas_roslin_rna.index \\\n-o ../output/kallisto_01/{} \\\n-t 40 \\\n--single -l 100 -s 10 ../data/{}_L001_R1_001.fastq.gz\n```\nThis command runs the abundance_estimates_to_matrix.pl script from the Trinity RNA-seq assembly software package to create a gene expression matrix from kallisto output files.\nThe specific options and arguments used in the command are as follows:\n\nperl /home/shared/trinityrnaseq-v2.12.0/util/abundance_estimates_to_matrix.pl: Run the abundance_estimates_to_matrix.pl script from Trinity.\n--est_method kallisto: Specify that the abundance estimates were generated using kallisto.\n--gene_trans_map none: Do not use a gene-to-transcript mapping file.\n--out_prefix ../output/kallisto_01: Use ../output/kallisto_01 as the output directory and prefix for the gene expression matrix file.\n--name_sample_by_basedir: Use the sample directory name (i.e., the final directory in the input file paths) as the sample name in the output matrix.\n\nAnd then there are the kallisto abundance files to use as input for creating the gene expression matrix.\n\n``` {{bash}}\nperl /home/shared/trinityrnaseq-v2.12.0/util/abundance_estimates_to_matrix.pl \\\n--est_method kallisto \\\n    --gene_trans_map none \\\n    --out_prefix ../output/kallisto_01 \\\n    --name_sample_by_basedir \\\n    ../output/kallisto_01/D54_S145/abundance.tsv \\\n    ../output/kallisto_01/D56_S136/abundance.tsv \\\n    ../output/kallisto_01/D58_S144/abundance.tsv \\\n    ../output/kallisto_01/M45_S140/abundance.tsv \\\n    ../output/kallisto_01/M48_S137/abundance.tsv \\\n    ../output/kallisto_01/M89_S138/abundance.tsv \\\n    ../output/kallisto_01/D55_S146/abundance.tsv \\\n    ../output/kallisto_01/D57_S143/abundance.tsv \\\n    ../output/kallisto_01/D59_S142/abundance.tsv \\\n    ../output/kallisto_01/M46_S141/abundance.tsv \\\n    ../output/kallisto_01/M49_S139/abundance.tsv \\\n    ../output/kallisto_01/M90_S147/abundance.tsv \\\n    ../output/kallisto_01/N48_S194/abundance.tsv \\\n    ../output/kallisto_01/N50_S187/abundance.tsv \\\n    ../output/kallisto_01/N52_S184/abundance.tsv \\\n    ../output/kallisto_01/N54_S193/abundance.tsv \\\n    ../output/kallisto_01/N56_S192/abundance.tsv \\\n    ../output/kallisto_01/N58_S195/abundance.tsv \\\n    ../output/kallisto_01/N49_S185/abundance.tsv \\\n    ../output/kallisto_01/N51_S186/abundance.tsv \\\n    ../output/kallisto_01/N53_S188/abundance.tsv \\\n    ../output/kallisto_01/N55_S190/abundance.tsv \\\n    ../output/kallisto_01/N57_S191/abundance.tsv \\\n    ../output/kallisto_01/N59_S189/abundance.tsv\n```"
  },
  {
    "objectID": "assignments/02-DGE.html#get-degs-based-on-desication",
    "href": "assignments/02-DGE.html#get-degs-based-on-desication",
    "title": "Differential Gene Expression",
    "section": "Get DEGs based on Desication",
    "text": "Get DEGs based on Desication\n``` {{r}}\ndeseq2.colData &lt;- data.frame(condition=factor(c(rep(\"control\", 12), rep(\"desicated\", 12))), \n                             type=factor(rep(\"single-read\", 24)))\nrownames(deseq2.colData) &lt;- colnames(data)\ndeseq2.dds &lt;- DESeqDataSetFromMatrix(countData = countmatrix,\n                                     colData = deseq2.colData, \n                                     design = ~ condition)\n```\n``` {{r}}\ndeseq2.dds &lt;- DESeq(deseq2.dds)\ndeseq2.res &lt;- results(deseq2.dds)\ndeseq2.res &lt;- deseq2.res[order(rownames(deseq2.res)), ]\n```\n``` {{r}}\nhead(deseq2.res)\n```\n``` {{r}}\n# Count number of hits with adjusted p-value less then 0.05\ndim(deseq2.res[!is.na(deseq2.res$padj) & deseq2.res$padj &lt;= 0.05, ])\n```\n``` {{r}}\ntmp &lt;- deseq2.res\n# The main plot\nplot(tmp$baseMean, tmp$log2FoldChange, pch=20, cex=0.45, ylim=c(-3, 3), log=\"x\", col=\"darkgray\",\n     main=\"DEG Dessication  (pval &lt;= 0.05)\",\n     xlab=\"mean of normalized counts\",\n     ylab=\"Log2 Fold Change\")\n# Getting the significant points and plotting them again so they're a different color\ntmp.sig &lt;- deseq2.res[!is.na(deseq2.res$padj) & deseq2.res$padj &lt;= 0.05, ]\npoints(tmp.sig$baseMean, tmp.sig$log2FoldChange, pch=20, cex=0.45, col=\"red\")\n# 2 FC lines\nabline(h=c(-1,1), col=\"blue\")\n```\n``` {{r}}\nwrite.table(tmp.sig, \"../output/DEGlist.tab\", row.names = T)\n```"
  },
  {
    "objectID": "assignments/09-backup.html",
    "href": "assignments/09-backup.html",
    "title": "Backup",
    "section": "",
    "text": "Assignment\n\n\n\nArchive your research repo with Zenodo AND rsync to another computer.\n\n\nThis week you will use Zenodo to archive your repo. See these instructions. Add the DOI from zenondo to your rpubs website.\nWe have used rsync in week 04. Use it this week to backup your entire directory on raven to another computer. In your code directory create a shell script named 00-rsync.sh with the rsync code used to backup your repo."
  },
  {
    "objectID": "assignments/04-hyak.html",
    "href": "assignments/04-hyak.html",
    "title": "Hyak",
    "section": "",
    "text": "Screen Recording\n\n\n\n\n\n\n\n\n\n\n\nAssignment\n\n\n\nTLDR: log into hyak, run a job, and transfer the the output using rsync\n\n\n\n\nSetup\n\nGo to https://uwnetid.washington.edu/manage/\nClick the “Computing Services” link on the left\nClick the “Hyak Server” and “Lolo Server” check boxes in the “Inactive Services” section.\nClick the “Subscribe &gt;” button at the bottom of the page.\nRead the notice and click the “Finish” button.\n\nFor two factor authentication, you can either sign up for Duo here and use your smart phone or request a security token here. Duo is much easier.\n\n\nLogging in\n\nOpen your favorite terminal\nType ssh &lt;YourUWNetID&gt;@mox.hyak.uw.edu (replace &lt;YourUWNetID&gt; with your own UW Net ID)\nInput your UWNetID password\nIf you’re signed up for 2-factor authentication via Duo, open your smart phone and approve the connection.\nYou’re logged in to a Login node for Hyak!\n\nExample:\nD-69-91-141-150:~ Sean$ ssh seanb80@mox.hyak.uw.edu\nPassword:\nEnter passcode or select one of the following options:\n\n 1. Duo Push to iOS (XXX-XXX-1239)\n 2. Phone call to iOS (XXX-XXX-1239)\n\nDuo passcode or option [1-2]: 1\nLast login: Thu Jun  8 14:59:10 2017 from d-173-250-161-130.dhcp4.washington.edu\n\n     ** NOTICE **\n     Users need to do all their interactive work, including compiling and\n     building software, on the compute nodes (n####) and NOT on the\n     head/login node (hyak.washington.edu). The login nodes are for\n     interacting with the scheduler and transferring data to and from the\n     system.\n\n     Please visit the Hyak User Wiki for more details\n     http://wiki.hyak.uw.edu\n\n\n[seanb80@mox2 ~]$\n\n\nRunning a job\nOnce logged into mox, navigate to /gscratch/scrubbed/your-username. If the dir is not there you shoud create. For every job you submit I recommend working within a directory. Usually I name these in a data format, but we can just mkdir assign_04.\nTo run a job you need to generate a shell script. Create a shell script in your code directory named 04-job.sh with contents such as.\n#!/bin/bash\n## Job Name\n#SBATCH --job-name=assign4\n## Allocation Definition\n#SBATCH --account=srlab\n#SBATCH --partition=srlab\n## Resources\n## Nodes\n#SBATCH --nodes=1\n## Walltime (days-hours:minutes:seconds format)\n#SBATCH --time=01-08:00:00\n## Memory per node\n#SBATCH --mem=100G\n#SBATCH --mail-type=ALL\n#SBATCH --mail-user=sr320@uw.edu\n## Specify the working directory for this job\n#SBATCH --chdir=/gscratch/scrubbed/sr320/assign_04\n\n#run a python script you wrote!\nmodule load intel-python3_2017\n\npython 04-hello.py\n\n# run blastx just to get manual\n/gscratch/srlab/programs/ncbi-blast-2.10.1+/bin/blastx -help\n\n#a few bash commands\npwd \n\nwhoami\n\necho \"yah! I ddi it!!!!!!!!!!\"\n\n#this writes out  tofile\necho \"yah! I ddi it!!!!!!!!!!\" &gt; text.file\nYou will also want to write some python code :)\nCreate a Python Script in the same directory name 04-hello.py with the contents:\nprint(\"Hello, World!\")\nNow we want to move these two files to mox into the assign_04 directory. To to this you will need to type something to the effect of the following in the terminal\nrsync -avz assignments/code/04-* sr320@mox.hyak.uw.edu:/gscratch/scrubbed/sr320/assign_04\nThen on mox, inside the assign_04 directory you will type”\nsbatch 04-job-sh to schedule the job.\nOnce done you should have a couple of new files in the directory.\nYou will want to check them to see if everything worked and then move the output back to your repo…\nrsync -avz sr320@mox.hyak.uw.edu:/gscratch/scrubbed/sr320/assign_04/ ."
  },
  {
    "objectID": "assignments/06-rnaseq-snp.html",
    "href": "assignments/06-rnaseq-snp.html",
    "title": "Alignment Data",
    "section": "",
    "text": "Assignment\n\n\n\nCreate and inspect and alignment files. Including visualizing and capturing “outside” graphics. Publish notebook in rpubs and provide link at top of code."
  },
  {
    "objectID": "assignments/06-rnaseq-snp.html#download-alignment-data",
    "href": "assignments/06-rnaseq-snp.html#download-alignment-data",
    "title": "Alignment Data",
    "section": "Download alignment data",
    "text": "Download alignment data\n\n\n\n\n\n\nCaution\n\n\n\nReminder - these are big files, be sure to ignore on commit.\n\n\n```{r, engine='bash'}         \ncd ../data\ncurl -O https://gannet.fish.washington.edu/seashell/bu-mox/scrubbed/120321-cvBS/19F_R1_val_1_bismark_bt2_pe.deduplicated.sorted.bam\ncurl -O https://gannet.fish.washington.edu/seashell/bu-mox/scrubbed/120321-cvBS/19F_R1_val_1_bismark_bt2_pe.deduplicated.sorted.bam.bai\n```\n\n```{r, engine='bash'}         \ncd ../data\ncurl -O https://gannet.fish.washington.edu/seashell/bu-mox/data/Cvirg-genome/GCF_002022765.2_C_virginica-3.0_genomic.fa\ncurl -O https://gannet.fish.washington.edu/seashell/bu-mox/data/Cvirg-genome/GCF_002022765.2_C_virginica-3.0_genomic.fa.fai\n```"
  },
  {
    "objectID": "assignments/06-rnaseq-snp.html#visualize-with-tview",
    "href": "assignments/06-rnaseq-snp.html#visualize-with-tview",
    "title": "Alignment Data",
    "section": "Visualize with tview",
    "text": "Visualize with tview\n\n\n\n\n\n\nImportant\n\n\n\nRun the following in Terminal as is interactive\n\n\n/home/shared/samtools-1.12/samtools tview \\\n../data/19F_R1_val_1_bismark_bt2_pe.deduplicated.sorted.bam \\\n../data/GCF_002022765.2_C_virginica-3.0_genomic.fa"
  },
  {
    "objectID": "assignments/06-rnaseq-snp.html#capture-image",
    "href": "assignments/06-rnaseq-snp.html#capture-image",
    "title": "Alignment Data",
    "section": "Capture Image",
    "text": "Capture Image\nTake a screen shot of the tview display and place in your notebook."
  },
  {
    "objectID": "assignments/06-rnaseq-snp.html#alignment",
    "href": "assignments/06-rnaseq-snp.html#alignment",
    "title": "Alignment Data",
    "section": "Alignment",
    "text": "Alignment\n```{r, engine='bash'}\n/home/shared/hisat2-2.2.1/hisat2-build \\\n-f ../data/cgigas_uk_roslin_v1_genomic-mito.fa \\\n../output/cgigas_uk_roslin_v1_genomic-mito.index\n```\n\n```{r, engine='bash'}\n/home/shared/hisat2-2.2.1/hisat2 \\\n-x ../output/cgigas_uk_roslin_v1_genomic-mito.index \\\n-p 4 \\\n-1 ../data/F143n08_R1_001.fastq.gz \\\n-2 ../data/F143n08_R2_001.fastq.gz \\\n-S ../output/F143_cgigas.sam\n```\nTake a look\n```{r, engine='bash'}\ntail -1 ../output/F143_cgigas.sam\n```\n\n```{r, engine='bash'}\n# Convert SAM to BAM, using 4 additional threads\n/home/shared/samtools-1.12/samtools view -@ 4 -bS \\\n../output/F143_cgigas.sam &gt; ../output/F143_cgigas.bam\n```\n\n```{r, engine='bash'}\n# Sort the BAM file, using 4 additional threads\n/home/shared/samtools-1.12/samtools sort -@ 4 \\\n../output/F143_cgigas.bam -o ../output/F143_cgigas_sorted.bam\n\n# Index the sorted BAM file (multi-threading is not applicable to this operation)\n/home/shared/samtools-1.12/samtools index \\\n../output/F143_cgigas_sorted.bam\n```"
  },
  {
    "objectID": "assignments/06-rnaseq-snp.html#mpileup",
    "href": "assignments/06-rnaseq-snp.html#mpileup",
    "title": "Alignment Data",
    "section": "mpileup",
    "text": "mpileup\n\n\n\n\n\n\nNow bcftools is recommended for mpileup instead of samtools (which was described in textbook)\n\n\n\n\n\n\n\n\n```{r, engine='bash'}\n/home/shared/bcftools-1.14/bcftools mpileup --threads 4 --no-BAQ \\\n--fasta-ref ../data/cgigas_uk_roslin_v1_genomic-mito.fa \\\n../output/F143_cgigas_sorted.bam &gt; ../output/F143_mpileup_output.txt\n```\n\n```{r, engine='bash'}\ntail ../output/F143_mpileup_output.txt\n```\n\n```{r, engine='bash'}\ncat ../output/F143_mpileup_output.txt \\\n| /home/shared/bcftools-1.14/bcftools call -mv -Oz \\\n&gt; ../output/F143_mpile.vcf.gz\n```\n\n\n```{r, engine='bash'}\nzgrep \"^##\" -v ../output/F143_mpile.vcf.gz | \\\nawk 'BEGIN{OFS=\"\\t\"} {split($8, a, \";\"); print $1,$2,$4,$5,$6,a[1],$9,$10}' | head\n\n```\n\n\nThe code below might not work. That is fine. The VCF in the above chunk can be used for visualization in IGV.\n\n```{r, engine='bash'}\n/home/shared/bcftools-1.14/bcftools call \\\n-v -c ../output/F143_mpile.vcf.gz \\\n&gt; ../output/F143_mpile_calls.vcf\n```"
  },
  {
    "objectID": "assignments/06-rnaseq-snp.html#visualize",
    "href": "assignments/06-rnaseq-snp.html#visualize",
    "title": "Alignment Data",
    "section": "Visualize",
    "text": "Visualize\nthese data in IGV and get a few cool snapshots.\nMinimally show bam file, and at least 2 genome feature files.\nBonus for annotating screenshots.\nUseful link: https://robertslab.github.io/resources/Genomic-Resources/#crassostrea-gigas-cgigas_uk_roslin_v1"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Course Description",
    "section": "",
    "text": "This one-week intensive graduate-level course explores the intriguing intersection of climate change, epigenetics, and marine invertebrate physiology, with a significant emphasis on the application of functional genomics. The course aims to highlight the substantial yet often overlooked role of marine invertebrates in coastal ecosystems, underlining their ecological significance and their responses to climate change at an epigenetic level.\nThe course starts by establishing a strong foundational understanding of epigenetics, climate change, and marine invertebrate physiology. Following this, students will delve into the intricate role of functional genomics in understanding these connections. Real-world examples will be extensively used to discuss how changes in climate directly and indirectly lead to alterations in the epigenetic mechanisms of various coastal marine invertebrates.\nBy the end of the course, students should have a comprehensive understanding of how climate change can influence the epigenetics and physiology of coastal marine invertebrates and the potential broader implications for marine ecosystems. The course is suitable for students with a basic understanding of genetics and climate science, although students from all disciplines are welcome as the course starts from foundational principles.\nThrough a mix of lectures, discussions, and interactive case studies, students will develop a nuanced understanding of the course topics. This course is highly recommended for students interested in marine biology, genetics, climate change, and conservation biology."
  },
  {
    "objectID": "index.html#methodology",
    "href": "index.html#methodology",
    "title": "Course Description",
    "section": "Methodology:",
    "text": "Methodology:\nThrough a mix of lectures, discussions, and interactive data analysis, students will develop a nuanced understanding of the course topics. This course is highly recommended for students interested in marine biology, genetics, climate change, and conservation biology. The general format will be lecture in the morning and interactive data analysis in the afternoon. The latter will primarily involve using R and bash."
  },
  {
    "objectID": "index.html#bibliography",
    "href": "index.html#bibliography",
    "title": "Course Description",
    "section": "Bibliography:",
    "text": "Bibliography:\nCrandall, Grace, Rhonda Elliott Thompson, Benoit Eudeline, Brent Vadopalas, Emma Timmins-Schiffman, and Steven Roberts. 2022. “Proteomic Response of Early Juvenile Pacific Oysters (Crassostrea Gigas) to Temperature.” PeerJ 10 (October): e14158. https://doi.org/10.7717/peerj.14158.\nCrandall, Grace, Pamela C. Jensen, Samuel J. White, and Steven Roberts. 2022. “Characterization of the Gene Repertoire and Environmentally Driven Expression Patterns in Tanner Crab (Chionoecetes Bairdi).” Marine Biotechnology 24 (1): 216–25. https://doi.org/10.1007/s10126–022–10100–8.\nDang, Xin, Yong-Kian Lim, Yang Li, Steven B. Roberts, Li Li, and Vengatesen Thiyagarajan. 2023. “Epigenetic-Associated Phenotypic Plasticity of the Ocean Acidification-Acclimated Edible Oyster in the Mariculture Environment.” Molecular Ecology 32 (2): 412–27. https://doi.org/10.1111/mec.16751.\nDimond, James L., and Steven B. Roberts. 2016. “Germline DNA Methylation in Reef Corals: Patterns and Potential Roles in Response to Environmental Change.” Molecular Ecology 25 (8): 1895–1904. https://doi.org/10.1111/mec.13414.\nEirin-Lopez, Jose M., and Hollie M. Putnam. 2019. “Marine Environmental Epigenetics.” Annual Review of Marine Science 11 (January): 335–68. https://doi.org/10.1146/annurev-marine–010318–095114.\nGallardo-Escárate, C., V. Valenzuela-Muñoz, S. Boltaña, G. Nuñez-Acuña, D. Valenzuela-Miranda, A. T. Gonçalves, C. Détrée, et al. 2017. “The Caligus Rogercresseyi miRNome: Discovery and Transcriptome Profiling during the Sea Lice Ontogeny.” Agri Gene 4 (June): 8–22. https://doi.org/10.1016/j.aggene.2017.03.002.\nGallardo-Escárate, Cristian, Gabriel Arriagada, Crisleri Carrera, Ana Teresa Gonçalves, Gustavo Nuñez-Acuña, Diego Valenzuela-Miranda, and Valentina Valenzuela-Muñoz. 2019. “The Race between Host and Sea Lice in the Chilean Salmon Farming: A Genomic Approach.” Reviews in Aquaculture 11 (2): 325–39. https://doi.org/10.1111/raq.12334.\nGallardo-Escárate, Cristian, Valentina Valenzuela-Muñoz, Gustavo Nuñez-Acuña, Diego Valenzuela-Miranda, Fabian J. Tapia, Marco Yévenes, Gonzalo Gajardo, et al. 2023. “Chromosome-Level Genome Assembly of the Blue Mussel Mytilus Chilensis Reveals Molecular Signatures Facing the Marine Environment.” Genes 14 (4). https://doi.org/10.3390/genes14040876.\nGavery, Mackenzie R., and Steven B. Roberts. 2014. “A Context Dependent Role for DNA Methylation in Bivalves.” Briefings in Functional Genomics 13 (3): 217–22. https://doi.org/10.1093/bfgp/elt054.\nGurr, Samuel J., Shelly A. Trigg, Brent Vadopalas, Steven B. Roberts, and Hollie M. Putnam. 2022. “Acclimatory Gene Expression of Primed Clams Enhances Robustness to Elevated pCO2.” Molecular Ecology 31 (19): 5005–23. https://doi.org/10.1111/mec.16644.\nJuárez, Oscar E., Fabiola Lafarga-De la Cruz, Ignacio Leyva-Valencia, Edgar López-Landavery, Zaúl García-Esquivel, Fernando Díaz, Denisse Re-Araujo, Brent Vadopalas, and Clara E. Galindo-Sánchez. 2018. “Transcriptomic and Metabolic Response to Chronic and Acute Thermal Exposure of Juvenile Geoduck Clams Panopea Globosa.” Marine Genomics 42 (December): 1–13. https://doi.org/10.1016/j.margen.2018.09.003.\nNúñez-Acuña, Gustavo, Constanza Sáez-Vera, Diego Valenzuela-Miranda, Valentina Valenzuela-Muñoz, and Cristian Gallardo-Escárate. 2023. “Whole-Genome Resequencing in the Sea Louse Caligus Rogercresseyi Uncovers Gene Duplications and Copy Number Variants Associated with Pesticide Resistance.” Frontiers in Marine Science 10. https://doi.org/10.3389/fmars.2023.1112691.\nPutnam, Hollie M., Shelly A. Trigg, Samuel J. White, Laura H. Spencer, Brent Vadopalas, Aparna Natarajan, Jonathan Hetzel, et al. 2022. “Dynamic DNA Methylation Contributes to Carryover Effects and Beneficial Acclimatization in Geoduck Clams.” bioRxiv. https://doi.org/10.1101/2022.06.24.497506.\nRoberts, Steven B., and Mackenzie R. Gavery. 2012. “Is There a Relationship between DNA Methylation and Phenotypic Plasticity in Invertebrates?” Frontiers in Physiology 2 (January): 116. https://doi.org/10.3389/fphys.2011.00116.\nRoberts, Steven B., and Mackenzie R Gavery. 2017. “Epigenetic Considerations in Aquaculture.” PeerJ 5 (December): e4147. https://doi.org/10.7717/peerj.4147.\nSadler, Kirsten C. 2023. “Epigenetics across the Evolutionary Tree: New Paradigms from Non-Model Animals.” BioEssays: News and Reviews in Molecular, Cellular and Developmental Biology 45 (1): e2200036. https://doi.org/10.1002/bies.202200036.\nSilliman, Katherine, Laura H. Spencer, Samuel J. White, and Steven B. Roberts. 2023. “Epigenetic and Genetic Population Structure Is Coupled in a Marine Invertebrate.” Genome Biology and Evolution 15 (2). https://doi.org/10.1093/gbe/evad013.\nSpencer, Laura H., Erin Horkan, Ryan Crim, and Steven B. Roberts. 2021. “Latent Effects of Winter Warming on Olympia Oyster Reproduction and Larval Viability.” Journal of Experimental Marine Biology and Ecology 542–543 (September): 151604. https://doi.org/10.1016/j.jembe.2021.151604.\nSpencer, Laura H., Yaamini R. Venkataraman, Ryan Crim, Stuart Ryan, Micah J. Horwith, and Steven B. Roberts. 2020. “Carryover Effects of Temperature and pCO2 across Multiple Olympia Oyster Populations.” Ecological Applications: A Publication of the Ecological Society of America 30 (3): e02060. https://doi.org/10.1002/eap.2060.\nTimmins-Schiffman, Emma B., Grace A. Crandall, Brent Vadopalas, Michael E. Riffle, Brook L. Nunn, and Steven B. Roberts. 2017. “Integrating Discovery-Driven Proteomics and Selected Reaction Monitoring To Develop a Noninvasive Assay for Geoduck Reproductive Maturation.” Journal of Proteome Research 16 (9): 3298–3309. https://doi.org/10.1021/acs.jproteome.7b00288.\nTimmins-Schiffman, Emma, Samuel J. White, Rhonda Elliott Thompson, Brent Vadopalas, Benoit Eudeline, Brook L. Nunn, and Steven B. Roberts. 2021. “Coupled Microbiome Analyses Highlights Relative Functional Roles of Bacteria in a Bivalve Hatchery.” Environmental Microbiome 16 (1): 7. https://doi.org/10.1186/s40793–021–00376-z.\nTrigg, Shelly A., Yaamini R. Venkataraman, Mackenzie R. Gavery, Steven B. Roberts, Debashish Bhattacharya, Alan Downey-Wall, Jose M. Eirin-Lopez, et al. 2022. “Invertebrate Methylomes Provide Insight into Mechanisms of Environmental Tolerance and Reveal Methodological Biases.” Molecular Ecology Resources 22 (4): 1247–61. https://doi.org/10.1111/1755–0998.13542.\nValenzuela-Muñoz, Valentina, Juan Antonio Váldes, and Cristian Gallardo-Escárate. 2021. “Transcriptome Profiling of Long Non-Coding RNAs During the Atlantic Salmon Smoltification Process.” Marine Biotechnology 23 (2): 308–20. https://doi.org/10.1007/s10126–021–10024–9.\nVenkataraman, Yaamini R., Alan M. Downey-Wall, Justin Ries, Isaac Westfield, Samuel J. White, Steven B. Roberts, and Kathleen E. Lotterhos. 2020. “General DNA Methylation Patterns and Environmentally-Induced Differential Methylation in the Eastern Oyster (Crassostrea Virginica).” Frontiers in Marine Science 7. https://doi.org/10.3389/fmars.2020.00225.\nVenkataraman, Yaamini R., Samuel J. White, and Steven B. Roberts. 2022. “Differential DNA Methylation in Pacific Oyster Reproductive Tissue in Response to Ocean Acidification.” BMC Genomics 23 (1): 556. https://doi.org/10.1186/s12864–022–08781–5.\nWanamaker, Shelly A., Kaitlyn R. Mitchell, Rhonda Elliott Thompson, Benoit Eudeline, Brent Vadopalas, Emma B. Timmins-Schiffman, and Steven B. Roberts. 2020. “Temporal Proteomic Profiling Reveals Insight into Critical Developmental Processes and Temperature-Influenced Physiological Response Differences in a Bivalve Mollusc.” BMC Genomics 21 (1): 723. https://doi.org/10.1186/s12864–020–07127–3."
  },
  {
    "objectID": "Description.html",
    "href": "Description.html",
    "title": "Epigenetic Phenomena Connecting Climate Change and Coastal Marine Species",
    "section": "",
    "text": "Course Title: Epigenetic Phenomena Connecting Climate Change and Coastal Marine Species\nCourse Duration: 1 week\nCourse Description:\nThis one-week intensive graduate-level course explores the intriguing intersection of climate change, epigenetics, and marine invertebrate physiology, with a significant emphasis on the application of functional genomics. The course aims to highlight the substantial yet often overlooked role of marine invertebrates in coastal ecosystems, underlining their ecological significance and their responses to climate change at an epigenetic level.\nThe course starts by establishing a strong foundational understanding of epigenetics, climate change, and marine invertebrate physiology. Following this, students will delve into the intricate role of functional genomics in understanding these connections. Real-world examples will be extensively used to discuss how changes in climate directly and indirectly lead to alterations in the epigenetic mechanisms of various coastal marine invertebrates.\nBy the end of the course, students should have a comprehensive understanding of how climate change can influence the epigenetics and physiology of coastal marine invertebrates and the potential broader implications for marine ecosystems. The course is suitable for students with a basic understanding of genetics and climate science, although students from all disciplines are welcome as the course starts from foundational principles.\nThrough a mix of lectures, discussions, and interactive case studies, students will develop a nuanced understanding of the course topics. This course is highly recommended for students interested in marine biology, genetics, climate change, and conservation biology.\nObjectives:\nObjective 1: Understanding the Interplay between Climate Change, Epigenetics, and Marine Invertebrate Physiology\nThe first objective of the course is to help students gain a comprehensive understanding of how climate change impacts the physiological processes of marine invertebrates, especially through the lens of epigenetics. Students will learn about the fundamentals of marine invertebrate physiology and the various ways in which climate change-related stressors can influence these physiological systems, causing significant changes at an epigenetic level.\nObjective 2: Gaining Proficiency in Functional Genomics\nThe second objective is to ensure that students gain proficiency in the methodologies and applications of functional genomics, particularly in studying the epigenetic changes in marine invertebrates in response to climate change. Students will learn about various genomic techniques, their applications, and how to interpret data from these studies to gain insights into the epigenetic responses of marine invertebrates to climate stressors.\nObjective 3: Developing Skills for Analyzing and Evaluating Scientific Research\nThe third objective is to develop students’ ability to critically analyze and evaluate scientific literature in the field of marine invertebrate epigenetics and climate change. Through case studies and discussions on recent research, students will hone their skills in analyzing experimental designs, interpreting results, and understanding the broader implications of research findings for climate change adaptation and marine conservation efforts.\nContents:\nThis intensive course is divided into five modules:\n\nBasics of Epigenetics and Climate Change: This module provides an overview of climate change science and the principles of epigenetics. We discuss how the two fields intersect and introduce the concept of environmental epigenetics.\nPhysiology of Marine Invertebrates: This module focuses on the essential aspects of marine invertebrate physiology, covering topics such as feeding, digestion, reproduction, and response to environmental stressors.\nImpact of Climate Change on Marine Invertebrate Physiology: This section explores the specific impacts of various climate change-related factors, such as ocean acidification, temperature rise, and salinity changes on the physiology of marine invertebrates.\nFunctional Genomics and Epigenetics: This module delves into the methodologies and applications of functional genomics in studying epigenetic changes. Various genomic techniques such as DNA methylation analysis, chromatin immunoprecipitation, and RNA sequencing will be discussed.\nCase Studies and Current Research: The final module will involve an in-depth examination of recent research studies in the field. Students will be encouraged to critically analyze the studies and discuss the implications of the findings on our understanding of climate change impacts and the role of epigenetics in marine invertebrate adaptation.\n\nMethodology:\nThrough a mix of lectures, discussions, and interactive data analysis, students will develop a nuanced understanding of the course topics. This course is highly recommended for students interested in marine biology, genetics, climate change, and conservation biology. The general format will be lecture in the morning and interactive data analysis in the afternoon. The latter will primarily involve using R and bash.\nBibliography:\nCrandall, Grace, Rhonda Elliott Thompson, Benoit Eudeline, Brent Vadopalas, Emma Timmins-Schiffman, and Steven Roberts. 2022. “Proteomic Response of Early Juvenile Pacific Oysters (Crassostrea Gigas) to Temperature.” PeerJ 10 (October): e14158. https://doi.org/10.7717/peerj.14158.\nCrandall, Grace, Pamela C. Jensen, Samuel J. White, and Steven Roberts. 2022. “Characterization of the Gene Repertoire and Environmentally Driven Expression Patterns in Tanner Crab (Chionoecetes Bairdi).” Marine Biotechnology 24 (1): 216–25. https://doi.org/10.1007/s10126-022-10100-8.\nDang, Xin, Yong-Kian Lim, Yang Li, Steven B. Roberts, Li Li, and Vengatesen Thiyagarajan. 2023. “Epigenetic-Associated Phenotypic Plasticity of the Ocean Acidification-Acclimated Edible Oyster in the Mariculture Environment.” Molecular Ecology 32 (2): 412–27. https://doi.org/10.1111/mec.16751.\nDimond, James L., and Steven B. Roberts. 2016. “Germline DNA Methylation in Reef Corals: Patterns and Potential Roles in Response to Environmental Change.” Molecular Ecology 25 (8): 1895–1904. https://doi.org/10.1111/mec.13414.\nEirin-Lopez, Jose M., and Hollie M. Putnam. 2019. “Marine Environmental Epigenetics.” Annual Review of Marine Science 11 (January): 335–68. https://doi.org/10.1146/annurev-marine-010318-095114.\nGallardo-Escárate, C., V. Valenzuela-Muñoz, S. Boltaña, G. Nuñez-Acuña, D. Valenzuela-Miranda, A. T. Gonçalves, C. Détrée, et al. 2017. “The Caligus Rogercresseyi miRNome: Discovery and Transcriptome Profiling during the Sea Lice Ontogeny.” Agri Gene 4 (June): 8–22. https://doi.org/10.1016/j.aggene.2017.03.002.\nGallardo-Escárate, Cristian, Gabriel Arriagada, Crisleri Carrera, Ana Teresa Gonçalves, Gustavo Nuñez-Acuña, Diego Valenzuela-Miranda, and Valentina Valenzuela-Muñoz. 2019. “The Race between Host and Sea Lice in the Chilean Salmon Farming: A Genomic Approach.” Reviews in Aquaculture 11 (2): 325–39. https://doi.org/10.1111/raq.12334.\nGallardo-Escárate, Cristian, Valentina Valenzuela-Muñoz, Gustavo Nuñez-Acuña, Diego Valenzuela-Miranda, Fabian J. Tapia, Marco Yévenes, Gonzalo Gajardo, et al. 2023. “Chromosome-Level Genome Assembly of the Blue Mussel Mytilus Chilensis Reveals Molecular Signatures Facing the Marine Environment.” Genes 14 (4). https://doi.org/10.3390/genes14040876.\nGavery, Mackenzie R., and Steven B. Roberts. 2014. “A Context Dependent Role for DNA Methylation in Bivalves.” Briefings in Functional Genomics 13 (3): 217–22. https://doi.org/10.1093/bfgp/elt054.\nGurr, Samuel J., Shelly A. Trigg, Brent Vadopalas, Steven B. Roberts, and Hollie M. Putnam. 2022. “Acclimatory Gene Expression of Primed Clams Enhances Robustness to Elevated pCO2.” Molecular Ecology 31 (19): 5005–23. https://doi.org/10.1111/mec.16644.\nJuárez, Oscar E., Fabiola Lafarga-De la Cruz, Ignacio Leyva-Valencia, Edgar López-Landavery, Zaúl García-Esquivel, Fernando Díaz, Denisse Re-Araujo, Brent Vadopalas, and Clara E. Galindo-Sánchez. 2018. “Transcriptomic and Metabolic Response to Chronic and Acute Thermal Exposure of Juvenile Geoduck Clams Panopea Globosa.” Marine Genomics 42 (December): 1–13. https://doi.org/10.1016/j.margen.2018.09.003.\nNúñez-Acuña, Gustavo, Constanza Sáez-Vera, Diego Valenzuela-Miranda, Valentina Valenzuela-Muñoz, and Cristian Gallardo-Escárate. 2023. “Whole-Genome Resequencing in the Sea Louse Caligus Rogercresseyi Uncovers Gene Duplications and Copy Number Variants Associated with Pesticide Resistance.” Frontiers in Marine Science 10. https://doi.org/10.3389/fmars.2023.1112691.\nPutnam, Hollie M., Shelly A. Trigg, Samuel J. White, Laura H. Spencer, Brent Vadopalas, Aparna Natarajan, Jonathan Hetzel, et al. 2022. “Dynamic DNA Methylation Contributes to Carryover Effects and Beneficial Acclimatization in Geoduck Clams.” bioRxiv. https://doi.org/10.1101/2022.06.24.497506.\nRoberts, Steven B., and Mackenzie R. Gavery. 2012. “Is There a Relationship between DNA Methylation and Phenotypic Plasticity in Invertebrates?” Frontiers in Physiology 2 (January): 116. https://doi.org/10.3389/fphys.2011.00116.\nRoberts, Steven B., and Mackenzie R Gavery. 2017. “Epigenetic Considerations in Aquaculture.” PeerJ 5 (December): e4147. https://doi.org/10.7717/peerj.4147.\nSadler, Kirsten C. 2023. “Epigenetics across the Evolutionary Tree: New Paradigms from Non-Model Animals.” BioEssays: News and Reviews in Molecular, Cellular and Developmental Biology 45 (1): e2200036. https://doi.org/10.1002/bies.202200036.\nSilliman, Katherine, Laura H. Spencer, Samuel J. White, and Steven B. Roberts. 2023. “Epigenetic and Genetic Population Structure Is Coupled in a Marine Invertebrate.” Genome Biology and Evolution 15 (2). https://doi.org/10.1093/gbe/evad013.\nSpencer, Laura H., Erin Horkan, Ryan Crim, and Steven B. Roberts. 2021. “Latent Effects of Winter Warming on Olympia Oyster Reproduction and Larval Viability.” Journal of Experimental Marine Biology and Ecology 542-543 (September): 151604. https://doi.org/10.1016/j.jembe.2021.151604.\nSpencer, Laura H., Yaamini R. Venkataraman, Ryan Crim, Stuart Ryan, Micah J. Horwith, and Steven B. Roberts. 2020. “Carryover Effects of Temperature and pCO2 across Multiple Olympia Oyster Populations.” Ecological Applications: A Publication of the Ecological Society of America 30 (3): e02060. https://doi.org/10.1002/eap.2060.\nTimmins-Schiffman, Emma B., Grace A. Crandall, Brent Vadopalas, Michael E. Riffle, Brook L. Nunn, and Steven B. Roberts. 2017. “Integrating Discovery-Driven Proteomics and Selected Reaction Monitoring To Develop a Noninvasive Assay for Geoduck Reproductive Maturation.” Journal of Proteome Research 16 (9): 3298–3309. https://doi.org/10.1021/acs.jproteome.7b00288.\nTimmins-Schiffman, Emma, Samuel J. White, Rhonda Elliott Thompson, Brent Vadopalas, Benoit Eudeline, Brook L. Nunn, and Steven B. Roberts. 2021. “Coupled Microbiome Analyses Highlights Relative Functional Roles of Bacteria in a Bivalve Hatchery.” Environmental Microbiome 16 (1): 7. https://doi.org/10.1186/s40793-021-00376-z.\nTrigg, Shelly A., Yaamini R. Venkataraman, Mackenzie R. Gavery, Steven B. Roberts, Debashish Bhattacharya, Alan Downey-Wall, Jose M. Eirin-Lopez, et al. 2022. “Invertebrate Methylomes Provide Insight into Mechanisms of Environmental Tolerance and Reveal Methodological Biases.” Molecular Ecology Resources 22 (4): 1247–61. https://doi.org/10.1111/1755-0998.13542.\nValenzuela-Muñoz, Valentina, Juan Antonio Váldes, and Cristian Gallardo-Escárate. 2021. “Transcriptome Profiling of Long Non-Coding RNAs During the Atlantic Salmon Smoltification Process.” Marine Biotechnology 23 (2): 308–20. https://doi.org/10.1007/s10126-021-10024-9.\nVenkataraman, Yaamini R., Alan M. Downey-Wall, Justin Ries, Isaac Westfield, Samuel J. White, Steven B. Roberts, and Kathleen E. Lotterhos. 2020. “General DNA Methylation Patterns and Environmentally-Induced Differential Methylation in the Eastern Oyster (Crassostrea Virginica).” Frontiers in Marine Science 7. https://doi.org/10.3389/fmars.2020.00225.\nVenkataraman, Yaamini R., Samuel J. White, and Steven B. Roberts. 2022. “Differential DNA Methylation in Pacific Oyster Reproductive Tissue in Response to Ocean Acidification.” BMC Genomics 23 (1): 556. https://doi.org/10.1186/s12864-022-08781-5.\nWanamaker, Shelly A., Kaitlyn R. Mitchell, Rhonda Elliott Thompson, Benoit Eudeline, Brent Vadopalas, Emma B. Timmins-Schiffman, and Steven B. Roberts. 2020. “Temporal Proteomic Profiling Reveals Insight into Critical Developmental Processes and Temperature-Influenced Physiological Response Differences in a Bivalve Mollusc.” BMC Genomics 21 (1): 723. https://doi.org/10.1186/s12864-020-07127-3."
  },
  {
    "objectID": "lectures/00-before.html",
    "href": "lectures/00-before.html",
    "title": "Course Preparation",
    "section": "",
    "text": "This course is designed for graduate students with core computational competence and an appropriate data set for analyses to be performed during the course. Before class starts (and add codes are distributed) the following tasks need to be completed.\n\nSubmit your github ID using this form.\nEstablish account on Roberts Lab (srlab) hyak account\nRead: Introducing the Shell\nRead: Navigating Files and Directories\nComplete this bash tutorial\nRead: Organize your data and code\nLearn (remember) proper project (repo) structure.\n\n\nFile StructureDataCode\n\n\n\nGood file structure\n\nAll project files in one main folder\nSubfolders (data, code, output)\n\nMain folder is R project\n\nSelf-contained project\nUse relative instead of absolute paths\n\nGood folder & file names\n\nDescriptive but not too long\nNo spaces\nConsistent format\n\n\n\n\n\nRaw data\n\nIn separate folder from cleaned data\nNever change!\nEach file should have metadata\n\n\n\n\n\nScripts with code\n\nRelative file paths to read in and create files\nLots of comments\nOrder: libraries, data, user-created functions, everything else\nGood variable & column names"
  },
  {
    "objectID": "lectures/07-methylation.html#dna-methylation---software",
    "href": "lectures/07-methylation.html#dna-methylation---software",
    "title": "Epigenetics",
    "section": "DNA methylation - Software",
    "text": "DNA methylation - Software\nSeveral software tools are available for characterizing DNA methylation, including:\n\nBismark: This software aligns bisulfite-treated sequencing reads and generates methylation reports and visualization tools.\nMethylKit: It is a user-friendly package to perform DNA methylation analysis using NGS data, including differential methylation analysis, clustering, and annotation.\nMethpipe: It provides a suite of programs to preprocess bisulfite sequencing data, call differentially methylated regions, and perform enrichment analysis.\nRnBeads: This software offers a comprehensive and modular analysis of DNA methylation data, including normalization, quality control, visualization, and integration with other molecular features.\n\nIn summary, epigenetics is a fascinating field that has revolutionized our understanding of gene regulation and the impact of the environment on health and disease. By utilizing various techniques and software tools, bioinformaticians can explore epigenetic mechanisms in an integrative and comprehensive manner.\n\nVisualization\nThere are several software packages available for visualizing DNA methylation variation. Three popular ones are:\n\nIGV (Integrative Genomics Viewer): IGV is a high-performance, easy-to-use, interactive software tool for visualizing and exploring genomic data. It allows users to view various types of data, including DNA methylation, gene expression, and sequence alignments. IGV supports a wide range of file formats, making it a versatile option for researchers. You can find more information at https://software.broadinstitute.org/software/igv/.\nMethylation plotter: Methylation plotter is a user-friendly web tool that creates graphical representations of DNA methylation data. It allows users to upload data in a variety of formats and customize the visualization to display various methylation contexts, such as CpG islands, gene promoters, or gene bodies. You can access Methylation plotter at https://bioinfo2.ugr.es/methylation_plotter/.\nSeqMonk: SeqMonk is an open-source, interactive, and highly customizable software for the visualization and analysis of large genomic datasets, including DNA methylation data. It can handle data from various high-throughput sequencing platforms and provides numerous analysis options to help users explore and understand their data. You can find more information about SeqMonk and download the software at https://www.bioinformatics.babraham.ac.uk/projects/seqmonk/.\n\nThese software packages cater to different user needs and offer various features for visualizing and analyzing DNA methylation data. The choice of software largely depends on the specific requirements of your project and your level of expertise with genomic data analysis tools."
  },
  {
    "objectID": "lectures/08-ranges.html",
    "href": "lectures/08-ranges.html",
    "title": "Genomic Ranges",
    "section": "",
    "text": "Screen Recording\n\n\n\n\n\n\n\nReading\nTextbook: Working with Range Data A crash course in Genomic Ranges 263-269\nWorking with Range Data with BEDtools 329-337\n\n\nObjectives\nUnderstand genomic ranges, what they are, and what they are useful for.\n\n\nGenomic Ranges:\nGenomic ranges are a way to represent regions of a genome, typically the start and end of specific features like genes, exons, or regulatory regions.\nA genomic range is typically represented by three pieces of information:\n\nChromosome: The chromosome on which the feature is located.\nStart position: The position on the chromosome where the feature begins.\nEnd position: The position on the chromosome where the feature ends.\n\nIt’s important to note that the start and end positions can refer to the genomic coordinates (see below), but it can also be as simple as the number of base pairs from the start of the chromosome.\n\n\nCoordinate Systems:\nIn genomics, there are two main types of coordinate systems:\n\n1-based coordinate system: In this system, the first base of a sequence is numbered as 1. This system is commonly used in most biological research, including genome annotations in databases such as GenBank and Ensembl.\n0-based coordinate system: In this system, the first base of a sequence is numbered as 0. This system is commonly used in some computational tools and languages, like Python and in the BED file format widely used in bioinformatics.\n\nThe distinction between 1-based and 0-based is important because it can lead to off-by-one errors if not accounted for.\nGenomicRanges Package:\nIn the R programming language, the GenomicRanges package is often used for handling genomic ranges. It provides a convenient and consistent way to handle and manipulate genomic ranges. Some of the functionalities include:\n\nCreating genomic ranges\nFinding overlaps between genomic ranges\nFinding the nearest genomic range\nShifting or narrowing genomic ranges\n\nHere’s a basic example of how you can create a GenomicRange object in R:\n# Loading the library\nlibrary(GenomicRanges)\n\n# Creating a GenomicRanges object\ngr &lt;- GRanges(seqnames = \"chr1\", \n              ranges = IRanges(start = c(100, 200), end = c(150, 250)))\n\n# Printing the object\nprint(gr)\nUnderstanding genomic ranges and coordinate systems is crucial when dealing with any kind of genomic data, as it allows precise representation and manipulation of genomic features. Be sure to always check the documentation of any tool or database you’re using to understand what kind of coordinate system it uses, and always be consistent in your own work.\n\n\nBEDTools:\nBEDTools is a suite of utilities for comparing and manipulating genomic features in various formats such as BED, GFF/GTF, VCF, and BAM. These tools are incredibly powerful for handling genomic intervals and are often used in bioinformatics pipelines. Some of its features include:\n\nComparing genomic features\nManipulating genomic features\nCounting genomic features\nCreating coverage tracks\n\nHere’s a basic rundown of some of the most commonly used BEDTools subcommands:\n\nintersect\nbedtools intersect is used to find overlapping regions between two BED files. For example, you could use it to find genes (from a genes.BED file) that overlap with regulatory regions (from a regulatory_regions.BED file):\nbedtools intersect -a genes.BED -b regulatory_regions.BED\nmerge\nbedtools merge combines overlapping intervals into a single interval. For example, if you have a BED file with multiple overlapping regions, you can merge them into single, continuous regions:\nbedtools merge -i input.BED\nsort\nbedtools sort sorts a BED file by chromosome and then by start position. This is often necessary before using other BEDTools commands, as many require sorted input:\nbedtools sort -i input.BED\nclosest\nbedtools closest finds the closest non-overlapping interval. For example, for each gene (in genes.BED), you could find the closest regulatory region (in regulatory_regions.BED):\nbedtools closest -a genes.BED -b regulatory_regions.BED\ncoverage\nbedtools coverage calculates the coverage of one set of intervals over another. For example, you could calculate the coverage of sequencing reads (in reads.BED) over genes (in genes.BED):\nbedtools coverage -a genes.BED -b reads.BED\n\nLastly, BEDTools is flexible and powerful, but it can be complex. Always check the documentation and test your commands on small, controlled datasets to make sure they’re doing what you expect."
  },
  {
    "objectID": "lectures/05-knit-slides.html#creating-a-presentation-in-quarto",
    "href": "lectures/05-knit-slides.html#creating-a-presentation-in-quarto",
    "title": "Knitting up some slides",
    "section": "Creating a Presentation in Quarto",
    "text": "Creating a Presentation in Quarto\nFollow these steps to create a presentation using Quarto:\n\nCreate a new Quarto presentation file: Create a new file with the extension .qmd (e.g., presentation.qmd). This file will contain your presentation content and code chunks.\nAdd YAML metadata: At the top of your .qmd file, include a YAML metadata block to specify the output format and other options. For a presentation, use the following:\n\n---\ntitle: \"Your Presentation Title\"\nformat: beamer\noutput:\n  beamer_presentation:\n    theme: metropolis\n    slide_level: 2\n---\n\nWrite your presentation: Use standard Markdown syntax for formatting your text and headings. To create a new slide, use a level-2 heading (e.g., ## Slide Title). You can include R, Python, or Julia code chunks using the following syntax:\n\n\n```{r}\n# Your R code here\n```\n\n```{python}\n# Your Python code here\n```"
  },
  {
    "objectID": "lectures/06-snps.html#snps-in-rna-seq-data",
    "href": "lectures/06-snps.html#snps-in-rna-seq-data",
    "title": "Alignment Data and Genetic Variation",
    "section": "SNPs in RNA-seq data",
    "text": "SNPs in RNA-seq data\nFinding SNPs (Single Nucleotide Polymorphisms) in RNA-seq data is an essential step in understanding genetic variation and its effects on gene expression. Here is a summary of the process:\n\nQuality control and preprocessing: Start by assessing the quality of your raw RNA-seq reads using tools like FastQC. Trim low-quality bases and adapter sequences using software like Trimmomatic or Cutadapt to ensure accurate downstream analysis.\nAlignment: Align the cleaned RNA-seq reads to a reference genome using a spliced aligner like STAR, HISAT2, or TopHat2. These tools can handle the intron-exon structure of RNA-seq data and provide accurate alignments.\nSort and index alignments: Use SAMtools or Picard to convert the alignment output (SAM) to a binary format (BAM), sort, and index the aligned reads for efficient data processing.\nVariant calling: Employ a variant caller such as GATK’s HaplotypeCaller, SAMtools mpileup, or FreeBayes to identify SNPs and other genetic variants in your RNA-seq data. These tools use statistical models to call SNPs based on the differences observed between aligned reads and the reference genome.\nFiltering: Apply quality filters to remove low-confidence variant calls. Use tools like GATK’s VariantFiltration or BCFtools filter to set quality thresholds based on parameters like depth, quality score, strand bias, and mapping quality.\nAnnotation: Annotate the filtered SNPs using software like ANNOVAR, SnpEff, or VEP to gain insights into their potential functional effects on genes, transcripts, and proteins.\nDownstream analysis: Investigate the potential impact of SNPs on gene expression, alternative splicing, or allele-specific expression using tools like DESeq2, edgeR, or Cufflinks.\n\nRemember that this is only a general summary and specific steps might vary depending on the organism, reference genome, and experimental conditions. Always consult the documentation of the chosen tools and follow best practices for RNA-seq data analysis."
  },
  {
    "objectID": "lectures/01-start-up.html",
    "href": "lectures/01-start-up.html",
    "title": "Getting Started",
    "section": "",
    "text": "Screen Recording\n\n\n\n\n\n\n\nText Reading\nHow to Learn Bioinformatics 1-18;\nSetting Up and Managing a Bioinformatics Project 21-35;\n\n\n\nObjectives\n\n\nSetting up for Success!\nAs part of this class you will be learning fundamental skills in working with genomic data. In addition you will be carrying out an independent project throughout the quarter. Generally Tuesday will be learning a skillset and Thursday will be working on your independent project.\nEach student will have two GitHub repositories, one where you complete “classwork” and as one devoted to your project. Both need to be in the organization course-fish546-2023.\nThe name of these repos:\npreferredname-classwork and\npreferredname-projectdescriptor\n\n\n\n\n\n\nImportant\n\n\n\nMake sure you have you local repo clone in logical location (eg ~/Documents/GitHub) and that you do not move, nor place in Dropbox or similar syncing directory.\n\n\nBe sure to comply with guidelines\n\nFile StructureDataCode\n\n\n\nGood file structure\n\nAll project files in one main folder\nSubfolders (data, code, output)\n\nMain folder is R project\n\nSelf-contained project\nUse relative instead of absolute paths\n\nGood folder & file names\n\nDescriptive but not too long\nNo spaces\nConsistent format\n\n\n\n\n\nRaw data\n\nIn separate folder from cleaned data\nNever change!\nEach file should have metadata\n\n\n\n\n\nScripts with code\n\nRelative file paths to read in and create files\nLots of comments\nOrder: libraries, data, user-created functions, everything else\nGood variable & column names\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nMax GitHub file size is 100MB\n\n\nThere will be times when files are two big to include in repositories (or even you laptop). There will also be times when you have to run code outside of a GitHub repository. You will need to determine a way to effectively document this in your repository.\n\n\nComputers\nTo no surprise you will need to have some type of computer to do your analysis. This might seem trivial, but is not. In some instances you might you multiple machines. Generally speaking there are two primary consideration- RAM and CPUs (ie memory and power). Memory comes into play in running programs need to temporary store information like transcriptome assembly. Power is beneficial from programs that can use mulitple CPUs. For example if hardware has 48 cores, it could run a program faster than one with 4 cores. Note there are a lot of nuances here but it is good to have this vocabulary. Some of the work takes a long time (even on big machines) - meaning hours to weeks, thus hardware access needs consideration.\nSome of the options you have are\n- your personal laptop (borrowed laptop) - duration limited?\n- JupyterHub Instance - UW cloud machine, duration limited,\n- Roberts Lab Raven Rstudio server - cloud machine\n- Hyak Supercomputer - powerful - advanced interface\nFor simply typing (something that is also important) you can do this with almost anything with a keyboard. Note that GitHub will be the platform that allows you to move across machines.\nMost of the Assignments are designed to run on lightweigt hardware, and we might want to try experience different platforms to see what works best for you. It is important to keep in mind that if you using muliptle machines there is the possibility of causing git conflicts.\nOrganization and thought is important, particulary when it comes to this.\n\n\nWorking in the command-line\nHaving already reviewed the prep material and completed the bash tutorial you are now ready to get to coding.\nFor the first task you will take an unknown multi-fasta file and annotate it using blast. You are welcome to do this in terminal, Rstudio, or jupyter. My recommendation, and how I will demonstrate is using Rmarkdown. Once you have have your project structured, we will download software, databases, a fasta file and run the code.\nIn your code directory create a file.\n01-blast.Rmd\n\n\n\n\n\n\nTip\n\n\n\nRmarkdown is a good option as you can use markdown, add pictures and more!"
  },
  {
    "objectID": "lectures/10-lastmile.html",
    "href": "lectures/10-lastmile.html",
    "title": "Final Presentations",
    "section": "",
    "text": "Screen Recording\n\n\n\n\n\n\n\nObjectives\nTo get you repo in pristine condition and effectively present your research compendium via your rpubs page.\n\n\nDetails\nOn Thursday you will be presenting your research project to the class. Everyone will have 10 minutes.\nThis presentation along with the rpubs compendium contents will be your Project Completion assignment grade.\nThis specific breakdown the Project Completion grade is as follows.\n\nGithub repo with clear directory structure, files in proper directory, code files numbered in order. (25%)\nRpubs page that has clear description of project at top targeting a general audience. (10%)\nRpubs page that has clear workflow taking initial files to resulting finding (in figure format). Visualization (eg head) of intermediate files and simple access to ultimate findings (eg tables). (30%)\nPresentation to the class that follows logical, planned highlighting of content. Things to include are research problem, workflow, findings, implication of findings and next steps (35%)\n\nMake sure there is only one URL (and it is the correct url) here"
  },
  {
    "objectID": "lectures/03-rstudio.html#adding-tables",
    "href": "lectures/03-rstudio.html#adding-tables",
    "title": "Rstudio Fundamentals",
    "section": "Adding tables:",
    "text": "Adding tables:\n\nMarkdown syntax: You can create a simple table using pipes | and hyphens -. Here’s an example:\n\n| Column1 | Column2 | Column3 |\n|---------|---------|---------|\n| A       | B       | C       |\n| X       | Y       | Z       |\nThis will create a table with two rows and three columns.\n\nR code: You can create more complex tables using R packages like kable from the knitr package, or gt and flextable. Here’s an example using kable:\n\n```{r}\nlibrary(knitr)\n\ndata &lt;- data.frame(\n  Column1 = c(\"A\", \"X\"),\n  Column2 = c(\"B\", \"Y\"),\n  Column3 = c(\"C\", \"Z\")\n)\n\nkable(data, caption = \"An example table\")\n```\nThis will generate a table with the specified data and caption."
  },
  {
    "objectID": "lectures/03-rstudio.html#adding-images",
    "href": "lectures/03-rstudio.html#adding-images",
    "title": "Rstudio Fundamentals",
    "section": "Adding images:",
    "text": "Adding images:\n\nMarkdown syntax: You can insert an image using the following syntax: ![alt text](path/to/image \"Optional title\"). Here’s an example:\n\n![Example image](path/to/image.jpg \"Optional title\")\nMake sure to replace path/to/image.jpg with the actual file path or URL of the image.\n\nR code: You can also add images using R code, especially if you’re generating images with R plots. Here’s two examples:\n\n```{r}\nplot(cars, main = \"An example plot\", xlab = \"Speed\", ylab = \"Distance\")\n```\n```{r schemat, echo = FALSE, out.width = “70%”, fig.align = “center”}\nknitr::include_graphics(“img/ncbi.png”)\n```\nThe benefit of the this code as opposed to Mardown (above) is that you the ability to change size and align"
  },
  {
    "objectID": "lectures/04-remote.html#reciprocal-blast",
    "href": "lectures/04-remote.html#reciprocal-blast",
    "title": "Remote Computing",
    "section": "Reciprocal BLAST",
    "text": "Reciprocal BLAST\nA reciprocal BLAST (Basic Local Alignment Search Tool) analysis is a technique used to identify homologous genes or proteins between two species by performing a BLAST search in both directions. In other words, it involves searching for similarities between a query sequence from species A against a database of sequences from species B, and then searching for similarities between a query sequence from species B against a database of sequences from species A. This approach helps confirm orthology, which is useful in comparative genomics and evolutionary studies.\nThree insightful visualizations of reciprocal BLAST analysis results could be:\n\nCircos Plot: A Circos plot is a circular layout that can display relationships between genomic sequences. In the context of reciprocal BLAST analysis, a Circos plot can effectively illustrate orthologous relationships between two species by connecting homologous genes or proteins with lines or curves. The strength of the connection (e.g., based on similarity scores or e-values) can be represented by the width or color of the lines, providing a comprehensive overview of the conservation between the two genomes.\nHeatmap: A heatmap is a graphical representation of data where values are represented by colors. In a reciprocal BLAST analysis, a heatmap can be used to visualize the similarity scores or e-values between pairs of orthologous genes or proteins. The rows and columns of the heatmap would represent genes or proteins from species A and species B, respectively, with the color intensity representing the strength of the orthologous relationship. This allows for easy identification of strongly conserved regions or potential functional similarities between the two species.\nScatter Plot with Dot Matrix: A scatter plot with dot matrix can visualize the distribution of reciprocal best hits (RBH) based on similarity scores or e-values. The x-axis would represent the scores or e-values for species A against species B, and the y-axis would represent the scores or e-values for species B against species A. Each dot on the plot would represent a pair of orthologs, with its position determined by the score or e-value in both directions. This visualization allows users to identify potential outliers, observe the overall correlation between the two species, and assess the quality of the reciprocal BLAST analysis results."
  },
  {
    "objectID": "lectures/04-remote.html#saving-my-pat",
    "href": "lectures/04-remote.html#saving-my-pat",
    "title": "Remote Computing",
    "section": "Saving my PAT",
    "text": "Saving my PAT\n\nsee https://happygitwithr.com/https-pat.html?q=password#store-pat\n\ninstall.packages(\"gitcreds\")\ngitcreds::gitcreds_set()"
  },
  {
    "objectID": "lectures/04-remote.html#chat-gpt-in-rstudio",
    "href": "lectures/04-remote.html#chat-gpt-in-rstudio",
    "title": "Remote Computing",
    "section": "Chat GPT in Rstudio",
    "text": "Chat GPT in Rstudio\nVideo of recent lab meeting"
  },
  {
    "objectID": "lectures/09-project.html",
    "href": "lectures/09-project.html",
    "title": "Archiving",
    "section": "",
    "text": "Objectives\nLearn how to backup data and code so others can understand and your future self can understand.\n\n\nDocumenting and backing up code\nDocumenting and backing up code and data are crucial practices in genomics, data science, and other fields that involve programming or handling large amounts of data. Here’s why:\n\nTraceability and reproducibility: Clear documentation ensures that every piece of code, data, or computational process is traceable and reproducible. This is especially important when you want to replicate an analysis or experiment, debug an issue, or understand the progression and changes in the code or data over time.\nKnowledge sharing and collaboration: Documentation allows others to understand your work, making it easier for collaboration. It also enables knowledge transfer when team members change over time. Code without documentation is often useless to others and sometimes even to the person who originally wrote it after some time has passed.\nProfessionalism and quality assurance: Good documentation is a sign of professionalism and maturity in software development. It shows that the code or data has been well thought out and tested, and it allows for quality assurance processes such as code reviews and audits.\nDisaster recovery: Backups are essential for disaster recovery. If a server fails, or data gets corrupted, having a backup means you can restore the code or data and continue with minimal disruption. Backups also protect against accidental deletions and malicious actions.\n\nHere’s how to document and backup effectively:\n\nComment your code: Write clear comments in your code explaining what specific sections or lines of code are doing. Wherever possible, use self-explanatory variable and function names.\nWrite documentation: Use readme files, wikis, or similar resources to provide high-level documentation. This can include how to use and install the software, the structure of the code or data, known issues, and more. Tools like Doxygen or Javadoc can be helpful.\nUse version control systems: Tools like Git can track changes in code and help document the history and reasoning behind those changes. Git, in combination with platforms like GitHub, also allows you to back up your code online and collaborate with others.\nAutomate backups: Use automated tools to regularly back up your code and data. This could be a simple cron job that copies files to a different location, or more sophisticated solutions that can handle large datasets, such as AWS S3, Google Cloud Storage, or other data backup services.\nImplement a testing framework: This will serve as an additional form of documentation, explaining how the code is supposed to behave, and it provides a way to verify that changes to the code have not broken anything.\nUse data versioning tools: In case of data science projects, tools like DVC can be used to track changes in data and models over time, similar to how Git is used for code.\n\nDocumenting and backing up code and data are crucial practices in software development, data science, and other fields that involve programming or handling large amounts of data. Here’s why:\nRemember, good documentation and regular backups are not afterthoughts or luxuries—they’re essential components of professional, reliable, and resilient projects.\n\n\nArchiving a repo with Zenodo\nArchiving your GitHub repository with Zenodo allows you to have a digital object identifier (DOI) for your repo, which is useful for citing your software in academic papers. Here’s a step-by-step guide:\n\nCreate a Zenodo account. You’ll need to visit Zenodo’s homepage (https://zenodo.org/) and sign up for an account if you don’t already have one.\nLink GitHub with Zenodo. After you’ve set up your Zenodo account, you can navigate to the “GitHub” section in the dashboard and link your GitHub account to Zenodo.\nAuthorize Zenodo on GitHub. Zenodo will request access to your GitHub repositories. You can either allow Zenodo to access all your repositories or only select ones. After you’ve made your choice, click “Authorize Zenodo.”\nSelect the repository. Back in Zenodo, you should see a list of all your GitHub repositories. You can choose to archive any repository by toggling the switch to “on” next to the repository name. Zenodo will now create a “webhook” for this repository, which will notify Zenodo whenever there is a new release of the repository on GitHub.\nCreate a new release on GitHub. If you haven’t done so already, you should create a new release of your repository on GitHub. Go to your repository page, click “releases” then “create a new release”. Tag version with the version number (for example “v1.0”) and add some description about this release. Zenodo will be notified once the release is published.\nCheck Zenodo for the deposit. After creating a new release, go back to Zenodo. You should see the new release in the “Upload” section. Zenodo automatically fills in some of the metadata for you, such as the title and authors, but you can edit this if needed.\nPublish the archive on Zenodo. Finally, you can publish the archive on Zenodo. Make sure all the information is correct, and then click the “Publish” button. Your repository is now archived, and you will have a DOI that you can use to cite your software.\n\nRemember that Zenodo will archive a new version of your repository every time you create a new release on GitHub, so it’s important to create meaningful and well-documented releases. Zenodo also allows you to link different versions of your software together, so that it’s clear how the software has changed over time."
  },
  {
    "objectID": "lectures/02-rna-seq.html#quality-control",
    "href": "lectures/02-rna-seq.html#quality-control",
    "title": "RNAs-seq",
    "section": "Quality Control",
    "text": "Quality Control\nThe first step in analyzing RNA-seq data is to perform quality control checks on the raw fastq files. This step is crucial to ensure that the data is of high quality and can be accurately quantified. One popular tool for quality control is FastQC, which generates various quality metrics such as per-base sequence quality, adapter contamination, and GC content.\nTo perform quality control using FastQC, run the following command:\nfastqc input.fastq\nThis will generate a HTML report that can be viewed in a web browser.\n\nAnother popular quality control program is fastp. Here is a very nice tutorial on using fastp"
  },
  {
    "objectID": "lectures/02-rna-seq.html#marineomics-rna-seq-panel-discussion",
    "href": "lectures/02-rna-seq.html#marineomics-rna-seq-panel-discussion",
    "title": "RNAs-seq",
    "section": "MarineOmics RNA-seq Panel Discussion",
    "text": "MarineOmics RNA-seq Panel Discussion"
  },
  {
    "objectID": "modules/DRAFT_DNA_methylation.html",
    "href": "modules/DRAFT_DNA_methylation.html",
    "title": "DNA Methylation Assessment",
    "section": "",
    "text": "Thanks to advances in sample preparation and sequencing methods, there is an influx of research examining epigenomics in non-model systems. Broadly, epigenetics can be defined as changes to gene expression that do not arise from changes in the DNA sequence. DNA methylation, or addition of a methyl (CH3) group to a cytosine base adjacent to a guanine (CpG) is one of the more commonly studies epigenetic mechanisms, partly because of its environmental sensitivity and potential role in phenotypic plasticity. For a good review of epigenetic mechanisms including DNA methylation in marine organisms see Eirin-Lopez and Putnam (2019).\nThe purpose of this tutorial is to provide an example of aligning bisulfite-treated and enzymatically converted DNA sequence data to an existing genome, and discuss needs for potential downstream applications. The principle behind creating bisulfite-treated DNA libraries is that when cytosines are not methylated, then they will be converted to uracil, which ultimately result in thymine nucleotides in the sequence data. Wanamaker et al. (2022) offers a comparison of three methods for quantifying DNA methylation at single base-pair resolution using whole genome bisulfite sequencing (WGBS), reduced representation bisulfite sequencing (RRBS), and methyl-CpG binding domain bisulfite sequencing (MBDBS). When aligning bisulfite-converted data to a reference genome the percent methylation at a given CpG loci is determined by examining the ratio of thymines to cytosines. For example if there is a given cytosine adjacent to a guanine (ie CpG locus) with 30% of the reads containing thymine, one would consider this cytosine loci (CpG) to be 70% methylated. Since bisulfite treatment can damage DNA, newer library preparation and sequencing methods (ex. NEBNext® Enzymatic Methyl-seq Kit; EM-seq are starting to gain popularity. As EM-Seq produces loci-level data, alignment of EM-Seq data is similar to WGBS. Also, technologies like Nanopore and PacBio sequencing can directly detect DNA methylation without bisulfite conversion. See Dimond, Nguyen, and Roberts (2021) as example of using Nanopore sequencing. As DNA passes through a nanopore or is sequenced by single-molecule real-time (SMRT) technology, changes in electrical current or fluorescence patterns can indicate the presence of methylated bases.\n\n\n\n\n\nSample size is a critical factor when conducting DNA methylation analysis, particularly because of the significant variation in methylation patterns both between individuals and across different tissues. This variation, which is not yet fully understood, can significantly impact the outcomes and interpretations of methylation studies.\nWhen selecting samples for DNA methylation analysis, the key principle is that larger sample sizes generally provide more reliable and generalizable results. A larger number of samples allows for a more accurate representation of the population, accounting for individual and tissue-level differences. This is essential for identifying true biological patterns rather than artifacts of small, non-representative samples.\nHowever, the practicality of obtaining a large number of samples must be balanced with resource constraints. In this context, a sample size of around eight can be considered a decent starting point. This number is often sufficient to begin observing trends and patterns in methylation across individuals and tissues, providing a foundational understanding which can be built upon with further research.\nIt’s crucial to remember that while more samples can enhance the reliability of the results, the quality of the samples and the robustness of the methylation analysis techniques are equally important. Therefore, researchers must strive for a balance between quantity and quality in their sample selection to ensure meaningful and accurate insights into DNA methylation.\n\n\n\n\n\n\nBisulfite Conversion Efficiency is important to assess. One common step during library preparation us spiking library with Lambda phage DNA. Lambda phage DNA lacks cytosine methylation, making it an ideal control for bisulfite treatment. Here’s how it works and why it’s useful:\n\nBackground on Lambda Phage DNA: Lambda phage DNA is a widely used control in bisulfite sequencing because it is naturally unmethylated. By including this DNA in your sample, you create a known unmethylated reference.\nPreparation and Spiking: You prepare the lambda phage DNA and spike it into your DNA samples before bisulfite treatment. The proportion of lambda DNA added should be enough to be detected post-sequencing, but not so much that it overwhelms the sample. A typical proportion might be around 0.1% to 1% of the total DNA.\nBisulfite Treatment and Sequencing: After spiking, you proceed with bisulfite treatment and sequencing of the entire sample, which now includes your genomic DNA of interest and the spiked lambda DNA.\nAssessing Conversion Efficiency: Once sequencing is complete, you analyze the lambda phage DNA sequences. Since this DNA is unmethylated, all cytosines should be converted to thymines after bisulfite treatment and subsequent PCR. By calculating the percentage of cytosines that were successfully converted to thymines in the lambda DNA, you can determine the bisulfite conversion efficiency. An efficient conversion typically results in over 99% of cytosines being converted.\n\n\n\n\n\nFor a good review of epigenetic mechanisms including DNA methylation in marine organisms see Eirin-Lopez and Putnam (2019). Wanamaker et al. (2022) offers a comparison of three methods for quantifying DNA methylation at single base-pair resolution using whole genome bisulfite sequencing (WGBS), reduced representation bisulfite sequencing (RRBS), and methyl-CpG binding domain bisulfite sequencing (MBDBS). The tutorial below is based on WGBS, though the general workflow would be consistent.\nThe tutorial will cover the following:\n- Sequence quality assessment\n- Read alignment\n- Methylation quantification\n- File conversions\n- Potential downstream applications. Although this tutorial will not provide code for downstream applications such as differential methylation analysis, it will link to other open-access resources and examples.\nThe tutorial below is based on WGBS from a species of marine intertebrate (Montipora capitata coral) examined in Wanamaker et al. (2022), and all raw data can be accessed under NCBI Bioproject PRJNA691891 if you would like to follow the tutorial with this dataset. The general workflow would be consistent for other library preparation and sequencing methods that produce loci-level data. The tutorial indicates places where special analytical consideration should be given to other data types. Given the size of the data files and the computational resources necessary this is not developed to be actionable and reproducible. For context this process outlined below took weeks on a university HPC system."
  },
  {
    "objectID": "modules/DRAFT_DNA_methylation.html#background",
    "href": "modules/DRAFT_DNA_methylation.html#background",
    "title": "DNA Methylation Assessment",
    "section": "",
    "text": "Thanks to advances in sample preparation and sequencing methods, there is an influx of research examining epigenomics in non-model systems. Broadly, epigenetics can be defined as changes to gene expression that do not arise from changes in the DNA sequence. DNA methylation, or addition of a methyl (CH3) group to a cytosine base adjacent to a guanine (CpG) is one of the more commonly studies epigenetic mechanisms, partly because of its environmental sensitivity and potential role in phenotypic plasticity. For a good review of epigenetic mechanisms including DNA methylation in marine organisms see Eirin-Lopez and Putnam (2019).\nThe purpose of this tutorial is to provide an example of aligning bisulfite-treated and enzymatically converted DNA sequence data to an existing genome, and discuss needs for potential downstream applications. The principle behind creating bisulfite-treated DNA libraries is that when cytosines are not methylated, then they will be converted to uracil, which ultimately result in thymine nucleotides in the sequence data. Wanamaker et al. (2022) offers a comparison of three methods for quantifying DNA methylation at single base-pair resolution using whole genome bisulfite sequencing (WGBS), reduced representation bisulfite sequencing (RRBS), and methyl-CpG binding domain bisulfite sequencing (MBDBS). When aligning bisulfite-converted data to a reference genome the percent methylation at a given CpG loci is determined by examining the ratio of thymines to cytosines. For example if there is a given cytosine adjacent to a guanine (ie CpG locus) with 30% of the reads containing thymine, one would consider this cytosine loci (CpG) to be 70% methylated. Since bisulfite treatment can damage DNA, newer library preparation and sequencing methods (ex. NEBNext® Enzymatic Methyl-seq Kit; EM-seq are starting to gain popularity. As EM-Seq produces loci-level data, alignment of EM-Seq data is similar to WGBS. Also, technologies like Nanopore and PacBio sequencing can directly detect DNA methylation without bisulfite conversion. See Dimond, Nguyen, and Roberts (2021) as example of using Nanopore sequencing. As DNA passes through a nanopore or is sequenced by single-molecule real-time (SMRT) technology, changes in electrical current or fluorescence patterns can indicate the presence of methylated bases."
  },
  {
    "objectID": "modules/DRAFT_DNA_methylation.html#experimental-considerations",
    "href": "modules/DRAFT_DNA_methylation.html#experimental-considerations",
    "title": "DNA Methylation Assessment",
    "section": "",
    "text": "Sample size is a critical factor when conducting DNA methylation analysis, particularly because of the significant variation in methylation patterns both between individuals and across different tissues. This variation, which is not yet fully understood, can significantly impact the outcomes and interpretations of methylation studies.\nWhen selecting samples for DNA methylation analysis, the key principle is that larger sample sizes generally provide more reliable and generalizable results. A larger number of samples allows for a more accurate representation of the population, accounting for individual and tissue-level differences. This is essential for identifying true biological patterns rather than artifacts of small, non-representative samples.\nHowever, the practicality of obtaining a large number of samples must be balanced with resource constraints. In this context, a sample size of around eight can be considered a decent starting point. This number is often sufficient to begin observing trends and patterns in methylation across individuals and tissues, providing a foundational understanding which can be built upon with further research.\nIt’s crucial to remember that while more samples can enhance the reliability of the results, the quality of the samples and the robustness of the methylation analysis techniques are equally important. Therefore, researchers must strive for a balance between quantity and quality in their sample selection to ensure meaningful and accurate insights into DNA methylation.\n\n\n\n\n\n\nBisulfite Conversion Efficiency is important to assess. One common step during library preparation us spiking library with Lambda phage DNA. Lambda phage DNA lacks cytosine methylation, making it an ideal control for bisulfite treatment. Here’s how it works and why it’s useful:\n\nBackground on Lambda Phage DNA: Lambda phage DNA is a widely used control in bisulfite sequencing because it is naturally unmethylated. By including this DNA in your sample, you create a known unmethylated reference.\nPreparation and Spiking: You prepare the lambda phage DNA and spike it into your DNA samples before bisulfite treatment. The proportion of lambda DNA added should be enough to be detected post-sequencing, but not so much that it overwhelms the sample. A typical proportion might be around 0.1% to 1% of the total DNA.\nBisulfite Treatment and Sequencing: After spiking, you proceed with bisulfite treatment and sequencing of the entire sample, which now includes your genomic DNA of interest and the spiked lambda DNA.\nAssessing Conversion Efficiency: Once sequencing is complete, you analyze the lambda phage DNA sequences. Since this DNA is unmethylated, all cytosines should be converted to thymines after bisulfite treatment and subsequent PCR. By calculating the percentage of cytosines that were successfully converted to thymines in the lambda DNA, you can determine the bisulfite conversion efficiency. An efficient conversion typically results in over 99% of cytosines being converted."
  },
  {
    "objectID": "modules/DRAFT_DNA_methylation.html#tutorial-details",
    "href": "modules/DRAFT_DNA_methylation.html#tutorial-details",
    "title": "DNA Methylation Assessment",
    "section": "",
    "text": "For a good review of epigenetic mechanisms including DNA methylation in marine organisms see Eirin-Lopez and Putnam (2019). Wanamaker et al. (2022) offers a comparison of three methods for quantifying DNA methylation at single base-pair resolution using whole genome bisulfite sequencing (WGBS), reduced representation bisulfite sequencing (RRBS), and methyl-CpG binding domain bisulfite sequencing (MBDBS). The tutorial below is based on WGBS, though the general workflow would be consistent.\nThe tutorial will cover the following:\n- Sequence quality assessment\n- Read alignment\n- Methylation quantification\n- File conversions\n- Potential downstream applications. Although this tutorial will not provide code for downstream applications such as differential methylation analysis, it will link to other open-access resources and examples.\nThe tutorial below is based on WGBS from a species of marine intertebrate (Montipora capitata coral) examined in Wanamaker et al. (2022), and all raw data can be accessed under NCBI Bioproject PRJNA691891 if you would like to follow the tutorial with this dataset. The general workflow would be consistent for other library preparation and sequencing methods that produce loci-level data. The tutorial indicates places where special analytical consideration should be given to other data types. Given the size of the data files and the computational resources necessary this is not developed to be actionable and reproducible. For context this process outlined below took weeks on a university HPC system."
  },
  {
    "objectID": "modules/DRAFT_DNA_methylation.html#software-considerations",
    "href": "modules/DRAFT_DNA_methylation.html#software-considerations",
    "title": "DNA Methylation Assessment",
    "section": "Software Considerations",
    "text": "Software Considerations\nThe software used as part of this tutorial is Bismark Bisulfite Mapper Krueger and Andrews (2011), which is one of the more widely-used softwares to align methylation data to a reference genome. Another commonly-used options include bwa-meth Pedersen et al. (2014) and BSMAP Xi and Li (2009) (see Nunn et al. (2021) and (Gong et al. 2022) for comparisons of these software for plant and mammal species, respectively). As with any software it is best to be famililar with the User Manual.\nWithin Bismark, the tutorial uses Bowtie 2 Langmead and Salzberg (2012) as the alignment software. Although the Bismark user manual covers the necessary Bowtie 2 parameters, it may be useful to refer to the Bowtie 2 manual.\nThe content below will provided with the assumption that the reader has read the manual and is meant to serve as guidance based on experience working with marine invertebrates."
  },
  {
    "objectID": "modules/DRAFT_DNA_methylation.html#genome-preparation",
    "href": "modules/DRAFT_DNA_methylation.html#genome-preparation",
    "title": "DNA Methylation Assessment",
    "section": "Genome Preparation",
    "text": "Genome Preparation\nThe first step in the process in preparing the reference genome by performing an in silico bisulfite conversion to allow Bowtie alignments. Example code:\n# Directories and programs\nbismark_dir=\"/programs/Bismark-0.21.0\"\nbowtie2_dir=\"/programs/bowtie2-2.3.4.1-linux-x86_64/\"\ngenome_folder=\"/where/the/fastafile/lives/\"\n\n${bismark_dir}/bismark_genome_preparation \\\n--verbose \\\n--parallel 28 \\\n--path_to_aligner ${bowtie2_dir} \\\n${genome_folder}\n\nBismark Genome Preparation Command:\n\n${bismark_dir}/bismark_genome_preparation: This is the command to run the genome preparation part of Bismark. The ${bismark_dir} variable is expanded to the path where Bismark is installed, so the script knows where to find the bismark_genome_preparation program.\n--verbose: This flag makes the program output more detailed information about what it is doing, which is helpful for debugging or understanding the process.\n--parallel 28: This option tells Bismark to use 28 threads in parallel to speed up the process. You can adjust this based on your computing power.\n--path_to_aligner ${bowtie2_dir}: This specifies the path to the aligner (Bowtie2) that Bismark will use. The ${bowtie2_dir} variable is expanded to the path where Bowtie2 is installed.\n${genome_folder}: Finally, this specifies the location of the genome files. The script uses the ${genome_folder} variable, which holds the path to these files.\n\n\nBismark will create two individual folders within this directory, one for a C-&gt;T converted genome and the other one for the G-&gt;A converted genome. After creating C-&gt;T and G-&gt;A versions of the genome they will be indexed in parallel usingthe indexer. Bismark will create the following subdirectories as outputs.\n./Bisulfite_Genome\n./Bisulfite_Genome/GA_conversion\n./Bisulfite_Genome/CT_conversion\nThis preparation step only needs to happen once for a given genome."
  },
  {
    "objectID": "modules/DRAFT_DNA_methylation.html#aligning-reads",
    "href": "modules/DRAFT_DNA_methylation.html#aligning-reads",
    "title": "DNA Methylation Assessment",
    "section": "Aligning reads",
    "text": "Aligning reads\nOnce the genome is prepared, reads can be aligned. For aligning the trimmed reads to the genome the following code structure is used:\nfind ${reads_dir}*_R1_001_val_1.fq.gz \\\n| xargs basename -s _R1_001_val_1.fq.gz | xargs -I{} ${bismark_dir}/bismark \\\n--path_to_bowtie ${bowtie2_dir} \\\n-genome ${genome_folder} \\\n-p 4 \\\n-score_min L,0,-0.6 \\\n--non_directional \\\n-1 ${reads_dir}{}_R1_001_val_1.fq.gz \\\n-2 ${reads_dir}{}_R2_001_val_2.fq.gz \\\n-o Mcap_tg\nThis will create BAM files (sequence alignment files) and txt files (mapping reports). The reports contain information such as mapping efficiency.\nSpecifically this is what the code chunk does:\n\nFinding Files and Preparing Filenames:\n\nfind ${reads_dir}*_R1_001_val_1.fq.gz: This command uses the find utility to search for files in the directory specified by the variable reads_dir. The files it looks for have a naming pattern ending with _R1_001_val_1.fq.gz, which typically denotes the first file of a pair in paired-end sequencing data.\n| xargs basename -s _R1_001_val_1.fq.gz: The xargs command takes the list of files found and passes them to basename, which strips the suffix _R1_001_val_1.fq.gz from each filename. This step extracts the base name of each file, which is used to pair the R1 and R2 files correctly in the following steps.\n\nRunning Bismark for Sequence Alignment:\n\n| xargs -I{} ${bismark_dir}/bismark: The xargs command here is used to pass the base filenames to Bismark for alignment. {} is a placeholder for each base filename.\n--path_to_bowtie ${bowtie2_dir}: This option specifies the path to the Bowtie 2 aligner, which Bismark uses for the alignment process.\n-genome ${genome_folder}: Specifies the directory where the genome files, prepared in advance, are located.\n-p 4: Sets the number of processing threads to 4, allowing Bismark to perform the alignment in parallel, which can speed up the process. This parameter can be changed based on your computing power.\n-score_min L,0,-0.6: This sets the scoring threshold for alignments, impacting the sensitivity and specificity of the alignment. The default scoring threshold used by Bismark is L,0,-0.2.\n--non_directional: This flag indicates the sequencing data is non-directional, which is important for correctly interpreting the methylation status in bisulfite sequencing.\n-1 ${reads_dir}{}_R1_001_val_1.fq.gz -2 ${reads_dir}{}_R2_001_val_2.fq.gz: Specifies the files for paired-end reads. R1 and R2 files are paired based on their base filename.\n-o Mcap_tg: Sets the output directory for the alignment results to Mcap_tg.\n\n\n\nOptimizing alignment sensitivity\nThe code above uses score_min L,0,-0.6 as the scoring threshold for alignment sensitivity and specificity. This is different than the Bismark default, score_min L,0,-0.2. Reducing the sensitivity of the alignment can increase the percentage of reads aligned to the reference genome, but of course the quality of the alignment may decrease. A good practice is to align a subset of your data to the reference genome using different score_min parameters, then comparing mapping efficiency from the mapping reports. Generally, you want to choose a parameter that maximizes percent alignment and specificity. Example code\nfind ${reads_dir}*_R1_001_val_1.fq.gz \\\n| xargs basename -s _R1_001_val_1.fq.gz | xargs -I{} ${bismark_dir}/bismark \\\n--path_to_bowtie ${bowtie2_dir} \\\n-genome ${genome_folder} \\\n-p 4 \\\n- u 10000 \\\n-score_min L,0,-0.6 \\\n--non_directional \\\n-1 ${reads_dir}{}_R1_001_val_1.fq.gz \\\n-2 ${reads_dir}{}_R2_001_val_2.fq.gz \\\n-o Mcap_tg\nThis is the same code as above, except -u 10000 is used to run the alignment on a 10,000 bp subset of the data in each input file."
  },
  {
    "objectID": "modules/DRAFT_DNA_methylation.html#deduplication",
    "href": "modules/DRAFT_DNA_methylation.html#deduplication",
    "title": "DNA Methylation Assessment",
    "section": "Deduplication",
    "text": "Deduplication\nFiles are then deduplicated if they are whole genome bisulfite treated samples. This command will deduplicate the Bismark alignment BAM file and remove all reads that align to the the very same position and in the same orientation except for one. This step is recommended for whole-genome bisulfite samples, but should not be used for reduced representation libraries such as RRBS, amplicon, or target enrichment libraries. The bismark manual provides additional information.\nfind *.bam | \\\nxargs basename -s .bam | \\\nxargs -I{} ${bismark_dir}/deduplicate_bismark \\\n--bam \\\n--paired \\\n{}.bam"
  },
  {
    "objectID": "modules/DRAFT_DNA_methylation.html#what-to-expect",
    "href": "modules/DRAFT_DNA_methylation.html#what-to-expect",
    "title": "DNA Methylation Assessment",
    "section": "What to expect",
    "text": "What to expect\nDNA methylation at CpG locus can vary to 0-100%. How methylated CpGs are across a genome can vary WIDELY depending on your focal taxa. Generally, invertebrate genomes are predominantly unmethylated, with smaller regions of higher methylation in a mosaic pattern. In some marine invertebrates we see ~15% methylation of CpGs (defined as &gt; 75% methylated) across the genome with most CpGs unmethylated. In these taxa methylation usually occurs in gene bodies.\nThe figure below shows the methylation ratio of CpGs across the Pacific oyster (Crassostrea gigas) genome, as well as the location of methylation across the genome:\n\n\n\n\n\nFrequency distribution of methylation ratios for CpG dinucleotides in oyster gill tissue. Gavery and Roberts (2013)\n\n\n\n\n\n\n\n\n\nScreenshot from IGV. Bedgraph files are displayed. See below for code to create these file (File Conversion)\n\n\n\n\n\nVertebrate genomes, on the other hand, have higher overall levels of methylation, with methylation occuring in CpG islands. An example analysis of methylation at individual CG dinucleotides in an elephant shark, mouse, and zebra fish is shown, below. Results reavel comparable levels of global methyaltion patterns with the majority of sites being highly methylated (≥ 80%), and fewer than one tenth being unmethylated (doi: 10.12688/f1000research.11281.1).\n\n\n\n\n\nGenome screenshots of methylation quantified in 2kb running windows over the first 3Mb of chromosome 1 zebrafish and mouse, and of the largest scaffold (NW_006890054.1) in elephant shark (doi: 10.12688/f1000research.11281.1)"
  },
  {
    "objectID": "modules/DRAFT_DNA_methylation.html#methylation-extraction",
    "href": "modules/DRAFT_DNA_methylation.html#methylation-extraction",
    "title": "DNA Methylation Assessment",
    "section": "Methylation extraction",
    "text": "Methylation extraction\nMethylation levels are then extracted using the bismark_methylation_extractor. For example:\n${bismark_dir}/bismark_methylation_extractor \\\n--bedGraph \\\n--counts \\\n--comprehensive \\\n--merge_non_CpG \\\n--multicore 28 \\\n--buffer_size 75% \\\n*deduplicated.bam\nThis particular command is used to extract and process methylation data from BAM files (generated after sequencing data alignment). Let’s break down the options:\n\nOptions:\n\n--bedGraph: Generates a BedGraph file output, which is a format that allows visualization of the methylation levels across the genome in genome browsers.\n--counts: Outputs the count of methylated and unmethylated calls for each cytosine in the genome.\n--comprehensive: This option makes the methylation extractor process all cytosines in the genome, not just those in a CpG context. It combines all three contexts (CpG, CHG, and CHH) into one output file.\n--merge_non_CpG: Merges the methylation information for CHG and CHH contexts (non-CpG) into a single file.\n--multicore 28: Specifies that the program should use 28 cores to process the data in parallel, which can significantly speed up the analysis. This can be adjusted depending on your computing power.\n--buffer_size 75%: This option sets the buffer size for reading in BAM files to 75% of the available system memory. It helps optimize the performance of the tool by adjusting how much memory is used for processing. This can be adjusted depending on your computing power.\n*deduplicated.bam: This is the input file pattern for the command. It indicates that the methylation extractor should process all BAM files in the current directory that have names ending with “deduplicated.bam”. These files are typically the output of a deduplication process applied to aligned bisulfite sequencing data.\n\n\nThis will create a file with the suffix deduplicated.bismark.cov.gz (example shown below in uncompressed format):\nNC_035784.1 141 141 37.5    3   5\nNC_035784.1 142 142 100 2   0\nNC_035784.1 155 155 70  7   3\nNC_035784.1 156 156 100 2   0\nNC_035784.1 291 291 0   0   2\nNC_035784.1 292 292 0   0   3\nNC_035784.1 313 313 0   0   1\nNC_035784.1 314 314 66.6666666666667    2   1\nNC_035784.1 470 470 66.6666666666667    4   2\nNC_035784.1 611 611 0   0   4\ncolumn organization of the file\n&lt;chromosome&gt; &lt;start position&gt; &lt;end position&gt; &lt;methylation percentage&gt; &lt;count methylated&gt; &lt;count unmethylated&gt;\nThe resultant bedGraph file contains information from unmerged strands, meaning reads aligning to forward and reverse strands in the same position are not combined. It is important to merge the reads from the forward and reserve strands for more accurate methylation quantification. A genome-wide cytosine report and merged bedGraph file are generated (including merged bedGraph file) from the deduplicated.bismark.cov.gz generated above in this example code:\nfind *deduplicated.bismark.cov.gz \\\n| xargs basename -s _trimmed_bismark_bt2.deduplicated.bismark.cov.gz \\\n| xargs -I{} ${bismark_dir}/coverage2cytosine \\\n--genome_folder ${genome_folder} \\\n-o {} \\\n--merge_CpG \\\n--zero_based \\\n{}_trimmed_bismark_bt2.deduplicated.bismark.cov.gz\nThe script converts coverage files into a format that details cytosine methylation levels across the genome. Here’s a breakdown of each step:\n\nFinding Files:\n\nfind *deduplicated.bismark.cov.gz: This command is used to search for all files in the current directory that end with deduplicated.bismark.cov.gz. These files are coverage files produced by Bismark, typically after aligning bisulfite-treated DNA sequences. They contain information about methylation at different sites in the genome.\n\nExtracting Base File Names:\n\n| xargs basename -s _trimmed_bismark_bt2.deduplicated.bismark.cov.gz: This pipeline uses xargs to pass each found file to basename, which strips the suffix _trimmed_bismark_bt2.deduplicated.bismark.cov.gz from each file name. This step extracts the base names of the files, which will be used to generate output filenames in the following steps.\n\nConverting Coverage to Cytosine Methylation Levels:\n\n| xargs -I{} ${bismark_dir}/coverage2cytosine: This pipeline uses xargs again to pass each base filename to the coverage2cytosine tool in the Bismark suite. The placeholder {} is replaced by the base filename.\n--genome_folder ${genome_folder}: This option specifies the location of the genome folder. The ${genome_folder} variable holds the path to this folder, which contains reference genome data necessary for the analysis.\n-o {}: The -o option specifies the output file name, using the base filename {} obtained from the previous steps.\n--merge_CpG: This flag instructs the program to merge the methylation information of cytosines in CpG context, providing a single methylation value for each CpG site.\n--zero_based: This option indicates that the output file should use zero-based coordinates, which is a common format in bioinformatics where the first base of a sequence is numbered 0 instead of 1.\n{}_trimmed_bismark_bt2.deduplicated.bismark.cov.gz: Finally, this specifies the input file for each iteration, where {} is replaced by the base filename obtained earlier. These are the coverage files previously identified by the find command.\n\n\nOutput file format .CpG_report.merged_CpG_evidence.cov (merged bedGraph file)\nNC_035785.1 217 219 100.000000  17  0\nNC_035785.1 523 525 87.500000   7   1\nNC_035785.1 556 558 50.000000   5   5\nNC_035785.1 727 729 100.000000  16  0\nNC_035785.1 1330    1332    0.000000    0   2\nNC_035785.1 1403    1405    0.000000    0   2\nNC_035785.1 1494    1496    66.666667   2   1\nNC_035785.1 1747    1749    100.000000  8   0\nNC_035785.1 2024    2026    100.000000  24  0\nNC_035785.1 2054    2056    93.333333   14  1\ncolumn organization of the file\n&lt;chromosome&gt; &lt;start position&gt; &lt;end position&gt; &lt;methylation percentage&gt; &lt;count methylated&gt; &lt;count unmethylated&gt;"
  },
  {
    "objectID": "modules/DRAFT_DNA_methylation.html#removing-c-to-t-c-t-snps",
    "href": "modules/DRAFT_DNA_methylation.html#removing-c-to-t-c-t-snps",
    "title": "DNA Methylation Assessment",
    "section": "Removing C to T (C->T) SNPs",
    "text": "Removing C to T (C-&gt;T) SNPs\nRemoving cytosine (C) to thymine (T) single nucleotide polymorphisms (SNPs) in bisulfite sequencing analysis could be something to consider for a few reasons:\n\nBisulfite Conversion Process: In bisulfite sequencing, unmethylated cytosines are converted to uracil, which is then read as thymine in sequencing. This conversion is key to identifying methylated cytosines. However, if a genomic DNA sequence naturally has a C to T (C-&gt;T) SNP, it can be falsely interpreted as a converted cytosine, leading to incorrect conclusions about methylation.\nReduction of False Positives: By removing C-&gt;T SNPs, one can reduce false positives in methylation analysis. Without removal, these SNPs might be misinterpreted as unmethylated cytosines that have undergone conversion, skewing the data towards an apparent increase in methylation.\nAccurate Methylation Mapping: For precise mapping of methylation patterns, it’s crucial to distinguish between true methylation signals and genetic variation (SNPs). Removing C-&gt;T SNPs allows for a more accurate representation of the methylation status of the genome.\nAnalysis Consistency: In comparative studies, where multiple samples or conditions are analyzed, removing C-&gt;T SNPs ensures consistency across samples. This is especially important in cases where SNP profiles may vary between samples, which could otherwise lead to inconsistent methylation data.\n\nBy eliminating these potential sources of error, researchers can have more confidence in their interpretation of the bisulfite sequencing data, leading to more reliable and valid conclusions about DNA methylation patterns.\nOne way to do this is using BS-Snper (Gao et al. (2015)). https://github.com/hellbelly/BS-Snper. This program finds SNPs in Bisulfite treated alignment files. The bash code below represents steps to identify SNPS and create a list of all C-&gt;T SNPS.\n\nsamtools merge \\\nPg_merged.bam \\\n*.sorted.bam\n\nperl /gscratch/srlab/programs/BS-Snper-master/BS-Snper.pl \\\nPg_merged.bam \\\n--fa Panopea-generosa-v1.0.fa \\\n--output snp.candidate.out \\\n--methcg meth.cg \\\n--methchg meth.chg \\\n--methchh meth.chh \\\n--minhetfreq 0.1 \\\n--minhomfreq 0.85 \\\n--minquali 15 \\\n--mincover 10 \\\n--maxcover 1000 \\\n--minread2 2 \\\n--errorate 0.02 \\\n--mapvalue 20 \\\n&gt;SNP.vcf 2&gt;SNP.log\n\n\ngrep $'C\\tG' output/SNP.vcf &gt; output/CT-SNP.vcf\nwc -l output/CT-SNP.vcf\n\nLet’s break down the command and its arguments:\n\nperl /gscratch/srlab/programs/BS-Snper-master/BS-Snper.pl: This part of the command calls the Perl interpreter to execute the BS-Snper.pl script. The script is located in the /gscratch/srlab/programs/BS-Snper-master/ directory.\nPg_merged.bam: This is the input file for the script, likely a BAM file (Binary Alignment/Map format) which contains alignment information of bisulfite-treated sequencing reads.\n--fa Panopea-generosa-v1.0.fa: This option specifies the reference genome file in FASTA format against which the BAM file will be compared.\n--output snp.candidate.out: This argument sets the name of the output file where the SNP candidates will be written.\n--methcg meth.cg, --methchg meth.chg, --methchh meth.chh: These options specify the output files for methylation analysis for CG, CHG, and CHH contexts, respectively (where H can be A, C, or T).\n--minhetfreq 0.1: Sets the minimum frequency for heterozygous bases to be considered in SNP calling (here, 10%).\n--minhomfreq 0.85: Sets the minimum frequency for homozygous bases to be considered in SNP calling (here, 85%).\n--minquali 15: Sets the minimum base quality score.\n--mincover 10, --maxcover 1000: These set the minimum and maximum coverage (number of reads) for a position to be considered in the analysis.\n--minread2 2: The minimum number of reads required to support a SNP candidate. It could be useful to use the same values for the minimum read threshold for SNP identification and for minimum coverage in the methylation data.\n--errorate 0.02: Sets the error rate of sequencing (here, 2%).\n--mapvalue 20: Specifies the mapping quality value, a threshold for considering reads in the analysis.\n&gt;SNP.vcf: Redirects the standard output to a file named SNP.vcf. This file will contain the SNP calls in VCF (Variant Call Format) format.\n2&gt;SNP.log: Redirects the standard error output to a file named SNP.log. This file will contain error messages and possibly additional log information.\n\nThe resulting could be used to mask the genome and the entire process described above could be repeated (ie Genome Preparation, Alignment, Quantification). An alternative and common practice is to remove cytosines from downstream analysis by removing loci in intermediate files (ie tab-delimited data files). Below is an example using antijoin to remove loci that correspond to CT SNPs from 5x coverage methyation quantification files.\n\n# Read in CT SNP file\nct &lt;- read.csv(\"../output/CT-SNP.vcf\", header = FALSE, sep = \"\\t\") %&gt;%\n  mutate(loci = paste0(V1, \"_\", V2))\n\n\n# 1. List all files with _5x.tab suffix\nfiles &lt;- list.files(path = \"../data/\", pattern = \"_5x.tab$\", full.names = TRUE)\n\n# 2. Iterate over each file\nfor(file in files) {\n  \n  # Extract base filename without the directory for naming purposes\n  base_name &lt;- basename(file)\n  \n  # Read the file\n  data &lt;- read.csv(file, header = FALSE, sep = \"\\t\")\n  \n  # Modify the data\n  modified_data &lt;- data %&gt;%\n    mutate(loci = paste0(V1, \"_\", V2)) %&gt;%\n    anti_join(ct, by = \"loci\") %&gt;%\n    select(-loci)\n    \n  \n  # Write the modified data to an output file\n  output_file &lt;- paste0(\"../output/f\", base_name)\n  write.table(modified_data, file = output_file, sep = \"\\t\", row.names = FALSE, quote = FALSE, col.names = FALSE)\n}\n\nLet’s break down what each part of the script does:\n\nReading in a CT SNP file:\n\nct &lt;- read.csv(\"../output/CT-SNP.vcf\", header = FALSE, sep = \"\\t\"): This line reads a file named CT-SNP.vcf from the specified directory. The file is expected to be in a tab-separated format (sep = \"\\t\"), and it doesn’t have a header row (header = FALSE).\nmutate(loci = paste0(V1, \"_\", V2)): The mutate function from the dplyr package is used to add a new column (loci) to the ct data frame. This column is created by concatenating the values in the first (V1) and second columns (V2) of each row, separated by an underscore.\n\nListing files with a specific suffix:\n\nfiles &lt;- list.files(path = \"../data/\", pattern = \"_5x.tab$\", full.names = TRUE): This line generates a list of file paths in the ../data/ directory. It only includes files that end with _5x.tab (as indicated by the pattern argument).\n\nProcessing each file:\n\nThe for(file in files) loop iterates over each file found in the previous step.\n\nbase_name &lt;- basename(file): Extracts the file name without the directory path.\ndata &lt;- read.csv(file, header = FALSE, sep = \"\\t\"): Reads the current file in the loop. Similar to the first read.csv call, it expects a tab-separated file with no header row.\nThe mutate and anti_join functions are used to modify data:\n\nmutate(loci = paste0(V1, \"_\", V2)): Adds a loci column similar to the ct data frame.\nanti_join(ct, by = \"loci\"): Removes rows from data that have matching loci values in the ct data frame.\nselect(-loci): Removes the loci column from the data.\n\nThe modified data is then written to an output file:\n\noutput_file &lt;- paste0(\"../output/f\", base_name): Constructs the output file path.\nwrite.table(modified_data, file = output_file, sep = \"\\t\", row.names = FALSE, quote = FALSE, col.names = FALSE): Writes the modified_data to the specified file. The output is tab-separated, and it doesn’t include row names, quotes, or column names."
  },
  {
    "objectID": "modules/DRAFT_DNA_methylation.html#downstream-application",
    "href": "modules/DRAFT_DNA_methylation.html#downstream-application",
    "title": "DNA Methylation Assessment",
    "section": "Downstream application",
    "text": "Downstream application\n\nIntegrating with population genetic data. example: Silliman et al. (2023)\nGene-level methylation analysis: example Putnam et al. (2022). It is worth considering the utility of converting loci-level resolution data to gene-level resolution data.\nDifferential loci methylation and/or region analyses. example: Venkataraman, White, and Roberts (2022). methylKit is a popular package to conduct this analysis differential methylation anlaysis\nFeature overlap. example: Venkataraman et al. (2020) Bedtools is excellent for quantitative assessment of feature overlap and IGV is good for visual assessment."
  },
  {
    "objectID": "modules/22-supp.html",
    "href": "modules/22-supp.html",
    "title": "Ecological Epigenetics",
    "section": "",
    "text": "The implications of epigenetics in species living in coastal regions, particularly in the context of adaptation and resilience to climate change, are profound and multifaceted. Coastal ecosystems are among the most dynamic and vulnerable environments, often facing challenges such as rising sea levels, increased storm frequency, and changing salinity. Epigenetics plays a crucial role in how these species adapt and survive under such rapidly changing conditions.\n\nRapid Adaptation to Environmental Changes: Unlike genetic mutations, epigenetic modifications can occur relatively quickly in response to environmental changes. This rapid epigenetic response can allow species in coastal regions to adjust to shifts in temperature, salinity, or other stress factors associated with climate change, without waiting for long-term evolutionary changes through natural selection.\nTransgenerational Adaptation: Some epigenetic changes can be passed from one generation to the next. This means that if a population is exposed to a certain stressor (like increased salinity), the offspring may inherit some of the adaptations that help them survive in that environment, even if they are not exposed to the stressor themselves. This transgenerational epigenetic inheritance can be crucial for the survival of species in rapidly changing environments.\nDevelopmental Plasticity: Epigenetics contributes to developmental plasticity, where organisms can modify their development in response to environmental cues. This can result in changes in physical characteristics or behaviors that are better suited to the new environment, enhancing survival and reproduction.\nResilience to Extreme Events: Coastal regions are often subject to extreme weather events like hurricanes or floods. Epigenetic mechanisms can contribute to the resilience of species by enabling rapid stress responses and recovery, ensuring survival and continuity.\nCommunity Dynamics and Ecosystem Functioning: The epigenetic responses of individual species can influence broader community dynamics and ecosystem functioning. For example, if a key species develops an epigenetic adaptation that alters its role in the ecosystem, it can have cascading effects on other species and overall ecosystem health.\nPotential for Conservation and Management Strategies: Understanding the epigenetic mechanisms underlying adaptation in coastal species can inform conservation efforts. It can lead to strategies that support the natural adaptive capacity of these species, thereby enhancing ecosystem resilience to climate change.\n\nCoastal ecosystems are characterized by dynamic environmental variations and anthropogenic impacts, making the role of epigenetic mechanisms in adaptation and resilience paramount (Mounger et al., 2021). Epigenetic variation has been found to be high in coastal foundation species, indicating its importance in enabling species to cope with diverse and challenging coastal environments (Mounger et al., 2020). This is further supported by the fact that individuals with similar genetic profiles exhibit divergent epigenetic profiles, suggesting that epigenetic changes may be associated with environmental heterogeneity (Lira et al., 2010). Additionally, rapid phenotypic alterations mediated by epigenetic mechanisms are particularly important for the persistence of species in dynamic ecosystems, such as those in coastal regions (Mounger et al., 2021). Furthermore, the capacity of species to cope with disturbances and adapt while maintaining essential functions, as defined by coastal resilience, is influenced by epigenetic mechanisms (Raub et al., 2021). It has been suggested that genotypic diversity can replace the role of species diversity in species-poor coastal ecosystems, buffering against extreme climatic events (Reusch et al., 2005). This is significant in the context of climate change, as coastal areas are increasingly at risk from both natural and human-induced hazards (Almutairi et al., 2020). Moreover, the sensitivity of coastal species to ocean acidification highlights the need to understand the epigenetic effects of environmental stressors on coastal species (Pousse et al., 2022). In conclusion, epigenetics has profound implications for species living in coastal regions, particularly in the context of adaptation and resilience in the face of climate change. The high epigenetic variation observed in coastal species, coupled with the capacity of epigenetic mechanisms to enable rapid phenotypic alterations, underscores the importance of epigenetics in facilitating species’ ability to cope with diverse and challenging coastal environments.\nReferences:\nAlmutairi, A., Mourshed, M., & Ameen, R. (2020). Coastal community resilience frameworks for disaster risk management. Natural Hazards, 101(2), 595-630. https://doi.org/10.1007/s11069-020-03875-3\nLira, C., Parisod, C., Fernandes, R., Mata, C., Cardoso, M., & Ferreira, P. (2010). Epigenetic variation in mangrove plants occurring in contrasting natural environment. Plos One, 5(4), e10326. https://doi.org/10.1371/journal.pone.0010326\nMounger, J., Boquete, M., Schmid, M., Granado, R., Robertson, M., Voors, S., … & Richards, C. (2020). Inheritance of DNA methylation differences in the mangrove Rhizophora mangle. https://doi.org/10.1101/2020.10.24.353482\nMounger, J., Boquete, M., Schmid, M., Granado, R., Robertson, M., Voors, S., … & Richards, C. (2021). Inheritance of DNA methylation differences in the mangrove Rhizophora mangle. Evolution & Development, 23(4), 351-374. https://doi.org/10.1111/ede.12388\nPousse, E., Munroe, D., Hart, D., Hennen, D., Cameron, L., Rheuban, J., … & Meseck, S. (2022). Dynamic energy budget modeling of Atlantic surfclam, Spisula solidissima, under future ocean acidification and warming. Marine Environmental Research, 177, 105602. https://doi.org/10.1016/j.marenvres.2022.105602\nRaub, K., Stepenuck, K., Panikkar, B., & Stephens, J. (2021). An analysis of resilience planning at the nexus of food, energy, water, and transportation in coastal US cities. Sustainability, 13(11), 6316. https://doi.org/10.3390/su13116316\nReusch, T., Ehlers, A., Hämmerli, A., & Worm, B. (2005). Ecosystem recovery after climatic extremes enhanced by genotypic diversity. Proceedings of the National Academy of Sciences, 102(8), 2826-2831. https://doi.org/10.1073/pnas.0500008102"
  },
  {
    "objectID": "modules/22-supp.html#more-on-adaptation-and-evolution",
    "href": "modules/22-supp.html#more-on-adaptation-and-evolution",
    "title": "Ecological Epigenetics",
    "section": "More on Adaptation and Evolution",
    "text": "More on Adaptation and Evolution\nYévenes, M., Gallardo-Escárate, C., & Gajardo, G. (2024). Epigenetic variation mediated by lncrnas accounts for adaptive genomic differentiation of the endemic blue mussel mytilus chilensis. Heliyon, 10(1), e23695. https://doi.org/10.1016/j.heliyon.2023.e23695\nLamka, G. F., Harder, A. M., Sundaram, M., Schwartz, T. S., Christie, M. R., DeWoody, J. A., … & Willoughby, J. R. (2022). Epigenetics in ecology, evolution, and conservation. Frontiers in Ecology and Evolution, 10. https://doi.org/10.3389/fevo.2022.871791\nAbstract: Epigenetic variation is often characterized by modifications to DNA that do not alter the underlying nucleotide sequence, but can influence behavior, morphology, and physiological phenotypes by affecting gene expression and protein synthesis. In this review, we consider how the emerging field of ecological epigenetics (eco-epi) aims to use epigenetic variation to explain ecologically relevant phenotypic variation and predict evolutionary trajectories that are important in conservation. Here, we focus on how epigenetic data have contributed to our understanding of wild populations, including plants, animals, and fungi. First, we identified published eco-epi literature and found that there was limited taxonomic and ecosystem coverage and that, by necessity of available technology, these studies have most often focused on the summarized epigenome rather than locus- or nucleotide-level epigenome characteristics. We also found that while many studies focused on adaptation and heritability of the epigenome, the field has thematically expanded into topics such as disease ecology and epigenome-based ageing of individuals. In the second part of our synthesis, we discuss key insights that have emerged from the epigenetic field broadly and use these to preview the path toward integration of epigenetics into ecology. Specifically, we suggest moving focus to nucleotide-level differences in the epigenome rather than whole-epigenome data and that we incorporate several facets of epigenome characterization (e.g., methylation, chromatin structure). Finally, we also suggest that incorporation of behavior and stress data will be critical to the process of fully integrating eco-epi data into ecology, conservation, and evolutionary biology.\nVenney, C. J., Anastasiadi, D., Wellenreuther, M., & Bernatchez, L. (2023). The evolutionary complexities of dna methylation in animals: from plasticity to genetic evolution. Genome Biology and Evolution, 15(12). https://doi.org/10.1093/gbe/evad216\nThe importance of DNA methylation in plastic responses to environmental change and evolutionary dynamics is increasingly recognized. Here, we provide a Perspective piece on the diverse roles of DNA methylation on broad evolutionary timescales, including (i) short-term transient acclimation, (ii) stable phenotypic evolution, and (iii) genomic evolution. We show that epigenetic responses vary along a continuum, ranging from short-term acclimatory responses in variable environments within a generation to long-term modifications in populations and species. DNA methylation thus unlocks additional potential for organisms to rapidly acclimate to their environment over short timeframes. If these changes affect fitness, they can circumvent the need for adaptive changes at the genome level. However, methylation has a complex reciprocal relationship with genetic variation as it can be genetically controlled, yet it can also induce point mutations and contribute to genomic evolution. When habitats remain constant over many generations, or populations are separated across habitats, initially plastic phenotypes can become hardwired through epigenetically facilitated mutagenesis. It remains unclear under what circumstances plasticity contributes to evolutionary outcomes, and when plastic changes will become permanently encoded into genotype. We highlight how studies investigating the evolution of epigenetic plasticity need to carefully consider how plasticity in methylation state could evolve among different evolutionary scenarios, the possible phenotypic outcomes, its effects on genomic evolution, and the proximate energetic and ultimate fitness costs of methylation. We argue that accumulating evidence suggests that DNA methylation can contribute toward evolution on various timescales, spanning a continuum from acclimatory plasticity to genomic evolution."
  },
  {
    "objectID": "modules/day01.html",
    "href": "modules/day01.html",
    "title": "Agenda",
    "section": "",
    "text": "google docs notepad"
  },
  {
    "objectID": "modules/day01.html#presentations",
    "href": "modules/day01.html#presentations",
    "title": "Agenda",
    "section": "Presentations",
    "text": "Presentations\nOn Friday you will give a presentation to the class. For this presentation you will give a presentation on how a taxa or coastal system could be impacted be climate change and how you would\n1) assess the impact on the organism and/or system with using any molecular biology\n2) assess the impact on the organism and/or system with using genomic technigues that could include gene expression and epigenetics mechanisms.\n3) include what possible results might look like and how you would interpret them\n4) be sure to include any caveats or stipulations (eg “however another explanation might be xxxxxxx)\n5)"
  },
  {
    "objectID": "modules/day01.html#biology",
    "href": "modules/day01.html#biology",
    "title": "Agenda",
    "section": "Biology",
    "text": "Biology\nObjective: Have general understanding of sub-organismal biology, computational framework of class. Gene expression.\n\nCourse Description\nInstructor Background\nClass Background and Interests\nHomeostasis\nCell and Molecular Biology\nDiscussion of Reproducible & Open Science\nOverview of Framework of for Computational Component Need to get GitHub User names\nDemonstration: Bash\nCase Study: Gene Expression George, M. N., Cattau, O., Middleton, M. A., Lawson, D., Vadopalas, B., Gavery, M. R., … & Roberts, S. B. (2023). Triploid pacific oysters exhibit stress response dysregulation and elevated mortality following marine heatwaves.. https://doi.org/10.1101/2023.03.02.530828 PDF"
  },
  {
    "objectID": "modules/day01.html#epigenetics",
    "href": "modules/day01.html#epigenetics",
    "title": "Agenda",
    "section": "Epigenetics",
    "text": "Epigenetics"
  },
  {
    "objectID": "modules/00-markdown.html",
    "href": "modules/00-markdown.html",
    "title": "markdown",
    "section": "",
    "text": "Markdown is a lightweight markup language created by John Gruber and Aaron Swartz in 2004. It is designed to be easy-to-read and easy-to-write. The syntax allows you to format text using simple symbols and characters you already know. For example, you can use asterisks * or underscores _ to create italics or bold text. You can also create lists, headers, links, and many other formatting features easily.\n\n\n\n\nMarkdown is much simpler to learn compared to other markup languages like HTML. This makes it a quick tool for anyone who wants to write well-formatted content without the steep learning curve.\n\n\n\nMarkdown files are plain text files with a .md extension, which means they can be opened with any text editor. This makes them extremely portable and easy to manage.\n\n\n\nMarkdown is commonly used for readme files, GitHub repositories, blogging platforms like Jekyll, and even in data science notebooks like Jupyter.\n\n\n\nThe syntax is designed to be readable as-is, so even if you’re not rendering the Markdown to HTML or another format, it’s still easy to understand what’s going on.\n\n\n\n\nHere are some basic examples of Markdown syntax:\n\nHeaders:\n# H1\n## H2\n### H3\nLists:\n- Item 1\n- Item 2\n    - Sub-item\nor\n1. Item 1\n2. Item 2\nLinks:\n[OpenAI](https://www.openai.com/)\nImages:\n![Alt text](url)\nCode Blocks:\n```bash\n Code goes here\n```\nIn-line Code:\nUse `code` in your markdown file.\n\n\n\n\n\n\nThe beauty of Markdown lies in its simplicity. Stick to the basics unless there’s a strong reason to use a complex feature.\n\n\n\nIf you’re working on a longer document or a series of documents, be consistent in your style and formatting. This will make it easier for others (or future you) to read and modify the content.\n\n\n\nConsider using a Markdown style guide or linter to keep your formatting consistent. This is especially helpful for collaborative projects.\n\n\n\nMany platforms support Markdown extensions that provide additional features like tables, footnotes, and task lists. While these are useful, keep in mind that they may not be supported everywhere.\n\n\n\nAlways preview your Markdown file before publishing to make sure it looks the way you intended.\nBy adopting these best practices, you’ll be able to create content that is clean, easy to read, and easy to maintain."
  },
  {
    "objectID": "modules/00-markdown.html#why-is-markdown-useful",
    "href": "modules/00-markdown.html#why-is-markdown-useful",
    "title": "markdown",
    "section": "",
    "text": "Markdown is much simpler to learn compared to other markup languages like HTML. This makes it a quick tool for anyone who wants to write well-formatted content without the steep learning curve.\n\n\n\nMarkdown files are plain text files with a .md extension, which means they can be opened with any text editor. This makes them extremely portable and easy to manage.\n\n\n\nMarkdown is commonly used for readme files, GitHub repositories, blogging platforms like Jekyll, and even in data science notebooks like Jupyter.\n\n\n\nThe syntax is designed to be readable as-is, so even if you’re not rendering the Markdown to HTML or another format, it’s still easy to understand what’s going on."
  },
  {
    "objectID": "modules/00-markdown.html#markdown-syntax-and-examples",
    "href": "modules/00-markdown.html#markdown-syntax-and-examples",
    "title": "markdown",
    "section": "",
    "text": "Here are some basic examples of Markdown syntax:\n\nHeaders:\n# H1\n## H2\n### H3\nLists:\n- Item 1\n- Item 2\n    - Sub-item\nor\n1. Item 1\n2. Item 2\nLinks:\n[OpenAI](https://www.openai.com/)\nImages:\n![Alt text](url)\nCode Blocks:\n```bash\n Code goes here\n```\nIn-line Code:\nUse `code` in your markdown file."
  },
  {
    "objectID": "modules/00-markdown.html#best-practices",
    "href": "modules/00-markdown.html#best-practices",
    "title": "markdown",
    "section": "",
    "text": "The beauty of Markdown lies in its simplicity. Stick to the basics unless there’s a strong reason to use a complex feature.\n\n\n\nIf you’re working on a longer document or a series of documents, be consistent in your style and formatting. This will make it easier for others (or future you) to read and modify the content.\n\n\n\nConsider using a Markdown style guide or linter to keep your formatting consistent. This is especially helpful for collaborative projects.\n\n\n\nMany platforms support Markdown extensions that provide additional features like tables, footnotes, and task lists. While these are useful, keep in mind that they may not be supported everywhere.\n\n\n\nAlways preview your Markdown file before publishing to make sure it looks the way you intended.\nBy adopting these best practices, you’ll be able to create content that is clean, easy to read, and easy to maintain."
  },
  {
    "objectID": "modules/slidedecks.html",
    "href": "modules/slidedecks.html",
    "title": "Slidedecks",
    "section": "",
    "text": "Intro\nUnable to display PDF file. Download instead.\n\n\nHomeostasis\nUnable to display PDF file. Download instead.\n\n\nTemperature\nUnable to display PDF file. Download instead.\n\n\nHost Response\nUnable to display PDF file. Download instead.\n\n\nDisease Selection\nUnable to display PDF file. Download instead.\n\n\nEpigentics 01\nUnable to display PDF file. Download instead.\n\n\nEpigentics 02\nUnable to display PDF file. Download instead.\n\n\nEpigentics 03\nUnable to display PDF file. Download instead."
  },
  {
    "objectID": "modules/10-climate.html",
    "href": "modules/10-climate.html",
    "title": "Climate Change",
    "section": "",
    "text": "Climate change, a pressing global issue, is significantly impacting coastal marine species, with notable effects on marine invertebrates.\n\n\nRising Sea Temperatures: The increase in global temperatures, a hallmark of climate change, has a direct effect on marine ecosystems. Marine invertebrates, such as corals, mollusks, and crustaceans, are particularly sensitive to temperature changes. For instance, coral bleaching, where corals lose their symbiotic algae due to stress from elevated temperatures, undermines the foundation of coral reef ecosystems. These reefs are vital as they provide habitat, food, and breeding grounds for a diverse range of marine life.\nOcean Acidification: The absorption of increased levels of atmospheric CO2 by the oceans leads to ocean acidification. This chemical change in seawater can have deleterious effects on calcifying invertebrates like mollusks and corals. The reduced pH levels interfere with the ability of these organisms to produce and maintain their calcium carbonate structures, essential for their survival and for the ecosystem as a whole.\nHabitat Loss and Alteration: Climate change contributes to sea-level rise and changes in ocean currents, leading to habitat loss and alteration. Coastal habitats like mangroves, which are crucial for many marine invertebrates, are being lost at an alarming rate. This loss not only affects the species residing there but also alters the larger ecological dynamics, including predator-prey relationships and breeding patterns.\nChanges in Species Distributions and Behavior: As ocean temperatures shift, marine invertebrates may migrate to cooler waters, leading to changes in geographical distribution. This shift can disrupt existing ecological balances, as new species interactions emerge and established ones are lost. For example, the migration of certain species might lead to the introduction of new predators or competitors, impacting local biodiversity.\nIncreased Incidence of Diseases: Warmer waters can lead to an increase in the prevalence and severity of diseases affecting marine invertebrates. Pathogens and parasites thrive in these conditions, posing a significant threat to species that have not evolved to cope with these new challenges.\nImpact on Reproduction and Life Cycles: Climate change can influence the reproductive cycles of marine invertebrates. Temperature changes can affect spawning times, larval development, and survival rates. For species with complex life cycles, like many marine invertebrates, even slight changes in environmental conditions can have cascading effects on their populations.\n\nIn conclusion, climate change is altering the very fabric of coastal marine ecosystems, with profound implications for marine invertebrates. Understanding and mitigating these impacts is crucial for preserving biodiversity and the health of our oceans. For those delving into marine biology or environmental sciences, appreciating the complexities of these changes is vital for developing effective aquaculture production and conservation strategies"
  },
  {
    "objectID": "modules/00-welcome.html",
    "href": "modules/00-welcome.html",
    "title": "Welcome",
    "section": "",
    "text": "There are few things to do beforehand if you have the time.\n\nComplete this pre-course survey\nGet a GitHub Account if you do not already have one."
  },
  {
    "objectID": "modules/00-welcome.html#pre-course-logistics",
    "href": "modules/00-welcome.html#pre-course-logistics",
    "title": "Welcome",
    "section": "",
    "text": "There are few things to do beforehand if you have the time.\n\nComplete this pre-course survey\nGet a GitHub Account if you do not already have one."
  },
  {
    "objectID": "modules/00-welcome.html#course-description",
    "href": "modules/00-welcome.html#course-description",
    "title": "Welcome",
    "section": "Course Description:",
    "text": "Course Description:\nThis one-week intensive graduate-level course explores the intriguing intersection of climate change, epigenetics, and marine invertebrate physiology, with a significant emphasis on the application of functional genomics. The course aims to highlight the substantial yet often overlooked role of marine invertebrates in coastal ecosystems, underlining their ecological significance and their responses to climate change at an epigenetic level.\nThe course starts by establishing a strong foundational understanding of epigenetics, climate change, and marine invertebrate physiology. Following this, students will delve into the intricate role of functional genomics in understanding these connections. Real-world examples will be extensively used to discuss how changes in climate directly and indirectly lead to alterations in the epigenetic mechanisms of various coastal marine invertebrates.\nBy the end of the course, students should have a comprehensive understanding of how climate change can influence the epigenetics and physiology of coastal marine invertebrates and the potential broader implications for marine ecosystems. The course is suitable for students with a basic understanding of genetics and climate science, although students from all disciplines are welcome as the course starts from foundational principles.\nThrough a mix of lectures, discussions, and interactive case studies, students will develop a nuanced understanding of the course topics. This course is highly recommended for students interested in marine biology, genetics, climate change, and conservation biology.\n\nObjectives:\nObjective 1: Understanding the Interplay between Climate Change, Epigenetics, and Marine Invertebrate Physiology\nThe first objective of the course is to help students gain a comprehensive understanding of how climate change impacts the physiological processes of marine invertebrates, especially through the lens of epigenetics. Students will learn about the fundamentals of marine invertebrate physiology and the various ways in which climate change-related stressors can influence these physiological systems, causing significant changes at an epigenetic level.\nObjective 2: Gaining Proficiency in Functional Genomics\nThe second objective is to ensure that students gain proficiency in the methodologies and applications of functional genomics, particularly in studying the epigenetic changes in marine invertebrates in response to climate change. Students will learn about various genomic techniques, their applications, and how to interpret data from these studies to gain insights into the epigenetic responses of marine invertebrates to climate stressors.\nObjective 3: Developing Skills for Analyzing and Evaluating Scientific Research\nThe third objective is to develop students’ ability to critically analyze and evaluate scientific literature in the field of marine invertebrate epigenetics and climate change. Through case studies and discussions on recent research, students will hone their skills in analyzing experimental designs, interpreting results, and understanding the broader implications of research findings for climate change adaptation and marine conservation efforts.\n\n\nContents:\nBasics of Epigenetics and Climate Change: This module provides an overview of climate change science and the principles of epigenetics. We discuss how the two fields intersect and introduce the concept of environmental epigenetics.\nPhysiology of Marine Invertebrates: This module focuses on the essential aspects of marine invertebrate physiology, covering topics such as feeding, digestion, reproduction, and response to environmental stressors.\nImpact of Climate Change on Marine Invertebrate Physiology: This section explores the specific impacts of various climate change-related factors, such as ocean acidification, temperature rise, and salinity changes on the physiology of marine invertebrates.\nFunctional Genomics and Epigenetics: This module delves into the methodologies and applications of functional genomics in studying epigenetic changes. Various genomic techniques such as DNA methylation analysis, chromatin immunoprecipitation, and RNA sequencing will be discussed.\nCase Studies and Current Research: The final module will involve an in-depth examination of recent research studies in the field. Students will be encouraged to critically analyze the studies and discuss the implications of the findings on our understanding of climate change impacts and the role of epigenetics in marine invertebrate adaptation."
  },
  {
    "objectID": "modules/00-welcome.html#methodology",
    "href": "modules/00-welcome.html#methodology",
    "title": "Welcome",
    "section": "Methodology:",
    "text": "Methodology:\nThrough a mix of lectures, discussions, and interactive data analysis, students will develop a nuanced understanding of the course topics. This course is highly recommended for students interested in marine biology, genetics, climate change, and conservation biology. The general format will be lecture in the morning and interactive data analysis in the afternoon. The latter will primarily involve using R and bash."
  },
  {
    "objectID": "modules/00-welcome.html#bibliography",
    "href": "modules/00-welcome.html#bibliography",
    "title": "Welcome",
    "section": "Bibliography:",
    "text": "Bibliography:\nCrandall, Grace, Rhonda Elliott Thompson, Benoit Eudeline, Brent Vadopalas, Emma Timmins-Schiffman, and Steven Roberts. 2022. “Proteomic Response of Early Juvenile Pacific Oysters (Crassostrea Gigas) to Temperature.” PeerJ 10 (October): e14158. https://doi.org/10.7717/peerj.14158.\nCrandall, Grace, Pamela C. Jensen, Samuel J. White, and Steven Roberts. 2022. “Characterization of the Gene Repertoire and Environmentally Driven Expression Patterns in Tanner Crab (Chionoecetes Bairdi).” Marine Biotechnology 24 (1): 216–25. https://doi.org/10.1007/s10126–022–10100–8.\nDang, Xin, Yong-Kian Lim, Yang Li, Steven B. Roberts, Li Li, and Vengatesen Thiyagarajan. 2023. “Epigenetic-Associated Phenotypic Plasticity of the Ocean Acidification-Acclimated Edible Oyster in the Mariculture Environment.” Molecular Ecology 32 (2): 412–27. https://doi.org/10.1111/mec.16751.\nDimond, James L., and Steven B. Roberts. 2016. “Germline DNA Methylation in Reef Corals: Patterns and Potential Roles in Response to Environmental Change.” Molecular Ecology 25 (8): 1895–1904. https://doi.org/10.1111/mec.13414.\nEirin-Lopez, Jose M., and Hollie M. Putnam. 2019. “Marine Environmental Epigenetics.” Annual Review of Marine Science 11 (January): 335–68. https://doi.org/10.1146/annurev-marine–010318–095114.\nGallardo-Escárate, C., V. Valenzuela-Muñoz, S. Boltaña, G. Nuñez-Acuña, D. Valenzuela-Miranda, A. T. Gonçalves, C. Détrée, et al. 2017. “The Caligus Rogercresseyi miRNome: Discovery and Transcriptome Profiling during the Sea Lice Ontogeny.” Agri Gene 4 (June): 8–22. https://doi.org/10.1016/j.aggene.2017.03.002.\nGallardo-Escárate, Cristian, Gabriel Arriagada, Crisleri Carrera, Ana Teresa Gonçalves, Gustavo Nuñez-Acuña, Diego Valenzuela-Miranda, and Valentina Valenzuela-Muñoz. 2019. “The Race between Host and Sea Lice in the Chilean Salmon Farming: A Genomic Approach.” Reviews in Aquaculture 11 (2): 325–39. https://doi.org/10.1111/raq.12334.\nGallardo-Escárate, Cristian, Valentina Valenzuela-Muñoz, Gustavo Nuñez-Acuña, Diego Valenzuela-Miranda, Fabian J. Tapia, Marco Yévenes, Gonzalo Gajardo, et al. 2023. “Chromosome-Level Genome Assembly of the Blue Mussel Mytilus Chilensis Reveals Molecular Signatures Facing the Marine Environment.” Genes 14 (4). https://doi.org/10.3390/genes14040876.\nGavery, Mackenzie R., and Steven B. Roberts. 2014. “A Context Dependent Role for DNA Methylation in Bivalves.” Briefings in Functional Genomics 13 (3): 217–22. https://doi.org/10.1093/bfgp/elt054.\nGurr, Samuel J., Shelly A. Trigg, Brent Vadopalas, Steven B. Roberts, and Hollie M. Putnam. 2022. “Acclimatory Gene Expression of Primed Clams Enhances Robustness to Elevated pCO2.” Molecular Ecology 31 (19): 5005–23. https://doi.org/10.1111/mec.16644.\nJuárez, Oscar E., Fabiola Lafarga-De la Cruz, Ignacio Leyva-Valencia, Edgar López-Landavery, Zaúl García-Esquivel, Fernando Díaz, Denisse Re-Araujo, Brent Vadopalas, and Clara E. Galindo-Sánchez. 2018. “Transcriptomic and Metabolic Response to Chronic and Acute Thermal Exposure of Juvenile Geoduck Clams Panopea Globosa.” Marine Genomics 42 (December): 1–13. https://doi.org/10.1016/j.margen.2018.09.003.\nNúñez-Acuña, Gustavo, Constanza Sáez-Vera, Diego Valenzuela-Miranda, Valentina Valenzuela-Muñoz, and Cristian Gallardo-Escárate. 2023. “Whole-Genome Resequencing in the Sea Louse Caligus Rogercresseyi Uncovers Gene Duplications and Copy Number Variants Associated with Pesticide Resistance.” Frontiers in Marine Science 10. https://doi.org/10.3389/fmars.2023.1112691.\nPutnam, Hollie M., Shelly A. Trigg, Samuel J. White, Laura H. Spencer, Brent Vadopalas, Aparna Natarajan, Jonathan Hetzel, et al. 2022. “Dynamic DNA Methylation Contributes to Carryover Effects and Beneficial Acclimatization in Geoduck Clams.” bioRxiv. https://doi.org/10.1101/2022.06.24.497506.\nRoberts, Steven B., and Mackenzie R. Gavery. 2012. “Is There a Relationship between DNA Methylation and Phenotypic Plasticity in Invertebrates?” Frontiers in Physiology 2 (January): 116. https://doi.org/10.3389/fphys.2011.00116.\nRoberts, Steven B., and Mackenzie R Gavery. 2017. “Epigenetic Considerations in Aquaculture.” PeerJ 5 (December): e4147. https://doi.org/10.7717/peerj.4147.\nSadler, Kirsten C. 2023. “Epigenetics across the Evolutionary Tree: New Paradigms from Non-Model Animals.” BioEssays: News and Reviews in Molecular, Cellular and Developmental Biology 45 (1): e2200036. https://doi.org/10.1002/bies.202200036.\nSilliman, Katherine, Laura H. Spencer, Samuel J. White, and Steven B. Roberts. 2023. “Epigenetic and Genetic Population Structure Is Coupled in a Marine Invertebrate.” Genome Biology and Evolution 15 (2). https://doi.org/10.1093/gbe/evad013.\nSpencer, Laura H., Erin Horkan, Ryan Crim, and Steven B. Roberts. 2021. “Latent Effects of Winter Warming on Olympia Oyster Reproduction and Larval Viability.” Journal of Experimental Marine Biology and Ecology 542–543 (September): 151604. https://doi.org/10.1016/j.jembe.2021.151604.\nSpencer, Laura H., Yaamini R. Venkataraman, Ryan Crim, Stuart Ryan, Micah J. Horwith, and Steven B. Roberts. 2020. “Carryover Effects of Temperature and pCO2 across Multiple Olympia Oyster Populations.” Ecological Applications: A Publication of the Ecological Society of America 30 (3): e02060. https://doi.org/10.1002/eap.2060.\nTimmins-Schiffman, Emma B., Grace A. Crandall, Brent Vadopalas, Michael E. Riffle, Brook L. Nunn, and Steven B. Roberts. 2017. “Integrating Discovery-Driven Proteomics and Selected Reaction Monitoring To Develop a Noninvasive Assay for Geoduck Reproductive Maturation.” Journal of Proteome Research 16 (9): 3298–3309. https://doi.org/10.1021/acs.jproteome.7b00288.\nTimmins-Schiffman, Emma, Samuel J. White, Rhonda Elliott Thompson, Brent Vadopalas, Benoit Eudeline, Brook L. Nunn, and Steven B. Roberts. 2021. “Coupled Microbiome Analyses Highlights Relative Functional Roles of Bacteria in a Bivalve Hatchery.” Environmental Microbiome 16 (1): 7. https://doi.org/10.1186/s40793–021–00376-z.\nTrigg, Shelly A., Yaamini R. Venkataraman, Mackenzie R. Gavery, Steven B. Roberts, Debashish Bhattacharya, Alan Downey-Wall, Jose M. Eirin-Lopez, et al. 2022. “Invertebrate Methylomes Provide Insight into Mechanisms of Environmental Tolerance and Reveal Methodological Biases.” Molecular Ecology Resources 22 (4): 1247–61. https://doi.org/10.1111/1755–0998.13542.\nValenzuela-Muñoz, Valentina, Juan Antonio Váldes, and Cristian Gallardo-Escárate. 2021. “Transcriptome Profiling of Long Non-Coding RNAs During the Atlantic Salmon Smoltification Process.” Marine Biotechnology 23 (2): 308–20. https://doi.org/10.1007/s10126–021–10024–9.\nVenkataraman, Yaamini R., Alan M. Downey-Wall, Justin Ries, Isaac Westfield, Samuel J. White, Steven B. Roberts, and Kathleen E. Lotterhos. 2020. “General DNA Methylation Patterns and Environmentally-Induced Differential Methylation in the Eastern Oyster (Crassostrea Virginica).” Frontiers in Marine Science 7. https://doi.org/10.3389/fmars.2020.00225.\nVenkataraman, Yaamini R., Samuel J. White, and Steven B. Roberts. 2022. “Differential DNA Methylation in Pacific Oyster Reproductive Tissue in Response to Ocean Acidification.” BMC Genomics 23 (1): 556. https://doi.org/10.1186/s12864–022–08781–5.\nWanamaker, Shelly A., Kaitlyn R. Mitchell, Rhonda Elliott Thompson, Benoit Eudeline, Brent Vadopalas, Emma B. Timmins-Schiffman, and Steven B. Roberts. 2020. “Temporal Proteomic Profiling Reveals Insight into Critical Developmental Processes and Temperature-Influenced Physiological Response Differences in a Bivalve Mollusc.” BMC Genomics 21 (1): 723. https://doi.org/10.1186/s12864–020–07127–3."
  },
  {
    "objectID": "modules/21-supp.html",
    "href": "modules/21-supp.html",
    "title": "Epigenetics : Basics",
    "section": "",
    "text": "Webpage: Why you should lick your rat\n#Types"
  },
  {
    "objectID": "modules/21-supp.html#histone-modification",
    "href": "modules/21-supp.html#histone-modification",
    "title": "Epigenetics : Basics",
    "section": "Histone Modification",
    "text": "Histone Modification\nHistone modifications are a key aspect of epigenetics, the study of how genes are regulated without altering the underlying DNA sequence. Histones are proteins around which DNA is wrapped in the cell nucleus. These proteins can be chemically modified in various ways, affecting how tightly or loosely the DNA is wound around them. These modifications include methylation, acetylation, phosphorylation, ubiquitination, and sumoylation, among others.\n\nMethylation: The addition of methyl groups (CH3) to histones can either activate or repress gene expression, depending on where the methyl group is added. For instance, methylation of histone H3 at lysine 4 (H3K4me) is generally associated with gene activation, while methylation at H3K9 is linked with gene repression.\nAcetylation: The addition of acetyl groups (COCH3) to histones typically loosens the DNA-histone interaction, making the DNA more accessible for transcription and thus promoting gene expression. Histone acetyltransferases (HATs) add acetyl groups, while histone deacetylases (HDACs) remove them.\nPhosphorylation: The addition of phosphate groups (PO4) to histones can influence various cellular processes, including DNA repair, chromosome condensation during cell division, and gene expression.\nUbiquitination and Sumoylation: These modifications involve adding ubiquitin or SUMO (small ubiquitin-like modifier) proteins to histones, affecting DNA repair, transcriptional regulation, and chromatin organization.\n\nHistone modifications regulate gene expression by altering the chromatin structure. This can either make the DNA more accessible to transcription factors and other proteins that promote transcription (thus activating gene expression) or make it less accessible (thereby repressing gene expression). These modifications do not change the DNA sequence but rather affect how cells interpret genes, contributing to the complexity of gene regulation in eukaryotic organisms.\nOverall, histone modifications are a dynamic and complex means of regulating gene expression, playing crucial roles in development, differentiation, and the response to environmental signals."
  },
  {
    "objectID": "modules/21-supp.html#short-rnas",
    "href": "modules/21-supp.html#short-rnas",
    "title": "Epigenetics : Basics",
    "section": "Short RNAs",
    "text": "Short RNAs\nShort RNAs, also known as small RNAs, are a class of non-coding RNA molecules that are typically 20-30 nucleotides long. They play crucial roles in gene regulation and maintenance of genome stability. The most well-known types of short RNAs include microRNAs (miRNAs), small interfering RNAs (siRNAs), and Piwi-interacting RNAs (piRNAs).\n\nMicroRNAs (miRNAs): miRNAs are involved in post-transcriptional regulation of gene expression. They bind to complementary sequences on messenger RNA (mRNA) molecules, typically leading to the degradation of the mRNA or inhibition of its translation into proteins. miRNAs are key players in regulating various cellular processes including development, differentiation, cell proliferation, and apoptosis.\nSmall Interfering RNAs (siRNAs): siRNAs are typically involved in the RNA interference (RNAi) pathway. They originate from long double-stranded RNA precursors and are processed into siRNAs by an enzyme called Dicer. siRNAs are incorporated into the RNA-induced silencing complex (RISC), where they guide the complex to complementary mRNA targets, leading to mRNA cleavage and gene silencing. This mechanism is widely used in research to knock down the expression of specific genes.\nPiwi-interacting RNAs (piRNAs): piRNAs are primarily found in animal cells and are associated with the Piwi subfamily of Argonaute proteins. They are involved in the silencing of transposable elements and other repetitive sequences in the genome, particularly in germ cells. This helps in protecting the integrity of the genome.\n\nApart from these, there are other classes of small RNAs, like small nucleolar RNAs (snoRNAs) and small nuclear RNAs (snRNAs), which are involved in processes like RNA modification (e.g., methylation) and splicing, respectively.\nOverall, short RNAs are fundamental to the regulation of gene expression and maintenance of genomic integrity in cells. They play vital roles in a variety of biological processes and are also being studied for their potential in therapeutic applications, such as in the treatment of diseases like cancer and viral infections."
  },
  {
    "objectID": "modules/21-supp.html#long-non-coding-rna",
    "href": "modules/21-supp.html#long-non-coding-rna",
    "title": "Epigenetics : Basics",
    "section": "Long-non coding RNA",
    "text": "Long-non coding RNA\nLong non-coding RNAs (lncRNAs) are a class of RNA molecules that are typically longer than 200 nucleotides and do not encode proteins. Despite not being translated into proteins, lncRNAs play significant roles in various cellular processes, and their functions are diverse and complex. Here are some key aspects of lncRNAs and their functions:\n\nRegulation of Gene Expression: LncRNAs can regulate gene expression at multiple levels, including chromatin modification, transcription, and post-transcriptional processing. They can act as guides, scaffolds, decoys, or enhancers, depending on their interaction with DNA, RNA, or proteins.\n\nAs Guides: LncRNAs can direct ribonucleoprotein complexes to specific genomic loci to modify chromatin and regulate transcription.\nAs Scaffolds: They can bring together multiple proteins to form ribonucleoprotein complexes, influencing the activity and localization of these proteins.\nAs Decoys: LncRNAs can bind to and sequester proteins (like transcription factors or other RNA-binding proteins) away from their usual targets.\nAs Enhancers: Some lncRNAs can enhance gene expression by interacting with enhancer regions of the genome.\n\nEpigenetic Regulation: LncRNAs are involved in epigenetic modifications like DNA methylation and histone modification, thereby influencing gene expression patterns without altering the underlying DNA sequence.\nStructural Organization: They contribute to the structural organization of certain cellular compartments and can influence the assembly of macromolecular complexes.\nX Chromosome Inactivation: In female mammals, the lncRNA Xist plays a crucial role in X chromosome inactivation, a process by which one of the X chromosomes is silenced to ensure dosage compensation.\nAlternative Splicing: LncRNAs can modulate alternative splicing of pre-mRNAs, affecting the diversity of proteins produced in cells.\nRegulation of mRNA Stability and Translation: Certain lncRNAs can influence the stability and translation of mRNAs, either by direct interaction or through the recruitment of other regulatory molecules.\nCell Differentiation and Development: LncRNAs are essential in numerous developmental processes, where they guide cell fate decisions and differentiation.\nDisease Association: Dysregulation of lncRNAs has been linked to various diseases, including cancers, neurological disorders, and cardiovascular diseases.\n\nResearch into lncRNAs is still a rapidly evolving field. These RNAs are more numerous and diverse than initially thought, and they are increasingly recognized as key regulatory molecules in a wide range of biological processes and diseases. Their complexity and versatility make them important subjects for both basic biological research and potential therapeutic applications."
  },
  {
    "objectID": "modules/21-supp.html#dna-methylation",
    "href": "modules/21-supp.html#dna-methylation",
    "title": "Epigenetics : Basics",
    "section": "DNA Methylation",
    "text": "DNA Methylation\nDNA methylation is a crucial epigenetic mechanism involved in regulating gene expression. It typically involves the addition of a methyl group (CH3) to the 5th carbon of the cytosine ring in DNA. This process predominantly occurs in regions of DNA where cytosine nucleotides are followed by guanine nucleotides, known as CpG sites. DNA methylation is catalyzed by enzymes called DNA methyltransferases (DNMTs).\nThe functional role of DNA methylation includes:\n\nGene Regulation: DNA methylation can either repress or activate gene expression. Methylation in gene promoter regions generally leads to gene silencing. This is because methylated DNA can inhibit the binding of transcription factors necessary for gene expression, or it can recruit proteins that condense the DNA, making it less accessible for transcription.\nDevelopment and Differentiation: DNA methylation patterns are crucial for normal development and cellular differentiation. As cells differentiate, specific genes are methylated or demethylated, which helps in the maintenance of cell-specific gene expression patterns.\nX-Chromosome Inactivation: In female mammals, DNA methylation plays a critical role in X-chromosome inactivation, a process by which one of the two X chromosomes in females is silenced to ensure dosage compensation with males, who have only one X chromosome.\nImprinting: Genomic imprinting is a process where only one allele of a gene is expressed while the other is silenced. This silencing is often achieved through DNA methylation. Imprinted genes are crucial for growth and development, especially in the womb.\nStability of the Genome: DNA methylation helps in maintaining genome stability by suppressing the activity of transposable elements, which can move around the genome and cause mutations or chromosomal rearrangements if not properly controlled.\nResponse to Environmental Factors: Environmental factors can influence DNA methylation patterns, which in turn can affect gene expression. This is one way in which environmental factors can have a long-term impact on an organism’s biology.\n\nAlterations in DNA methylation patterns are associated with various diseases, including cancer, where abnormal hypermethylation of tumor suppressor genes or hypomethylation of oncogenes can occur. Research in this field continues to uncover the complex roles of DNA methylation in health and disease, making it a critical area of study in genetics and epigenetics."
  },
  {
    "objectID": "modules/03-rstudio.html",
    "href": "modules/03-rstudio.html",
    "title": "RStudio",
    "section": "",
    "text": "RStudio is an Integrated Development Environment (IDE) for R, a programming language for statistical computing and data visualization. Developed by RStudio, Inc., this IDE provides a user-friendly interface to R, making it easier to write code, run analyses, and produce plots. It includes features such as syntax highlighting, code completion, and the ability to run R code interactively.\n\n\n\n\nRStudio consolidates your code, plots, and output in one place, improving workflow and making the process more efficient.\n\n\n\nWith features like auto-completion and built-in debugging tools, RStudio speeds up the coding process.\n\n\n\nRStudio simplifies the process of creating complex data visualizations by providing easy-to-use interfaces for ggplot2, Shiny, and other R packages.\n\n\n\nRStudio includes integrated support for Git and GitHub, making it easier to manage changes to your code and collaborate with others.\n\n\n\nRStudio supports various R packages and also allows the use of other programming languages like C++, Python, and SQL within the IDE.\n\n\n\n\n\nScript Editor: Write and edit your R scripts.\nConsole: Run R commands interactively.\n&gt; print(\"Hello, World!\")\nEnvironment: View and manage all variables, data frames, and other objects in your R session.\nPlots: Visualize your data and generate plots easily.\nggplot(data, aes(x=x, y=y)) + geom_point()\nPackages: Install and manage R packages.\ninstall.packages(\"tidyverse\")\nHelp: Access R documentation quickly.\nFile Browser: Navigate your file system and manage your project files.\nVersion Control: Manage Git repositories directly within RStudio.\ngit commit -m \"Initial commit\"\nShiny Apps: Build interactive web apps right within RStudio.\n\n\n\n\n\n\nUse RStudio Projects to keep your scripts, data, and other files organized. This makes it easier to manage complex analyses and collaborate with others.\n\n\n\nUse comments to describe what your code is doing. This makes it easier for you (and others) to understand the logic later.\n```R\n# Calculate the mean of x\nmean_x &lt;- mean(x)\n```\n\n\n\nMake your code and analyses reproducible. Use relative file paths and R Markdown documents to ensure others can easily run your code.\n\n\n\nUse Git to keep track of changes in your project. This is invaluable for collaboration and data science project management.\n\n\n\nDon’t reinvent the wheel. Make use of R’s extensive library of packages and functions to perform common tasks.\n\n\n\nLearn RStudio’s keyboard shortcuts to navigate the IDE more efficiently.\n\n\n\nTo make use of the latest features and improvements, keep your R and RStudio installations up to date.\nBy adhering to these best practices, you can make the most out of RStudio, whether you’re doing data analysis, statistical modeling, or creating data visualizations."
  },
  {
    "objectID": "modules/03-rstudio.html#why-is-rstudio-useful",
    "href": "modules/03-rstudio.html#why-is-rstudio-useful",
    "title": "RStudio",
    "section": "",
    "text": "RStudio consolidates your code, plots, and output in one place, improving workflow and making the process more efficient.\n\n\n\nWith features like auto-completion and built-in debugging tools, RStudio speeds up the coding process.\n\n\n\nRStudio simplifies the process of creating complex data visualizations by providing easy-to-use interfaces for ggplot2, Shiny, and other R packages.\n\n\n\nRStudio includes integrated support for Git and GitHub, making it easier to manage changes to your code and collaborate with others.\n\n\n\nRStudio supports various R packages and also allows the use of other programming languages like C++, Python, and SQL within the IDE."
  },
  {
    "objectID": "modules/03-rstudio.html#key-rstudio-features-and-examples",
    "href": "modules/03-rstudio.html#key-rstudio-features-and-examples",
    "title": "RStudio",
    "section": "",
    "text": "Script Editor: Write and edit your R scripts.\nConsole: Run R commands interactively.\n&gt; print(\"Hello, World!\")\nEnvironment: View and manage all variables, data frames, and other objects in your R session.\nPlots: Visualize your data and generate plots easily.\nggplot(data, aes(x=x, y=y)) + geom_point()\nPackages: Install and manage R packages.\ninstall.packages(\"tidyverse\")\nHelp: Access R documentation quickly.\nFile Browser: Navigate your file system and manage your project files.\nVersion Control: Manage Git repositories directly within RStudio.\ngit commit -m \"Initial commit\"\nShiny Apps: Build interactive web apps right within RStudio."
  },
  {
    "objectID": "modules/03-rstudio.html#best-practices",
    "href": "modules/03-rstudio.html#best-practices",
    "title": "RStudio",
    "section": "",
    "text": "Use RStudio Projects to keep your scripts, data, and other files organized. This makes it easier to manage complex analyses and collaborate with others.\n\n\n\nUse comments to describe what your code is doing. This makes it easier for you (and others) to understand the logic later.\n```R\n# Calculate the mean of x\nmean_x &lt;- mean(x)\n```\n\n\n\nMake your code and analyses reproducible. Use relative file paths and R Markdown documents to ensure others can easily run your code.\n\n\n\nUse Git to keep track of changes in your project. This is invaluable for collaboration and data science project management.\n\n\n\nDon’t reinvent the wheel. Make use of R’s extensive library of packages and functions to perform common tasks.\n\n\n\nLearn RStudio’s keyboard shortcuts to navigate the IDE more efficiently.\n\n\n\nTo make use of the latest features and improvements, keep your R and RStudio installations up to date.\nBy adhering to these best practices, you can make the most out of RStudio, whether you’re doing data analysis, statistical modeling, or creating data visualizations."
  },
  {
    "objectID": "modules/03-rstudio.html#r-markdown-and-report-generation",
    "href": "modules/03-rstudio.html#r-markdown-and-report-generation",
    "title": "RStudio",
    "section": "R Markdown and report generation",
    "text": "R Markdown and report generation\nTo effectively generate pretty reports you need to understand\n\nDocument structure: Learn the structure of an R Markdown document, which consists of a YAML header (metadata), code chunks, and narrative text.\nYAML header: Familiarize yourself with the YAML header and its key components such as ‘title’, ‘author’, ‘date’, and ‘output’. Customize the output format and options (e.g., ‘html_document’, ‘pdf_document’, or ‘word_document’).\nCode chunks: Understand how to insert and customize code chunks using triple backticks (```{r}), options like ‘echo’, ‘eval’, ‘include’, and ‘cache’, and inline R code using r.\nMarkdown syntax: Learn the basic Markdown syntax for formatting text, such as headers, lists, tables, links, images, and emphasis (bold, italics).\nKnitting: Get comfortable with the process of knitting an R Markdown document to generate the desired output format (e.g., HTML, PDF, or Word) using the “Knit” button in RStudio or the ‘rmarkdown::render()’ function.\nReproducible research: Learn the importance of reproducible research and best practices for organizing R projects, version control, and data management."
  },
  {
    "objectID": "modules/03-rstudio.html#code-chunks",
    "href": "modules/03-rstudio.html#code-chunks",
    "title": "RStudio",
    "section": "Code Chunks",
    "text": "Code Chunks\nCode chunk options are used to control the behavior and appearance of R code chunks in R Markdown documents. They are set within the curly braces {} following the language identifier (e.g., r). Here is a description of essential code chunk options to know and use:\n\necho: Determines whether the code chunk is displayed in the output document. Set echo=TRUE to display the code or echo=FALSE to hide it. The default is TRUE.\neval: Controls whether the code chunk is executed. Set eval=TRUE to execute the code or eval=FALSE to prevent execution. The default is TRUE.\ninclude: Determines whether the code chunk, its output, or both are included in the final output. Set include=TRUE to include both or include=FALSE to exclude both. The default is TRUE.\nresults: Controls the display of code chunk results. Options include 'markup' (default) to include the output as-is, 'hide' to hide the output, 'asis' to display raw results, and 'hold' to display all output at once at the end of the code chunk.\nmessage: Controls whether to display messages generated by the code chunk. Set message=TRUE to display messages or message=FALSE to hide them. The default is TRUE.\nwarning: Determines whether to display warnings generated by the code chunk. Set warning=TRUE to display warnings or warning=FALSE to hide them. The default is TRUE.\nerror: Controls whether to stop knitting if a code chunk generates an error. Set error=TRUE to continue knitting even if an error occurs or error=FALSE to stop knitting. The default is FALSE.\nfig.width and fig.height: Set the width and height of the output plots, respectively, in inches. For example, fig.width=6 and fig.height=4 set a 6x4-inch plot size.\nfig.align: Controls the horizontal alignment of plots in the output document. Options include 'left', 'center', and 'right'. The default is 'default', which depends on the output format.\ncache: Determines whether to cache the results of a code chunk. Set cache=TRUE to cache the results or cache=FALSE to re-run the code chunk every time the document is knit. The default is FALSE.\n\nBy understanding and using these essential code chunk options, you can gain better control over the execution, display, and formatting of your R code and its output within R Markdown documents.\n\ncache option\nThe cache option in R Markdown allows you to cache the results of code chunks, so they don’t need to be re-evaluated every time the document is knit. This can significantly speed up the knitting process for documents with computationally intensive or time-consuming code chunks.\nBenefits of using the cache option:\n\nFaster knitting: By caching the results of expensive code chunks, you can save time and resources when re-knitting your document, especially when only making small changes that don’t affect the cached chunks.\nConsistency: When working with random processes or time-sensitive data, caching the results can help maintain consistency across multiple versions of the document.\nResource management: Caching can help manage resources for large datasets or computationally intensive tasks that may otherwise cause the knitting process to fail or become unresponsive.\n\nHere’s an example of using the cache option in an R Markdown code chunk:\n```{r expensive-operation, cache=TRUE}\n# Simulate a time-consuming operation\nSys.sleep(10)\nresult &lt;- rnorm(1000, mean=100, sd=15)\nsummary(result)\n```\nIn this example, the code chunk simulates a time-consuming operation by waiting for 10 seconds before generating random data. By setting cache=TRUE, the results of this code chunk are cached, so that they are not re-evaluated every time the document is knit. This can save time and ensure that the random data remains consistent between document versions.\nKeep in mind that you should use the cache option carefully, as it may cause unexpected behavior if you’re caching results that depend on external resources or dynamic data. Always verify that your document produces the desired output when using the cache option."
  },
  {
    "objectID": "modules/03-rstudio.html#global-chunk-options",
    "href": "modules/03-rstudio.html#global-chunk-options",
    "title": "RStudio",
    "section": "Global Chunk Options",
    "text": "Global Chunk Options\nGlobal chunk options are settings that apply to all code chunks in an R Markdown document by default. You can set global options using the knitr::opts_chunk$set() function at the beginning of your R Markdown document, typically in an initial code chunk. By setting global options, you can maintain consistency across all code chunks and reduce the need to set options individually for each chunk.\nHere’s an example of setting global chunk options:\n```{r}\nlibrary(knitr)\nopts_chunk$set(\n  echo = TRUE,         # Display code chunks\n  eval = TRUE,         # Evaluate code chunks\n  warning = FALSE,     # Hide warnings\n  message = FALSE,     # Hide messages\n  fig.width = 6,       # Set plot width in inches\n  fig.height = 4,      # Set plot height in inches\n  fig.align = \"center\" # Align plots to the center\n)\n```"
  },
  {
    "objectID": "modules/03-rstudio.html#tables-and-images",
    "href": "modules/03-rstudio.html#tables-and-images",
    "title": "RStudio",
    "section": "Tables and Images",
    "text": "Tables and Images\nIn R Markdown, you can add tables and images using either Markdown syntax or R code.\n\nAdding tables:\n\nMarkdown syntax: You can create a simple table using pipes | and hyphens -. Here’s an example:\n\n| Column1 | Column2 | Column3 |\n|---------|---------|---------|\n| A       | B       | C       |\n| X       | Y       | Z       |\nThis will create a table with two rows and three columns.\n\nR code: You can create more complex tables using R packages like kable from the knitr package, or gt and flextable. Here’s an example using kable:\n\n```{r}\nlibrary(knitr)\n\ndata &lt;- data.frame(\n  Column1 = c(\"A\", \"X\"),\n  Column2 = c(\"B\", \"Y\"),\n  Column3 = c(\"C\", \"Z\")\n)\n\nkable(data, caption = \"An example table\")\n```\nThis will generate a table with the specified data and caption."
  },
  {
    "objectID": "modules/03-rstudio.html#adding-images",
    "href": "modules/03-rstudio.html#adding-images",
    "title": "RStudio",
    "section": "Adding images:",
    "text": "Adding images:\n\nMarkdown syntax: You can insert an image using the following syntax: ![alt text](path/to/image \"Optional title\"). Here’s an example:\n\n![Example image](path/to/image.jpg \"Optional title\")\nMake sure to replace path/to/image.jpg with the actual file path or URL of the image.\n\nR code: You can also add images using R code, especially if you’re generating images with R plots. Here’s two examples:\n\n```{r}\nplot(cars, main = \"An example plot\", xlab = \"Speed\", ylab = \"Distance\")\n```\n```{r schemat, echo = FALSE, out.width = “70%”, fig.align = “center”}\nknitr::include_graphics(“img/ncbi.png”)\n```\nThe benefit of the this code as opposed to Mardown (above) is that you the ability to change size and align"
  },
  {
    "objectID": "modules/03-rstudio.html#shiny-apps",
    "href": "modules/03-rstudio.html#shiny-apps",
    "title": "RStudio",
    "section": "Shiny Apps",
    "text": "Shiny Apps\nRStudio also provides a framework for building interactive web applications called Shiny apps. Shiny apps are built using R code and can be easily deployed on the web. They allow users to interact with data and visualizations in real-time, making it easy to explore and analyze complex data sets. Shiny apps are ideal for building dashboards, interactive reports, and other data-driven applications."
  },
  {
    "objectID": "modules/05-computers.html",
    "href": "modules/05-computers.html",
    "title": "Computing Resources",
    "section": "",
    "text": "Our lab has a number of different computing resources available for people to take advantage of."
  },
  {
    "objectID": "modules/05-computers.html#raven",
    "href": "modules/05-computers.html#raven",
    "title": "Computing Resources",
    "section": "Raven",
    "text": "Raven\nRaven is a Linux (Ubuntu 18.04LTS) computer with 48 CPUs, 256GB of RAM, and ~24TB of hard drive (HDD) storage."
  },
  {
    "objectID": "modules/05-computers.html#hyak",
    "href": "modules/05-computers.html#hyak",
    "title": "Computing Resources",
    "section": "Hyak",
    "text": "Hyak\nHyak is the Univ. of Washington’s high-performance computing (HPC) cluster. We have two Linux (RedHat CentOS) “nodes” (i.e. computers) available:\n\n28 CPUs, 128GB RAM\n28 CPUs, 512GB RAM"
  },
  {
    "objectID": "modules/05-computers.html#raven-1",
    "href": "modules/05-computers.html#raven-1",
    "title": "Computing Resources",
    "section": "Raven",
    "text": "Raven\n\nRStudio Server\nRead about why you should use RStudio Server.\n\nRequest Access to RStudio Server\n\nRequest RStudio Server access using the repo Issues. If you are not an existing UW employee/student, please provide a current email address.\n\nIf you already have a UW NetID, proceed to Step #3.\nIf you do not have a UW NetID, proceed to Step #2.\n\nIf you are not an existing UW employee/student, please provide Sam with a current email address. You will receive instructions from UW IT with instructions on how to register with UW and receive a UW NetID. Steven or Sam will notify you in the the repo Issue you created when you can proceed to Step 3.\nOnce you have a UW NetID, download and install Husky OnNet. This is a VPN service that provides access to computers hosted on the UW network.\nPost your UW Net ID in the repo Issue you created.\nSteven or Sam will send you a direct message in Slack with your RStudio Server login credentials.\n\n\n\n\nAccessing RStudio Server\n\nActivate Husky OnNet VPN service.\nPaste the following URL in your internet browser:\n\nhttp://raven.fish.washington.edu:8787\nIf you receive a notice from your browser regarding “insecure connection”, you may safely ignore this and proceed.\n\nUse login credentials provided by Steven or Sam.\nIf you encounter any issues, please create a new Issue. Please post screenshots and paste text of any error messages you encounter.\n\n\n\n\nAdministration Instructions\n\nSteven/Sam go here to sponsor UW NetID: https://uwnetid.washington.edu/sponsor/\nAfter acceptance by user, provision access to Husky OnNet with their UW NetID: https://provision.uw.edu/"
  },
  {
    "objectID": "modules/30-lncRNA.html",
    "href": "modules/30-lncRNA.html",
    "title": "lncRNA Discovery",
    "section": "",
    "text": "1 Run HiSat on RNA-seq\n\n1.1 Grab Trimmed RNA-seq Reads\n1.2 Genome\n1.3 HiSat\n1.4 convert to bams\n1.5 Looking at Bams\n\n2 StringTie\n3 GFFcompare\n4 Filter\n5 Bedtools\n6 CPC2\n7 subsetting fasta\n8 Getting genome feature track"
  },
  {
    "objectID": "modules/30-lncRNA.html#grab-trimmed-rna-seq-reads",
    "href": "modules/30-lncRNA.html#grab-trimmed-rna-seq-reads",
    "title": "lncRNA Discovery",
    "section": "1.1 Grab Trimmed RNA-seq Reads",
    "text": "1.1 Grab Trimmed RNA-seq Reads\n\nwget -r \\\n--no-directories --no-parent \\\n-P ../data/fastq/ \\\n-A \"*fastq.gz\" https://gannet.fish.washington.edu/Atumefaciens/20230519-E5_coral-fastqc-fastp-multiqc-RNAseq/A_pulchra/trimmed/"
  },
  {
    "objectID": "modules/30-lncRNA.html#genome",
    "href": "modules/30-lncRNA.html#genome",
    "title": "lncRNA Discovery",
    "section": "1.2 Genome",
    "text": "1.2 Genome\ncd ../data\n\ncurl -O http://gannet.fish.washington.edu/seashell/snaps/GCF_013753865.1_Amil_v2.1_genomic.fna\nhead ../data/Amil/ncbi_dataset/data/GCF_013753865.1/genomic.gtf\n\nwc -l ../data/Amil/ncbi_dataset/data/GCF_013753865.1/genomic.gtf\n\ngrep -v '^#' ../data/Amil/ncbi_dataset/data/GCF_013753865.1/genomic.gtf | cut -f3 | sort | uniq\n\ngrep -c \"transcript\" ../data/Amil/ncbi_dataset/data/GCF_013753865.1/genomic.gtf\n\ngrep -c \"gene\" ../data/Amil/ncbi_dataset/data/GCF_013753865.1/genomic.gtf\nhead ../data/Amil/ncbi_dataset/data/GCF_013753865.1/genomic.gff\n\nwc -l ../data/Amil/ncbi_dataset/data/GCF_013753865.1/genomic.gff\n\ngrep -v '^#' ../data/Amil/ncbi_dataset/data/GCF_013753865.1/genomic.gff | cut -f3 | sort | uniq\n\ngrep -c \"transcript\" ../data/Amil/ncbi_dataset/data/GCF_013753865.1/genomic.gff\n\ngrep -c \"gene\" ../data/Amil/ncbi_dataset/data/GCF_013753865.1/genomic.gff"
  },
  {
    "objectID": "modules/30-lncRNA.html#hisat",
    "href": "modules/30-lncRNA.html#hisat",
    "title": "lncRNA Discovery",
    "section": "1.3 HiSat",
    "text": "1.3 HiSat\n/home/shared/hisat2-2.2.1/hisat2-build \\\n../data/GCF_013753865.1_Amil_v2.1_genomic.fna \\\n../output/05.33-lncRNA-discovery/GCF_013753865.1_Amil_v2.1.index \\\n-p 24 \\\n../data/Amil/ncbi_dataset/data/GCF_013753865.1/genomic.gtf \\\n2&gt; ../output/05.33-lncRNA-discovery/hisat2-build_stats.txt\ncat ../output/05.33-lncRNA-discovery/hisat2-build_stats.txt\nSettings:\n  Output files: \"../output/05.33-lncRNA-discovery/GCF_013753865.1_Amil_v2.1.index.*.ht2\"\n  Line rate: 6 (line is 64 bytes)\n  Lines per side: 1 (side is 64 bytes)\n  Offset rate: 4 (one in 16)\n  FTable chars: 10\n  Strings: unpacked\n  Local offset rate: 3 (one in 8)\n  Local fTable chars: 6\n  Local sequence length: 57344\n  Local sequence overlap between two consecutive indexes: 1024\n  Endianness: little\n  Actual local endianness: little\n  Sanity checking: disabled\n  Assertions: disabled\n  Random seed: 0\n  Sizeofs: void*:8, int:4, long:8, size_t:8\nInput files DNA, FASTA:\n  ../data/GCF_013753865.1_Amil_v2.1_genomic.fna\nReading reference sizes\n  Time reading reference sizes: 00:00:02\nCalculating joined length\nWriting header\nReserving space for joined string\nJoining reference sequences\n  Time to join reference sequences: 00:00:02\n  Time to read SNPs and splice sites: 00:00:00\nUsing parameters --bmax 3713613 --dcv 1024\n  Doing ahead-of-time memory usage test\n  Passed!  Constructing with these parameters: --bmax 3713613 --dcv 1024\nConstructing suffix-array element generator\nConverting suffix-array elements to index image\nAllocating ftab, absorbFtab\nEntering GFM loop\nExited GFM loop\nfchr[A]: 0\nfchr[C]: 144834195\nfchr[G]: 237637860\nfchr[T]: 330514140\nfchr[$]: 475342477\nExiting GFM::buildToDisk()\nReturning from initFromVector\nWrote 162768226 bytes to primary GFM file: ../output/05.33-lncRNA-discovery/GCF_013753865.1_Amil_v2.1.index.1.ht2\nWrote 118835624 bytes to secondary GFM file: ../output/05.33-lncRNA-discovery/GCF_013753865.1_Amil_v2.1.index.2.ht2\nRe-opening _in1 and _in2 as input streams\nReturning from GFM constructor\nReturning from initFromVector\nWrote 212701387 bytes to primary GFM file: ../output/05.33-lncRNA-discovery/GCF_013753865.1_Amil_v2.1.index.5.ht2\nWrote 120924662 bytes to secondary GFM file: ../output/05.33-lncRNA-discovery/GCF_013753865.1_Amil_v2.1.index.6.ht2\nRe-opening _in5 and _in5 as input streams\nReturning from HGFM constructor\nHeaders:\n    len: 475342477\n    gbwtLen: 475342478\n    nodes: 475342478\n    sz: 118835620\n    gbwtSz: 118835620\n    lineRate: 6\n    offRate: 4\n    offMask: 0xfffffff0\n    ftabChars: 10\n    eftabLen: 0\n    eftabSz: 0\n    ftabLen: 1048577\n    ftabSz: 4194308\n    offsLen: 29708905\n    offsSz: 118835620\n    lineSz: 64\n    sideSz: 64\n    sideGbwtSz: 48\n    sideGbwtLen: 192\n    numSides: 2475743\n    numLines: 2475743\n    gbwtTotLen: 158447552\n    gbwtTotSz: 158447552\n    reverse: 0\n    linearFM: Yes\nTotal time for call to driver() for forward index: 00:02:37\nfind ../data/fastq/*gz\n../data/fastq/RNA-ACR-140-S1-TP2_R1_001.fastp-trim.20230519.fastq.gz\n../data/fastq/RNA-ACR-140-S1-TP2_R2_001.fastp-trim.20230519.fastq.gz\n../data/fastq/RNA-ACR-145-S1-TP2_R1_001.fastp-trim.20230519.fastq.gz\n../data/fastq/RNA-ACR-145-S1-TP2_R2_001.fastp-trim.20230519.fastq.gz\n../data/fastq/RNA-ACR-150-S1-TP2_R1_001.fastp-trim.20230519.fastq.gz\n../data/fastq/RNA-ACR-150-S1-TP2_R2_001.fastp-trim.20230519.fastq.gz\n../data/fastq/RNA-ACR-173-S1-TP2_R1_001.fastp-trim.20230519.fastq.gz\n../data/fastq/RNA-ACR-173-S1-TP2_R2_001.fastp-trim.20230519.fastq.gz\n../data/fastq/RNA-ACR-178-S1-TP2_R1_001.fastp-trim.20230519.fastq.gz\n../data/fastq/RNA-ACR-178-S1-TP2_R2_001.fastp-trim.20230519.fastq.gz\nfind ../data/fastq/*R2_001.fastp-trim.20230519.fastq.gz \\\n| xargs basename -s -S1-TP2_R2_001.fastp-trim.20230519.fastq.gz | xargs -I{} \\\necho {}\nRNA-ACR-140\nRNA-ACR-145\nRNA-ACR-150\nRNA-ACR-173\nRNA-ACR-178\nfind ../data/fastq/*R2_001.fastp-trim.20230519.fastq.gz \\\n| xargs basename -s -S1-TP2_R2_001.fastp-trim.20230519.fastq.gz | xargs -I{} \\\n/home/shared/hisat2-2.2.1/hisat2 \\\n-x ../output/05.33-lncRNA-discovery/GCF_013753865.1_Amil_v2.1.index\\ \\\n-p 48 \\\n-1 ../data/fastq/{}-S1-TP2_R1_001.fastp-trim.20230519.fastq.gz \\\n-2 ../data/fastq/{}-S1-TP2_R2_001.fastp-trim.20230519.fastq.gz \\\n-S ../output/05.33-lncRNA-discovery/{}.sam \\\n2&gt; ../output/05.33-lncRNA-discovery/hisat.out\ncat ../output/05.33-lncRNA-discovery/hisat.out\n47710408 reads; of these:\n  47710408 (100.00%) were paired; of these:\n    27060558 (56.72%) aligned concordantly 0 times\n    19176285 (40.19%) aligned concordantly exactly 1 time\n    1473565 (3.09%) aligned concordantly &gt;1 times\n    ----\n    27060558 pairs aligned concordantly 0 times; of these:\n      1274633 (4.71%) aligned discordantly 1 time\n    ----\n    25785925 pairs aligned 0 times concordantly or discordantly; of these:\n      51571850 mates make up the pairs; of these:\n        41230080 (79.95%) aligned 0 times\n        9296317 (18.03%) aligned exactly 1 time\n        1045453 (2.03%) aligned &gt;1 times\n56.79% overall alignment rate\n42864294 reads; of these:\n  42864294 (100.00%) were paired; of these:\n    23640036 (55.15%) aligned concordantly 0 times\n    17633629 (41.14%) aligned concordantly exactly 1 time\n    1590629 (3.71%) aligned concordantly &gt;1 times\n    ----\n    23640036 pairs aligned concordantly 0 times; of these:\n      1207857 (5.11%) aligned discordantly 1 time\n    ----\n    22432179 pairs aligned 0 times concordantly or discordantly; of these:\n      44864358 mates make up the pairs; of these:\n        35417854 (78.94%) aligned 0 times\n        8221441 (18.33%) aligned exactly 1 time\n        1225063 (2.73%) aligned &gt;1 times\n58.69% overall alignment rate\n43712298 reads; of these:\n  43712298 (100.00%) were paired; of these:\n    30353070 (69.44%) aligned concordantly 0 times\n    12098419 (27.68%) aligned concordantly exactly 1 time\n    1260809 (2.88%) aligned concordantly &gt;1 times\n    ----\n    30353070 pairs aligned concordantly 0 times; of these:\n      826489 (2.72%) aligned discordantly 1 time\n    ----\n    29526581 pairs aligned 0 times concordantly or discordantly; of these:\n      59053162 mates make up the pairs; of these:\n        51066335 (86.48%) aligned 0 times\n        6685083 (11.32%) aligned exactly 1 time\n        1301744 (2.20%) aligned &gt;1 times\n41.59% overall alignment rate\n47501524 reads; of these:\n  47501524 (100.00%) were paired; of these:\n    27841628 (58.61%) aligned concordantly 0 times\n    18249998 (38.42%) aligned concordantly exactly 1 time\n    1409898 (2.97%) aligned concordantly &gt;1 times\n    ----\n    27841628 pairs aligned concordantly 0 times; of these:\n      1281752 (4.60%) aligned discordantly 1 time\n    ----\n    26559876 pairs aligned 0 times concordantly or discordantly; of these:\n      53119752 mates make up the pairs; of these:\n        42721011 (80.42%) aligned 0 times\n        9197577 (17.31%) aligned exactly 1 time\n        1201164 (2.26%) aligned &gt;1 times\n55.03% overall alignment rate\n42677752 reads; of these:\n  42677752 (100.00%) were paired; of these:\n    25633048 (60.06%) aligned concordantly 0 times\n    15651560 (36.67%) aligned concordantly exactly 1 time\n    1393144 (3.26%) aligned concordantly &gt;1 times\n    ----\n    25633048 pairs aligned concordantly 0 times; of these:\n      1075688 (4.20%) aligned discordantly 1 time\n    ----\n    24557360 pairs aligned 0 times concordantly or discordantly; of these:\n      49114720 mates make up the pairs; of these:\n        38244722 (77.87%) aligned 0 times\n        9400848 (19.14%) aligned exactly 1 time\n        1469150 (2.99%) aligned &gt;1 times\n55.19% overall alignment rate"
  },
  {
    "objectID": "modules/30-lncRNA.html#convert-to-bams",
    "href": "modules/30-lncRNA.html#convert-to-bams",
    "title": "lncRNA Discovery",
    "section": "1.4 convert to bams",
    "text": "1.4 convert to bams\nfor samfile in ../output/05.33-lncRNA-discovery/*.sam; do\n  bamfile=\"${samfile%.sam}.bam\"\n  sorted_bamfile=\"${samfile%.sam}.sorted.bam\"\n  \n  # Convert SAM to BAM\n  /home/shared/samtools-1.12/samtools view -bS -@ 20 \"$samfile\" &gt; \"$bamfile\"\n  \n  # Sort BAM\n  /home/shared/samtools-1.12/samtools sort -@ 20 \"$bamfile\" -o \"$sorted_bamfile\"\n  \n  # Index sorted BAM\n  /home/shared/samtools-1.12/samtools index -@ 20 \"$sorted_bamfile\"\ndone"
  },
  {
    "objectID": "modules/30-lncRNA.html#looking-at-bams",
    "href": "modules/30-lncRNA.html#looking-at-bams",
    "title": "lncRNA Discovery",
    "section": "1.5 Looking at Bams",
    "text": "1.5 Looking at Bams\n\n\n\nigv"
  },
  {
    "objectID": "modules/01-github.html",
    "href": "modules/01-github.html",
    "title": "GitHub",
    "section": "",
    "text": "GitHub is a web-based platform that provides hosting for software development and a set of tools for version control using Git. Created by Tom Preston-Werner, Chris Wanstrath, and PJ Hyett in 2008, GitHub has become one of the most popular platforms for developers to share code, collaborate on projects, and even showcase their portfolio.\n\n\n\n\nGitHub makes it extremely easy for multiple people to work on the same project. Features like forks, pull requests, and issues help facilitate effective collaboration.\n\n\n\nBuilt on Git, GitHub provides robust version control capabilities, allowing you to keep track of changes, revert to previous states, and work on different branches simultaneously.\n\n\n\nGitHub is not just for code. It’s widely used for documentation, thanks to support for Markdown and GitHub Pages which can turn repositories into full-fledged, static websites.\n\n\n\n\n\nRepositories: Central locations for storing all of your project’s files and revision history.\nCommits: Save changes to the repository.\ngit add .\ngit commit -m \"Your message here\"\nIssues: Report bugs, request features, and manage tasks in a project.\nActions: Automate tasks like building, testing, and deploying your code.\nGitHub Pages: Host websites directly from a GitHub repository.\n\n\n\n\n\n\nCommit messages should be concise and describe what was done and why. This makes it easier to review changes later.\n\n\n\nAlways include a README.md file to explain what your project is, how to set it up, and how to use it. Good documentation encourages collaboration.\n\n\n\nMake use of GitHub features like Projects, Milestones, and Labels to organize and track work on your repositories.\nBy adhering to these best practices, you can make the most out of GitHub’s myriad features for both personal and collaborative projects."
  },
  {
    "objectID": "modules/01-github.html#why-is-github-useful",
    "href": "modules/01-github.html#why-is-github-useful",
    "title": "GitHub",
    "section": "",
    "text": "GitHub makes it extremely easy for multiple people to work on the same project. Features like forks, pull requests, and issues help facilitate effective collaboration.\n\n\n\nBuilt on Git, GitHub provides robust version control capabilities, allowing you to keep track of changes, revert to previous states, and work on different branches simultaneously.\n\n\n\nGitHub is not just for code. It’s widely used for documentation, thanks to support for Markdown and GitHub Pages which can turn repositories into full-fledged, static websites."
  },
  {
    "objectID": "modules/01-github.html#key-github-features-and-examples",
    "href": "modules/01-github.html#key-github-features-and-examples",
    "title": "GitHub",
    "section": "",
    "text": "Repositories: Central locations for storing all of your project’s files and revision history.\nCommits: Save changes to the repository.\ngit add .\ngit commit -m \"Your message here\"\nIssues: Report bugs, request features, and manage tasks in a project.\nActions: Automate tasks like building, testing, and deploying your code.\nGitHub Pages: Host websites directly from a GitHub repository."
  },
  {
    "objectID": "modules/01-github.html#best-practices",
    "href": "modules/01-github.html#best-practices",
    "title": "GitHub",
    "section": "",
    "text": "Commit messages should be concise and describe what was done and why. This makes it easier to review changes later.\n\n\n\nAlways include a README.md file to explain what your project is, how to set it up, and how to use it. Good documentation encourages collaboration.\n\n\n\nMake use of GitHub features like Projects, Milestones, and Labels to organize and track work on your repositories.\nBy adhering to these best practices, you can make the most out of GitHub’s myriad features for both personal and collaborative projects."
  },
  {
    "objectID": "modules/02-bash.html",
    "href": "modules/02-bash.html",
    "title": "bash",
    "section": "",
    "text": "Bash, or the Bourne Again Shell, is a Unix shell and command-line interface for operating systems like Linux and macOS. Created by Brian Fox in 1989 as a free software replacement for the Bourne Shell, Bash is one of the most widely-used and powerful shells available. It serves as both a scripting language and an interactive command interpreter.\n\n\n\n\nBash scripts can automate repetitive tasks, making your workflow more efficient.\n\n\n\nBash is crucial for system administration tasks such as managing users, processes, and system updates.\n\n\n\nBash commands can be combined and executed in a variety of ways to perform complex tasks.\n\n\n\nBash scripts are usually portable across different Unix-based systems with little to no modification.\n\n\n\nBash can interact with other command-line tools and languages like awk, sed, and even Python, further extending its utility.\n\n\n\n\n\nCommands: The basic units of instruction in Bash.\nls  # List files\ncd directory/  # Change directory\nVariables: Store data that can be accessed and manipulated.\nNAME=\"John\"\necho $NAME\nControl Structures: If-else statements, loops, and case statements to control flow.\nif [ \"$a\" -eq \"$b\" ]; then\n    echo \"a is equal to b\"\nfi\nPipes and Redirection: Connect multiple commands and redirect output.\nls | grep '.txt'  # List only .txt files\necho \"Hello\" &gt; file.txt  # Write to file\nScripting: Combine all of the above to write complex scripts.\n#!/bin/bash\n# This is a simple script\necho \"Hello, world!\"\n\n\n\n\n\n\nAlways comment your scripts to explain what each part is doing. This is especially useful for more complex scripts.\n# This is a comment explaining the following line of code\nls\n\n\n\nChoose descriptive variable names to make your script more readable.\n# Good\nfile_count=10\n\n# Bad\nfc=10\n\n\n\nBash has many features, but that doesn’t mean you have to use them all. Stick to the simplest approach that gets the job done.\nBy following these best practices, you can write effective, robust, and maintainable Bash scripts, making the most of what this versatile tool has to offer."
  },
  {
    "objectID": "modules/02-bash.html#why-is-bash-useful",
    "href": "modules/02-bash.html#why-is-bash-useful",
    "title": "bash",
    "section": "",
    "text": "Bash scripts can automate repetitive tasks, making your workflow more efficient.\n\n\n\nBash is crucial for system administration tasks such as managing users, processes, and system updates.\n\n\n\nBash commands can be combined and executed in a variety of ways to perform complex tasks.\n\n\n\nBash scripts are usually portable across different Unix-based systems with little to no modification.\n\n\n\nBash can interact with other command-line tools and languages like awk, sed, and even Python, further extending its utility."
  },
  {
    "objectID": "modules/02-bash.html#key-bash-features-and-examples",
    "href": "modules/02-bash.html#key-bash-features-and-examples",
    "title": "bash",
    "section": "",
    "text": "Commands: The basic units of instruction in Bash.\nls  # List files\ncd directory/  # Change directory\nVariables: Store data that can be accessed and manipulated.\nNAME=\"John\"\necho $NAME\nControl Structures: If-else statements, loops, and case statements to control flow.\nif [ \"$a\" -eq \"$b\" ]; then\n    echo \"a is equal to b\"\nfi\nPipes and Redirection: Connect multiple commands and redirect output.\nls | grep '.txt'  # List only .txt files\necho \"Hello\" &gt; file.txt  # Write to file\nScripting: Combine all of the above to write complex scripts.\n#!/bin/bash\n# This is a simple script\necho \"Hello, world!\""
  },
  {
    "objectID": "modules/02-bash.html#best-practices",
    "href": "modules/02-bash.html#best-practices",
    "title": "bash",
    "section": "",
    "text": "Always comment your scripts to explain what each part is doing. This is especially useful for more complex scripts.\n# This is a comment explaining the following line of code\nls\n\n\n\nChoose descriptive variable names to make your script more readable.\n# Good\nfile_count=10\n\n# Bad\nfc=10\n\n\n\nBash has many features, but that doesn’t mean you have to use them all. Stick to the simplest approach that gets the job done.\nBy following these best practices, you can write effective, robust, and maintainable Bash scripts, making the most of what this versatile tool has to offer."
  },
  {
    "objectID": "modules/04-blast.html",
    "href": "modules/04-blast.html",
    "title": "NCBI Blast",
    "section": "",
    "text": "/home/shared/ncbi-blast-2.11.0+/bin/makeblastdb \\\n-in ../data/uniprot_sprot_r2023_01.fasta \\\n-dbtype prot \\\n-out ../blastdb/uniprot_sprot_r2023_01\n/home/shared/ncbi-blast-2.11.0+/bin/blastx \\\n-query ../data/Ab_4denovo_CLC6_a.fa \\\n-db ../blastdb/uniprot_sprot_r2023_01 \\\n-out ../output/Ab_4-uniprot_blastx.tab \\\n-evalue 1E-20 \\\n-num_threads 20 \\\n-max_target_seqs 1 \\\n-outfmt 6"
  },
  {
    "objectID": "modules/04-blast.html#database-creation",
    "href": "modules/04-blast.html#database-creation",
    "title": "NCBI Blast",
    "section": "Database Creation",
    "text": "Database Creation\n\nObtain Fasta (UniProt/Swiss-Prot)\nThis is from here picur reviewe sequences I named based on the identify of the database given\n\ncurrent_time &lt;- format(Sys.time(), \"%B %d, %Y %H:%M:%S\")\ncat(\"current date and time is \", current_time)\n\n\ncd ../data\ncurl -O https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.fasta.gz\nmv uniprot_sprot.fasta.gz uniprot_sprot_r2023_04.fasta.gz\ngunzip -k uniprot_sprot_r2023_04.fasta.gz\n\n\n\nMaking the database\n\nmkdir ../blastdb\n/home/shared/ncbi-blast-2.11.0+/bin/makeblastdb \\\n-in ../data/uniprot_sprot_r2023_01.fasta \\\n-dbtype prot \\\n-out ../blastdb/uniprot_sprot_r2023_01"
  },
  {
    "objectID": "modules/04-blast.html#getting-the-query-fasta-file",
    "href": "modules/04-blast.html#getting-the-query-fasta-file",
    "title": "NCBI Blast",
    "section": "Getting the query fasta file",
    "text": "Getting the query fasta file\n\ncurl https://eagle.fish.washington.edu/cnidarian/Ab_4denovo_CLC6_a.fa \\\n-k \\\n&gt; ../data/Ab_4denovo_CLC6_a.fa\n\nExploring what fasta file\n\nhead -3 ../data/Ab_4denovo_CLC6_a.fa\n\n\necho \"How many sequences are there?\"\ngrep -c \"&gt;\" ../data/Ab_4denovo_CLC6_a.fa\n\n\n# Read FASTA file\nfasta_file &lt;- \"../data/Ab_4denovo_CLC6_a.fa\"  # Replace with the name of your FASTA file\nsequences &lt;- readDNAStringSet(fasta_file)\n\n# Calculate sequence lengths\nsequence_lengths &lt;- width(sequences)\n\n# Create a data frame\nsequence_lengths_df &lt;- data.frame(Length = sequence_lengths)\n\n# Plot histogram using ggplot2\nggplot(sequence_lengths_df, aes(x = Length)) +\n  geom_histogram(binwidth = 1, color = \"grey\", fill = \"blue\", alpha = 0.75) +\n  labs(title = \"Histogram of Sequence Lengths\",\n       x = \"Sequence Length\",\n       y = \"Frequency\") +\n  theme_minimal()\n\n\n# Read FASTA file\nfasta_file &lt;- \"../data/Ab_4denovo_CLC6_a.fa\"\nsequences &lt;- readDNAStringSet(fasta_file)\n\n# Calculate base composition\nbase_composition &lt;- alphabetFrequency(sequences, baseOnly = TRUE)\n\n# Convert to data frame and reshape for ggplot2\nbase_composition_df &lt;- as.data.frame(base_composition)\nbase_composition_df$ID &lt;- rownames(base_composition_df)\nbase_composition_melted &lt;- reshape2::melt(base_composition_df, id.vars = \"ID\", variable.name = \"Base\", value.name = \"Count\")\n\n# Plot base composition bar chart using ggplot2\nggplot(base_composition_melted, aes(x = Base, y = Count, fill = Base)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", color = \"black\") +\n  labs(title = \"Base Composition\",\n       x = \"Base\",\n       y = \"Count\") +\n  theme_minimal() +\n  scale_fill_manual(values = c(\"A\" = \"green\", \"C\" = \"blue\", \"G\" = \"yellow\", \"T\" = \"red\"))\n\n\n# Read FASTA file\nfasta_file &lt;- \"../data/Ab_4denovo_CLC6_a.fa\"\nsequences &lt;- readDNAStringSet(fasta_file)\n\n# Count CG motifs in each sequence\ncount_cg_motifs &lt;- function(sequence) {\n  cg_motif &lt;- \"CG\"\n  return(length(gregexpr(cg_motif, sequence, fixed = TRUE)[[1]]))\n}\n\ncg_motifs_counts &lt;- sapply(sequences, count_cg_motifs)\n\n# Create a data frame\ncg_motifs_counts_df &lt;- data.frame(CG_Count = cg_motifs_counts)\n\n# Plot CG motifs distribution using ggplot2\nggplot(cg_motifs_counts_df, aes(x = CG_Count)) +\n  geom_histogram(binwidth = 1, color = \"black\", fill = \"blue\", alpha = 0.75) +\n  labs(title = \"Distribution of CG Motifs\",\n       x = \"Number of CG Motifs\",\n       y = \"Frequency\") +\n  theme_minimal()"
  },
  {
    "objectID": "modules/04-blast.html#running-blastx",
    "href": "modules/04-blast.html#running-blastx",
    "title": "NCBI Blast",
    "section": "Running Blastx",
    "text": "Running Blastx\n\n~/applications/ncbi-blast-2.13.0+/bin/blastx \\\n-query ../data/Ab_4denovo_CLC6_a.fa \\\n-db ../blastdb/uniprot_sprot_r2023_01 \\\n-out ../output/Ab_4-uniprot_blastx.tab \\\n-evalue 1E-20 \\\n-num_threads 20 \\\n-max_target_seqs 1 \\\n-outfmt 6\n\n\nhead -2 ../output/Ab_4-uniprot_blastx.tab\n\n\necho \"Number of lines in output\"\nwc -l ../output/Ab_4-uniprot_blastx.tab"
  },
  {
    "objectID": "modules/04-blast.html#joining-blast-table-with-annoations.",
    "href": "modules/04-blast.html#joining-blast-table-with-annoations.",
    "title": "NCBI Blast",
    "section": "Joining Blast table with annoations.",
    "text": "Joining Blast table with annoations.\n\nPrepping Blast table for easy join\n\ntr '|' '\\t' &lt; ../output/Ab_4-uniprot_blastx.tab \\\n&gt; ../output/Ab_4-uniprot_blastx_sep.tab\n\nhead -1 ../output/Ab_4-uniprot_blastx_sep.tab"
  },
  {
    "objectID": "modules/04-blast.html#could-do-some-cool-stuff-in-r-here-reading-in-table",
    "href": "modules/04-blast.html#could-do-some-cool-stuff-in-r-here-reading-in-table",
    "title": "NCBI Blast",
    "section": "Could do some cool stuff in R here reading in table",
    "text": "Could do some cool stuff in R here reading in table\n\nbltabl &lt;- read.csv(\"../output/Ab_4-uniprot_blastx_sep.tab\", sep = '\\t', header = FALSE)\n\nspgo &lt;- read.csv(\"https://gannet.fish.washington.edu/seashell/snaps/uniprot_table_r2023_01.tab\", sep = '\\t', header = TRUE)\n\n\ndatatable(head(bltabl), options = list(scrollX = TRUE, scrollY = \"400px\", scrollCollapse = TRUE, paging = FALSE))\n\n\ndatatable(head(spgo), options = list(scrollX = TRUE, scrollY = \"400px\", scrollCollapse = TRUE, paging = FALSE))\n\n\ndatatable(\n  left_join(bltabl, spgo,  by = c(\"V3\" = \"Entry\")) %&gt;%\n  select(V1, V3, V13, Protein.names, Organism, Gene.Ontology..biological.process., Gene.Ontology.IDs) %&gt;% mutate(V1 = str_replace_all(V1, \n            pattern = \"solid0078_20110412_FRAG_BC_WHITE_WHITE_F3_QV_SE_trimmed\", replacement = \"Ab\"))\n)\n\n\nannot_tab &lt;-\n  left_join(bltabl, spgo,  by = c(\"V3\" = \"Entry\")) %&gt;%\n  select(V1, V3, V13, Protein.names, Organism, Gene.Ontology..biological.process., Gene.Ontology.IDs) %&gt;% mutate(V1 = str_replace_all(V1, \n            pattern = \"solid0078_20110412_FRAG_BC_WHITE_WHITE_F3_QV_SE_trimmed\", replacement = \"Ab\"))\n\n\n# Read dataset\ndataset &lt;- read.csv(\"../output/blast_annot_go.tab\", sep = '\\t')  # Replace with the path to your dataset\n\n# Select the column of interest\ncolumn_name &lt;- \"Organism\"  # Replace with the name of the column of interest\ncolumn_data &lt;- dataset[[column_name]]\n\n# Count the occurrences of the strings in the column\nstring_counts &lt;- table(column_data)\n\n# Convert to a data frame, sort by count, and select the top 10\nstring_counts_df &lt;- as.data.frame(string_counts)\ncolnames(string_counts_df) &lt;- c(\"String\", \"Count\")\nstring_counts_df &lt;- string_counts_df[order(string_counts_df$Count, decreasing = TRUE), ]\ntop_10_strings &lt;- head(string_counts_df, n = 10)\n\n# Plot the top 10 most common strings using ggplot2\nggplot(top_10_strings, aes(x = reorder(String, -Count), y = Count, fill = String)) +\n  geom_bar(stat = \"identity\", position = \"dodge\", color = \"black\") +\n  labs(title = \"Top 10 Species hits\",\n       x = column_name,\n       y = \"Count\") +\n  theme_minimal() +\n  theme(legend.position = \"none\") +\n  coord_flip()\n\n\ndata &lt;- read.csv(\"../output/blast_annot_go.tab\", sep = '\\t')\n\n# Rename the `Gene.Ontology..biological.process.` column to `Biological_Process`\ncolnames(data)[colnames(data) == \"Gene.Ontology..biological.process.\"] &lt;- \"Biological_Process\"\n\n# Separate the `Biological_Process` column into individual biological processes\ndata_separated &lt;- unlist(strsplit(data$Biological_Process, split = \";\"))\n\n# Trim whitespace from the biological processes\ndata_separated &lt;- gsub(\"^\\\\s+|\\\\s+$\", \"\", data_separated)\n\n# Count the occurrences of each biological process\nprocess_counts &lt;- table(data_separated)\nprocess_counts &lt;- data.frame(Biological_Process = names(process_counts), Count = as.integer(process_counts))\nprocess_counts &lt;- process_counts[order(-process_counts$Count), ]\n\n# Select the 20 most predominant biological processes\ntop_20_processes &lt;- process_counts[1:20, ]\n\n# Create a color palette for the bars\nbar_colors &lt;- rainbow(nrow(top_20_processes))\n\n# Create a staggered vertical bar plot with different colors for each bar\nbarplot(top_20_processes$Count, names.arg = rep(\"\", nrow(top_20_processes)), col = bar_colors,\n        ylim = c(0, max(top_20_processes$Count) * 1.25),\n        main = \"Occurrences of the 20 Most Predominant Biological Processes\", xlab = \"Biological Process\", ylab = \"Count\")\n\n\n# Create a separate plot for the legend\npng(\"../output/GOlegend.png\", width = 800, height = 600)\npar(mar = c(0, 0, 0, 0))\nplot.new()\nlegend(\"center\", legend = top_20_processes$Biological_Process, fill = bar_colors, cex = 1, title = \"Biological Processes\")\ndev.off()\n\n\nknitr::include_graphics(\"../output/GOlegend.png\")"
  },
  {
    "objectID": "modules/04-blast.html#navigating-annotation",
    "href": "modules/04-blast.html#navigating-annotation",
    "title": "NCBI Blast",
    "section": "Navigating Annotation",
    "text": "Navigating Annotation\nThe following is a stepwise example or annotation of a gene set using UniProt::Swiss-Prot (reviewed) such that Gene Ontology terms can be associated with each gene.\nIn this following chunk where the fasta file is downloaded the release is noted and the file name is modified accordingly.\n\ncd DRAFT_Funct_Enrich/annot\n\ncurl -O https://ftp.uniprot.org/pub/databases/uniprot/current_release/knowledgebase/complete/uniprot_sprot.fasta.gz\n\nmv uniprot_sprot.fasta.gz uniprot_sprot_r2023_02.fasta.gz\ngunzip -k uniprot_sprot_r2023_02.fasta.gz\n\nA protein blast database is then made.\n\n/home/shared/ncbi-blast-2.11.0+/bin/makeblastdb \\\n-in DRAFT_Funct_Enrich/annot/uniprot_sprot_r2023_02.fasta \\\n-dbtype prot \\\n-out DRAFT_Funct_Enrich/annot/uniprot_sprot_r2023_02\n\nIn a majority of cases you will want to annotate a gene set to get gene ontology information. If you are creating your own genome or transcriptome it should be rather straightforward to know what file to annotate. If using a widely studied system where there are publically available resources, it is advisable to use those as this is the best way to facilitate integration of data sets. In this case study we will be considering the Eastern oyster, (Crassostrea virginica) for which there is data at NCBI and Ensembl Metazoa. At NCBI there is both a GenBank and RefSeq assembly available.\nIn order to know which of the numerous fasta files should annotated with gene ontology information one should think downstream (or look to files already generated) to the identifiers in genesets that would be subject to functional enrichment tests.\nThe resulting fpkm count matrix for our case study is from an experiment where male and female oysters where exposed to low pH (and control) conditions. The count matrix is accessible here (csv). Hisat2/Stringtie was used to generate the count matrix with GCF_002022765.2_C_virginica-3.0_genomic.gff formatting thus responsible for gene naming. Specifically the naming format is as follows gene-LOC111099033,gene-LOC111099034,gene-LOC111099035.\nThe following fasta was selected for annotation: GCF_002022765.2_C_virginica-3.0_translated_cds.faa.gz\n\ncd DRAFT_Funct_Enrich/annot\n\ncurl -O https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/002/022/765/GCF_002022765.2_C_virginica-3.0/GCF_002022765.2_C_virginica-3.0_translated_cds.faa.gz\n\ngunzip -k GCF_002022765.2_C_virginica-3.0_translated_cds.faa.gz\n\n\nhead -2 DRAFT_Funct_Enrich/annot/GCF_002022765.2_C_virginica-3.0_translated_cds.faa\n\necho \"number of sequences\"\ngrep -c  \"&gt;\" DRAFT_Funct_Enrich/annot/GCF_002022765.2_C_virginica-3.0_translated_cds.faa\n\n&gt;lcl|NC_035780.1_prot_XP_022327646.1_1 [gene=LOC111126949] [db_xref=GeneID:111126949] [protein=UNC5C-like protein] [protein_id=XP_022327646.1] [location=join(30535..31557,31736..31887,31977..32565,32959..33204)] [gbkey=CDS]\nMTEVCYIWASSSTTVVICGIFFIVWRCFISIKKRASPLHGSSQQVCQTCQIEGHDFGEFQLSCRRQNTNVGYDLQGRRSD\nThis protein fasta is used as query for blast of uniprot_sprot database.\n\n/home/shared/ncbi-blast-2.11.0+/bin/blastp \\\n-query DRAFT_Funct_Enrich/annot/GCF_002022765.2_C_virginica-3.0_translated_cds.faa \\\n-db DRAFT_Funct_Enrich/annot/uniprot_sprot_r2023_02 \\\n-out DRAFT_Funct_Enrich/annot/Cvir_transcds-uniprot_blastp.tab \\\n-evalue 1E-20 \\\n-num_threads 40 \\\n-max_target_seqs 1 \\\n-outfmt 6\n\nHere is what the output file looks like, and at this point we want to get the UniProt Accession number for each gene\n\nhead -2 DRAFT_Funct_Enrich/annot/Cvir_transcds-uniprot_blastp.tab\n\n\nblast &lt;- read.csv(\"DRAFT_Funct_Enrich/annot/Cvir_transcds-uniprot_blastp.tab\", sep = '\\t', header = FALSE)\n\nConvert fasta to tab\n\nperl -e '$count=0; $len=0; while(&lt;&gt;) {s/\\r?\\n//; s/\\t/ /g; if (s/^&gt;//) { if ($. != 1) {print \"\\n\"} s/ |$/\\t/; $count++; $_ .= \"\\t\";} else {s/ //g; $len += length($_)} print $_;} print \"\\n\"; warn \"\\nConverted $count FASTA records in $. lines to tabular format\\nTotal sequence length: $len\\n\\n\";' \\\nDRAFT_Funct_Enrich/annot/GCF_002022765.2_C_virginica-3.0_translated_cds.faa &gt; DRAFT_Funct_Enrich/annot/GCF_002022765.2_C_virginica-3.0_translated_cds.tab\n\n\nhead -1 DRAFT_Funct_Enrich/annot/GCF_002022765.2_C_virginica-3.0_translated_cds.tab\n\n\ncdsftab &lt;- read.csv(\"DRAFT_Funct_Enrich/annot/GCF_002022765.2_C_virginica-3.0_translated_cds.tab\", sep = '\\t', header = FALSE, row.names=NULL)\n\nNow we can take the two data frames: A) blast output of taking protein fasta and comparing to uniprot_swiss-prot and B) a tabular version of same fasta file that has ID numbers of importance. Note this importance was determined based on what we want to use down stream.\n\ng.spid &lt;- left_join(blast, cdsftab, by = \"V1\") %&gt;%\n  mutate(gene = str_extract(V2.y, \"(?&lt;=\\\\[gene=)\\\\w+\")) %&gt;%\n  select(gene, V11, V2.x) %&gt;%\n  mutate(SPID = str_extract(V2.x, \"(?&lt;=\\\\|)[^\\\\|]*(?=\\\\|)\")) %&gt;%\n  distinct(gene, SPID, .keep_all = TRUE)\n\nLet’s break it down step by step:\n\ng.spid &lt;- left_join(blast, cdsftab, by = \"V1\") - This line is using the left_join() function from dplyr to merge the blast and cdsftab datasets by the column “V1”. A left join retains all the rows in the blast data frame and appends the matching rows in the cdsftab data frame. If there is no match, the result is NA. The result of this operation is assigned to the g.spid object.\nmutate(gene = str_extract(V2.y, \"(?&lt;=\\\\[gene=)\\\\w+\")) - This line is using the mutate() function from dplyr to add a new column called “gene” to the data frame. The new column is created by extracting substrings from the “V2.y” column based on the given regular expression pattern \"(?&lt;=\\\\[gene=)\\\\w+\". This regular expression matches and extracts any word (sequence of word characters, i.e., alphanumeric and underscore) that comes after “[gene=”.\nselect(gene, V11, V2.x) - This line is using the select() function from dplyr to keep only the specified columns (“gene”, “V11”, and “V2.x”) in the data frame.\nmutate(SPID = str_extract(V2.x, \"(?&lt;=\\\\|)[^\\\\|]*(?=\\\\|)\")) - Again, the mutate() function is used to add another new column named “SPID”. This column is created by extracting substrings from the “V2.x” column. The regular expression \"(?&lt;=\\\\|)[^\\\\|]*(?=\\\\|)\" is designed to extract any character(s) that is/are surrounded by “|” (pipe symbol). This is a common format for delimited strings.\ndistinct(gene, SPID, .keep_all = TRUE) - This line is using the distinct() function from dplyr to remove duplicate rows based on the “gene” and “SPID” columns. The .keep_all = TRUE argument means that all other columns are also kept in the result, not just the “gene” and “SPID” columns.\n\nThe resulting g.spid data frame should have unique rows with respect to the “gene” and “SPID” columns, and it should contain these two new columns, “gene” and “SPID”, extracted from the original data based on specific string patterns.\nNow lets just write out SPIDs.\n\nleft_join(blast, cdsftab, by = \"V1\") %&gt;%\n  mutate(gene = str_extract(V2.y, \"(?&lt;=\\\\[gene=)\\\\w+\")) %&gt;%\n  select(gene, V11, V2.x) %&gt;%\n  mutate(SPID = str_extract(V2.x, \"(?&lt;=\\\\|)[^\\\\|]*(?=\\\\|)\")) %&gt;%\n  distinct(gene, SPID, .keep_all = TRUE) %&gt;%\n  select(SPID) %&gt;%\n  write.table(file = \"DRAFT_Funct_Enrich/annot/SPID.txt\", sep = \"\\t\", row.names = FALSE, quote = FALSE\n ) \n\nWith a list of matching Swiss-Prot IDs, (technically UniProt Accession number) we can go back to https://www.uniprot.org and grab corresponding GO terms. This can be done via a web or using Python API.\nUsing Web\nUsing ID Mapping\n\n\n\nid\n\n\n\n\n\nfinished\n\n\nNow will customize columns to get GO IDs.\n\n\n\ncustcol\n\n\n\nhead -2 DRAFT_Funct_Enrich/annot/uniprotGO.tab\n\nFinally we can join table to get “LOCIDs” the notation for our DEGs, with GO terms.\n\ngo &lt;- read.csv(\"DRAFT_Funct_Enrich/annot/uniprotGO.tab\", sep = '\\t', header = TRUE, row.names=NULL)\n\n\nleft_join(g.spid, go, by = c(\"SPID\" = \"Entry\")) %&gt;%\n  select(gene,Gene.Ontology.IDs) %&gt;%\n  write.table(file = \"DRAFT_Funct_Enrich/annot/geneGO.txt\", sep = \"\\t\", row.names = FALSE, quote = FALSE\n  )\n\n\nhead DRAFT_Funct_Enrich/annot/geneGO.txt\n\nUsing API\n\npython3 DRAFT_Funct_Enrich/annot/uniprot-retrieval.py DRAFT_Funct_Enrich/annot/SPID.txt"
  }
]